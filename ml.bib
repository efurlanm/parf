
@article{sharma_review_2023,
	title = {A {Review} of {Physics}-{Informed} {Machine} {Learning} in {Fluid} {Mechanics}},
	volume = {16},
	copyright = {Review},
	issn = {1996-1073},
	shorttitle = {{PINN} {Fluid} {Mechanics}},
	url = {https://www.mdpi.com/1996-1073/16/5/2343},
	doi = {10.3390/en16052343},
	abstract = {Physics-informed machine-learning (PIML) enables the integration of domain knowledge with machine learning (ML) algorithms, which results in higher data efﬁciency and more stable predictions. This provides opportunities for augmenting—and even replacing—high-ﬁdelity numerical simulations of complex turbulent ﬂows, which are often expensive due to the requirement of high temporal and spatial resolution. In this review, we (i) provide an introduction and historical perspective of ML methods, in particular neural networks (NN), (ii) examine existing PIML applications to ﬂuid mechanics problems, especially in complex high Reynolds number ﬂows, (iii) demonstrate the utility of PIML techniques through a case study, and (iv) discuss the challenges and opportunities of developing PIML for ﬂuid mechanics.},
	language = {en},
	number = {5},
	urldate = {2023-04-28},
	journal = {Energies},
	author = {Sharma, Pushan and Chung, Wai Tong and Akoush, Bassem and Ihme, Matthias},
	month = feb,
	year = {2023},
	pages = {2343},
	file = {Sharma et al. - 2023 - A Review of Physics-Informed Machine Learning in F.pdf:/home/x/Library/library-ml-zotero/storage/MJMFUEHX/Sharma et al. - 2023 - A Review of Physics-Informed Machine Learning in F.pdf:application/pdf},
}

@article{lawal_physics-informed_2022,
	title = {Physics-{Informed} {Neural} {Network} ({PINN}) {Evolution} and {Beyond}: {A} {Systematic} {Literature} {Review} and {Bibliometric} {Analysis}},
	volume = {6},
	copyright = {Evolution and Beyond},
	issn = {2504-2289},
	shorttitle = {{PINN} evolution},
	url = {https://www.mdpi.com/2504-2289/6/4/140},
	doi = {10.3390/bdcc6040140},
	abstract = {This research aims to study and assess state-of-the-art physics-informed neural networks (PINNs) from different researchers’ perspectives. The PRISMA framework was used for a systematic literature review, and 120 research articles from the computational sciences and engineering domain were speciﬁcally classiﬁed through a well-deﬁned keyword search in Scopus and Web of Science databases. Through bibliometric analyses, we have identiﬁed journal sources with the most publications, authors with high citations, and countries with many publications on PINNs. Some newly improved techniques developed to enhance PINN performance and reduce high training costs and slowness, among other limitations, have been highlighted. Different approaches have been introduced to overcome the limitations of PINNs. In this review, we categorized the newly proposed PINN methods into Extended PINNs, Hybrid PINNs, and Minimized Loss techniques. Various potential future research directions are outlined based on the limitations of the proposed solutions.},
	language = {en},
	number = {4},
	urldate = {2023-04-28},
	journal = {Big Data and Cognitive Computing},
	author = {Lawal, Zaharaddeen Karami and Yassin, Hayati and Lai, Daphne Teck Ching and Che Idris, Azam},
	month = nov,
	year = {2022},
	pages = {140},
	file = {Lawal et al. - 2022 - Physics-Informed Neural Network (PINN) Evolution a.pdf:/home/x/Library/library-ml-zotero/storage/J952V88S/Lawal et al. - 2022 - Physics-Informed Neural Network (PINN) Evolution a.pdf:application/pdf},
}

@article{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {II}): {Data}-driven {Discovery} of {Nonlinear} {Partial} {Diﬀerential} {Equations}},
	shorttitle = {{PINN} {II}},
	url = {https://arxiv.org/abs/1711.10566},
	language = {en},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	year = {2017},
	file = {Raissi et al. - Physics Informed Deep Learning (Part II) Data-dri.pdf:/home/x/Library/library-ml-zotero/storage/PX7MPUSK/Raissi et al. - Physics Informed Deep Learning (Part II) Data-dri.pdf:application/pdf},
}

@article{raissi_cs598_2021,
	title = {{CS598}: {Physics}-{Informed} {Neural} {Networks}:  {A} deep learning framework for solving forward and inverse problems involving nonlinear {PDEs}},
	shorttitle = {{PINN} slides},
	url = {https://arindam.cs.illinois.edu/courses/f21cs598/slides/pml11_598f21.pdf},
	language = {en},
	author = {Raissi, M and Perdikaris, P and Karniadakis, GE},
	year = {2021},
	file = {Raissi et al. - CS598 Physics-Informed Neural Networks  A deep l.pdf:/home/x/Library/library-ml-zotero/storage/Y7E7BZYH/Raissi et al. - CS598 Physics-Informed Neural Networks  A deep l.pdf:application/pdf},
}

@article{cuomo_scientific_2022,
	title = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}: {Where} we are and {What}’s {Next}},
	volume = {92},
	issn = {0885-7474, 1573-7691},
	shorttitle = {{PINN} review},
	url = {https://link.springer.com/10.1007/s10915-022-01939-z},
	doi = {10.1007/s10915-022-01939-z},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must ﬁt observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
	language = {en},
	number = {3},
	urldate = {2023-04-28},
	journal = {Journal of Scientific Computing},
	author = {Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	month = sep,
	year = {2022},
	pages = {88},
	file = {Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:/home/x/Library/library-ml-zotero/storage/BCTXFUNL/Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:application/pdf},
}

@article{sacchetti_neural_2022,
	title = {Neural {Networks} to {Solve} {Partial} {Differential} {Equations}: {A} {Comparison} {With} {Finite} {Elements}},
	volume = {10},
	copyright = {Heat 2D},
	issn = {2169-3536},
	shorttitle = {{ANN} {PDE} {FE} comparison},
	url = {https://ieeexplore.ieee.org/document/9737092/},
	doi = {10.1109/ACCESS.2022.3160186},
	abstract = {We compare the Finite Element Method (FEM) simulation of a standard Partial Differential Equation thermal problem of a plate with a hole with a Neural Network (NN) simulation. The largest deviation from the true solution obtained from FEM (0.015 for a solution on the order of unity) is easily achieved with NN too without much tuning of the hyperparameters. Accuracies below 0.01 instead require reﬁnement with an alternative optimizer to reach a similar performance with NN. A rough comparison between the Floating Point Operations values, as a machine-independent quantiﬁcation of the computational performance, suggests a signiﬁcant difference between FEM and NN in favour of the former. This also strongly holds for computation time: for an accuracy on the order of 10−5, FEM and NN require 54 and 1100 seconds, respectively. A detailed analysis of the effect of varying different hyperparameters shows that accuracy and computational time only weakly depend on the major part of them. Accuracies below 0.01 cannot be achieved with the ‘‘adam’’ optimizers and it looks as though accuracies below 10−5 cannot be achieved at all. In conclusion, the present work shows that for the concrete case of solving a steady-state 2D heat equation, the performance of a FEM algorithm is signiﬁcantly better than the solution via networks.},
	language = {en},
	urldate = {2023-04-28},
	journal = {IEEE Access},
	author = {Sacchetti, Andrea and Bachmann, Benjamin and Loffel, Kaspar and Kunzi, Urs-Martin and Paoli, Beatrice},
	year = {2022},
	pages = {32271--32279},
	file = {Sacchetti et al. - 2022 - Neural Networks to Solve Partial Differential Equa.pdf:/home/x/Library/library-ml-zotero/storage/KBYQEYSC/Sacchetti et al. - 2022 - Neural Networks to Solve Partial Differential Equa.pdf:application/pdf},
}

@misc{he_unsupervised_2020,
	title = {An unsupervised learning approach to solving heat equations on chip based on {Auto} {Encoder} and {Image} {Gradient}},
	copyright = {Heat 3D},
	shorttitle = {{PINN} on chip},
	url = {http://arxiv.org/abs/2007.09684},
	abstract = {Solving heat transfer equations on chip becomes very critical in the upcoming 5G and AI chip-package-systems. However, batches of simulations have to be performed for data driven supervised machine learning models. Data driven methods are data hungry, to address this, Physics Informed Neural Networks (PINN) have been proposed. However, vanilla PINN models solve one ﬁxed heat equation at a time, so the models have to be retrained for heat equations with diﬀerent source terms. Additionally, issues related to multi-objective optimization have to be resolved while using PINN to minimize the PDE residual, satisfy boundary conditions and ﬁt the observed data etc. Therefore, this paper investigates an unsupervised learning approach for solving heat transfer equations on chip without using solution data and generalizing the trained network for predicting solutions for heat equations with unseen source terms. Speciﬁcally, a hybrid framework of Auto Encoder (AE) and Image Gradient (IG) based network is designed. The AE is used to encode diﬀerent source terms of the heat equations. The IG based network implements a second order central diﬀerence algorithm for structured grids and minimizes the PDE residual. The eﬀectiveness of the designed network is evaluated by solving heat equations for various use cases. It is proved that with limited number of source terms to train the AE network, the framework can not only solve the given heat transfer problems with a single training process, but also make reasonable predictions for unseen cases (heat equations with new source terms) without retraining.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {He, Haiyang and Pathak, Jay},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Physics - Applied Physics, Physics - Computational Physics, Statistics - Machine Learning},
	file = {He and Pathak - 2020 - An unsupervised learning approach to solving heat .pdf:/home/x/Library/library-ml-zotero/storage/BJK29L52/He and Pathak - 2020 - An unsupervised learning approach to solving heat .pdf:application/pdf},
}

@misc{ma_heat_2022,
	title = {Heat {Conduction} {Plate} {Layout} {Optimization} using {Physics}-driven {Convolutional} {Neural} {Networks}},
	shorttitle = {Heat {PINN} {Convolutional}},
	url = {http://arxiv.org/abs/2201.10002},
	abstract = {The layout optimization of the heat conduction is essential during design in engineering, especially for thermal sensible products. When the optimization algorithm iteratively evaluates different loading cases, the traditional numerical simulation methods used usually lead to a substantial computational cost. To effectively reduce the computational effort, data-driven approaches are used to train a surrogate model as a mapping between the prescribed external loads and various geometry. However, the existing model are trained by data-driven methods which requires intensive training samples that from numerical simulations and not really effectively solve the problem. Choosing the steady heat conduction problems as examples, this paper proposes a Physics-driven Convolutional Neural Networks (PD-CNN) method to infer the physical field solutions for random varied loading cases. After that, the Particle Swarm Optimization (PSO) algorithm is used to optimize the sizes and the positions of the hole masks in the prescribed design domain, and the average temperature value of the entire heat conduction field is minimized, and the goal of minimizing heat transfer is achieved. Compared with the existing data-driven approaches, the proposed PD-CNN optimization framework not only predict field solutions that are highly consistent with conventional simulation results, but also generate the solution space with without any pre-obtained training data.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Ma, Hao and Sun, Yang and Chiarelli, Mario},
	month = jan,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Engineering, Finance, and Science},
	file = {Ma et al. - 2022 - Heat Conduction Plate Layout Optimization using Ph.pdf:/home/x/Library/library-ml-zotero/storage/DATU86B9/Ma et al. - 2022 - Heat Conduction Plate Layout Optimization using Ph.pdf:application/pdf},
}

@article{liu_novel_2022,
	title = {A novel meta-learning initialization method for physics-informed neural networks},
	volume = {34},
	copyright = {With heat sources},
	issn = {0941-0643, 1433-3058},
	shorttitle = {Initialization method for {PINN}},
	url = {https://link.springer.com/10.1007/s00521-022-07294-2},
	doi = {10.1007/s00521-022-07294-2},
	abstract = {Physics-informed neural networks (PINNs) have been widely used to solve various scientiﬁc computing problems. However, large training costs limit PINNs for some real-time applications. Although some works have been proposed to improve the training eﬃciency of PINNs, few consider the inﬂuence of initialization. To this end, we propose a New Reptile initialization based Physics-Informed Neural Network (NRPINN). The original Reptile algorithm is a meta-learning initialization method based on labeled data. PINNs can be trained with less labeled data or even without any labeled data by adding partial diﬀerential equations (PDEs) as a penalty term into the loss function. Inspired by this idea, we propose the new Reptile initialization to sample more tasks from the parameterized PDEs and adapt the penalty term of the loss. The new Reptile initialization can acquire initialization parameters from related tasks by supervised, unsupervised, and semi-supervised learning. Then, PINNs with initialization parameters can eﬃciently solve PDEs. Besides, the new Reptile initialization can also be used for the variants of PINNs. Finally, we demonstrate and verify the NRPINN considering both forward problems, including solving Poisson, Burgers, and Schr¨odinger equations, as well as inverse problems, where unknown parameters in the PDEs are estimated. Experimental results show that the NRPINN training is much faster and achieves higher accuracy than PINNs with other initialization methods.},
	language = {en},
	number = {17},
	urldate = {2023-04-28},
	journal = {Neural Computing and Applications},
	author = {Liu, Xu and Zhang, Xiaoya and Peng, Wei and Zhou, Weien and Yao, Wen},
	month = sep,
	year = {2022},
	pages = {14511--14534},
	file = {Liu et al. - 2022 - A novel meta-learning initialization method for ph.pdf:/home/x/Library/library-ml-zotero/storage/H9JU9N4Y/Liu et al. - 2022 - A novel meta-learning initialization method for ph.pdf:application/pdf},
}

@article{krasnopolsky_new_2005,
	title = {New {Approach} to {Calculation} of {Atmospheric} {Model} {Physics}: {Accurate} and {Fast} {Neural} {Network} {Emulation} of {Longwave} {Radiation} in a {Climate} {Model}},
	volume = {133},
	copyright = {Atmospheric},
	issn = {1520-0493, 0027-0644},
	shorttitle = {{NN} for {Atmospheric} {Radiation}},
	url = {http://journals.ametsoc.org/doi/10.1175/MWR2923.1},
	doi = {10.1175/MWR2923.1},
	abstract = {A new approach based on a synergetic combination of statistical/machine learning and deterministic modeling within atmospheric models is presented. The approach uses neural networks as a statistical or machine learning technique for an accurate and fast emulation or statistical approximation of model physics parameterizations. It is applied to development of an accurate and fast approximation of an atmospheric longwave radiation parameterization for the NCAR Community Atmospheric Model, which is the most time consuming component of model physics. The developed neural network emulation is two orders of magnitude, 50–80 times, faster than the original parameterization. A comparison of the parallel 10-yr climate simulations performed with the original parameterization and its neural network emulations confirmed that these simulations produce almost identical results. The obtained results show the conceptual and practical possibility of an efficient synergetic combination of deterministic and statistical learning components within an atmospheric climate or forecast model. A developmental framework and practical validation criteria for neural network emulations of model physics components are outlined.},
	language = {en},
	number = {5},
	urldate = {2023-04-28},
	journal = {Monthly Weather Review},
	author = {Krasnopolsky, Vladimir M. and Fox-Rabinovitz, Michael S. and Chalikov, Dmitry V.},
	month = may,
	year = {2005},
	pages = {1370--1383},
	file = {Krasnopolsky et al. - 2005 - New Approach to Calculation of Atmospheric Model P.pdf:/home/x/Library/library-ml-zotero/storage/6UL498IT/Krasnopolsky et al. - 2005 - New Approach to Calculation of Atmospheric Model P.pdf:application/pdf},
}

@article{dueben_challenges_2018,
	title = {Challenges and design choices for global weather and climate models based on machine learning},
	volume = {11},
	issn = {1991-9603},
	shorttitle = {Climate model \& {ML}},
	url = {https://gmd.copernicus.org/articles/11/3999/2018/},
	doi = {10.5194/gmd-11-3999-2018},
	abstract = {Can models that are based on deep learning and trained on atmospheric data compete with weather and climate models that are based on physical principles and the basic equations of motion? This question has been asked often recently due to the boom in deep-learning techniques. The question is valid given the huge amount of data that are available, the computational efﬁciency of deep-learning techniques and the limitations of today’s weather and climate models in particular with respect to resolution and complexity.},
	language = {en},
	number = {10},
	urldate = {2023-04-28},
	journal = {Geoscientific Model Development},
	author = {Dueben, Peter D. and Bauer, Peter},
	month = oct,
	year = {2018},
	pages = {3999--4009},
	file = {Dueben and Bauer - 2018 - Challenges and design choices for global weather a.pdf:/home/x/Library/library-ml-zotero/storage/UVGWDATR/Dueben and Bauer - 2018 - Challenges and design choices for global weather a.pdf:application/pdf},
}

@article{sonnewald_bridging_2021,
	title = {Bridging observations, theory and numerical simulation of the ocean using machine learning},
	volume = {16},
	issn = {1748-9326},
	shorttitle = {Ocean {ML}},
	url = {https://iopscience.iop.org/article/10.1088/1748-9326/ac0eb0},
	doi = {10.1088/1748-9326/ac0eb0},
	abstract = {Progress within physical oceanography has been concurrent with the increasing sophistication of tools available for its study. The incorporation of machine learning (ML) techniques offers exciting possibilities for advancing the capacity and speed of established methods and for making substantial and serendipitous discoveries. Beyond vast amounts of complex data ubiquitous in many modern scientific fields, the study of the ocean poses a combination of unique challenges that ML can help address. The observational data available is largely spatially sparse, limited to the surface, and with few time series spanning more than a handful of decades. Important timescales span seconds to millennia, with strong scale interactions and numerical modelling efforts complicated by details such as coastlines. This review covers the current scientific insight offered by applying ML and points to where there is imminent potential. We cover the main three branches of the field: observations, theory, and numerical modelling. Highlighting both challenges and opportunities, we discuss both the historical context and salient ML tools. We focus on the use of ML in situ sampling and satellite observations, and the extent to which ML applications can advance theoretical oceanographic exploration, as well as aid numerical simulations. Applications that are also covered include model error and bias correction and current and potential use within data assimilation. While not without risk, there is great interest in the potential benefits of oceanographic ML applications; this review caters to this interest within the research community.},
	language = {en},
	number = {7},
	urldate = {2023-04-28},
	journal = {Environmental Research Letters},
	author = {Sonnewald, Maike and Lguensat, Redouane and Jones, Daniel C and Dueben, Peter D and Brajard, Julien and Balaji, V},
	month = jul,
	year = {2021},
	pages = {073008},
	file = {Sonnewald et al. - 2021 - Bridging observations, theory and numerical simula.pdf:/home/x/Library/library-ml-zotero/storage/46AGWTT9/Sonnewald et al. - 2021 - Bridging observations, theory and numerical simula.pdf:application/pdf},
}

@article{lee_neural_1990,
	title = {Neural algorithm for solving differential equations},
	volume = {91},
	issn = {00219991},
	shorttitle = {Neural algorithm for {ODE}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/002199919090007N},
	doi = {10.1016/0021-9991(90)90007-N},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {Journal of Computational Physics},
	author = {Lee, Hyuk and Kang, In Seok},
	month = nov,
	year = {1990},
	pages = {110--131},
	file = {Lee and Kang - 1990 - Neural algorithm for solving differential equation.pdf:/home/x/Library/library-ml-zotero/storage/NRM9AAWR/Lee and Kang - 1990 - Neural algorithm for solving differential equation.pdf:application/pdf},
}

@article{lagaris_neural-network_2000,
	title = {Neural-network methods for boundary value problems with irregular boundaries},
	volume = {11},
	issn = {10459227},
	shorttitle = {{ANN} \& boundaries},
	url = {http://ieeexplore.ieee.org/document/870037/},
	doi = {10.1109/72.870037},
	abstract = {Partial differential equations (PDEs) with boundary conditions (Dirichlet or Neumann) defined on boundaries with simple geometry have been successfully treated using sigmoidal multilayer perceptrons in previous works. This article deals with the case of complex boundary geometry, where the boundary is determined by a number of points that belong to it and are closely located, so as to offer a reasonable representation. Two networks are employed: a multilayer perceptron and a radial basis function network. The later is used to account for the exact satisfaction of the boundary conditions. The method has been successfully tested on two-dimensional and three-dimensional PDEs and has yielded accurate results.},
	language = {en},
	number = {5},
	urldate = {2023-04-28},
	journal = {IEEE Transactions on Neural Networks},
	author = {Lagaris, I.E. and Likas, A.C. and Papageorgiou, D.G.},
	month = sep,
	year = {2000},
	pages = {1041--1049},
	file = {Lagaris et al. - 2000 - Neural-network methods for boundary value problems.pdf:/home/x/Library/library-ml-zotero/storage/W59PPKV5/Lagaris et al. - 2000 - Neural-network methods for boundary value problems.pdf:application/pdf},
}

@article{baymani_artificial_2010,
	title = {Artificial {Neural} {Networks} {Approach} for {Solving} {Stokes} {Problem}},
	volume = {01},
	issn = {2152-7385, 2152-7393},
	shorttitle = {{PINN} {Stokes} problem},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/am.2010.14037},
	doi = {10.4236/am.2010.14037},
	abstract = {In this paper a new method based on neural network has been developed for obtaining the solution of the Stokes problem. We transform the mixed Stokes problem into three independent Poisson problems which by solving them the solution of the Stokes problem is obtained. The results obtained by this method, has been compared with the existing numerical method and with the exact solution of the problem. It can be observed that the current new approximation has higher accuracy. The number of model parameters required is less than conventional methods. The proposed new method is illustrated by an example.},
	language = {en},
	number = {04},
	urldate = {2023-04-28},
	journal = {Applied Mathematics},
	author = {Baymani, Modjtaba and Kerayechian, Asghar and Effati, Sohrab},
	year = {2010},
	pages = {288--292},
	file = {Baymani et al. - 2010 - Artificial Neural Networks Approach for Solving St.pdf:/home/x/Library/library-ml-zotero/storage/PHN5H9ST/Baymani et al. - 2010 - Artificial Neural Networks Approach for Solving St.pdf:application/pdf},
}

@article{chiaramonte_solving_2013,
	title = {Solving differential equations using neural networks},
	shorttitle = {{ODE} using {ANN}},
	url = {http://cs229.stanford.edu/proj2013/ChiaramonteKiener-SolvingDifferentialEquationsUsingNeuralNetworks.pdf},
	language = {en},
	author = {Chiaramonte, M M and Kiener, M},
	year = {2013},
	file = {Chiaramonte and Kiener - 2013 - Solving differential equations using neural networ.pdf:/home/x/Library/library-ml-zotero/storage/785JUM55/Chiaramonte and Kiener - 2013 - Solving differential equations using neural networ.pdf:application/pdf},
}

@article{rudd_solving_2013,
	title = {Solving {Partial} {Differential} {Equations} {Using} {Artificial} {Neural} {Networks}},
	copyright = {Thesis},
	shorttitle = {{PDE} \& {ANN}},
	url = {https://dukespace.lib.duke.edu/dspace/handle/10161/8197},
	language = {en},
	author = {Rudd, Keith},
	year = {2013},
	file = {Rudd - 2013 - Solving Partial Differential Equations Using Artif.pdf:/home/x/Library/library-ml-zotero/storage/CWQXRGKL/Rudd - 2013 - Solving Partial Differential Equations Using Artif.pdf:application/pdf},
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {Tensorflow library},
	url = {https://arxiv.org/abs/1605.08695},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataﬂow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataﬂow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives ﬂexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataﬂow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	language = {en},
	author = {Abadi, Martın and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	file = {Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:/home/x/Library/library-ml-zotero/storage/3ZZSB442/Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}

@article{sirignano_dgm_2018,
	title = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
	volume = {375},
	copyright = {DGM algorithm},
	issn = {00219991},
	shorttitle = {{DGM} algorithm},
	url = {http://arxiv.org/abs/1708.07469},
	doi = {10.1016/j.jcp.2018.08.029},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve highdimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the diﬀerential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE and Burgers’ equation. The deep learning algorithm approximates the general solution to the Burgers’ equation for a continuum of diﬀerent boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = dec,
	year = {2018},
	keywords = {Statistics - Machine Learning, Mathematics - Numerical Analysis, Quantitative Finance - Computational Finance, Quantitative Finance - Mathematical Finance},
	pages = {1339--1364},
	file = {Sirignano and Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:/home/x/Library/library-ml-zotero/storage/SY9JSX9Q/Sirignano and Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:application/pdf},
}

@article{long_pde-net_2018,
	title = {{PDE}-{Net}: {Learning} {PDEs} from {Data}},
	shorttitle = {{PDENet} model},
	url = {https://arxiv.org/abs/1710.09668},
	abstract = {Partial differential equations (PDEs) play a prominent role in many disciplines of science and engineering. PDEs are commonly derived based on empirical observations. However, with the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efﬁciently stored. Such vast quantity of data offers new opportunities for data-driven discovery of physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDENet, to fulﬁll two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. Comparing with existing approaches, our approach has the most ﬂexibility by learning both differential operators and the nonlinear response function of the underlying PDE model. A special feature of the proposed PDE-Net is that all ﬁlters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of ﬁlters (an important concept originated from wavelet theory). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.},
	language = {en},
	author = {Long, Zichao and Lu, Yiping and Ma, Xianzhong and Dong, Bin},
	year = {2018},
	file = {Long et al. - 2018 - PDE-Net Learning PDEs from Data.pdf:/home/x/Library/library-ml-zotero/storage/TRKVN9RK/Long et al. - 2018 - PDE-Net Learning PDEs from Data.pdf:application/pdf},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {00219991},
	shorttitle = {Raissi inverse problems},
	url = {https://doi.org/10.1016/j.jcp.2018.10.045},
	doi = {10.1016/j.jcp.2018.10.045},
	language = {en},
	urldate = {2023-04-28},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
	month = feb,
	year = {2019},
	pages = {686--707},
	file = {Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf:/home/x/Library/library-ml-zotero/storage/JRKAET65/Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf:application/pdf},
}

@article{al-aradi_solving_2018,
	title = {Solving {Nonlinear} and {High}-{Dimensional} {Partial} {Differential} {Equations} via {Deep} {Learning}},
	copyright = {Book},
	shorttitle = {{PDE} {Book}},
	url = {http://utstat.toronto.edu/~ali/papers/PDEandDeepLearning.pdf},
	language = {en},
	author = {Al-Aradi, Ali and Correia, Adolfo and Naiff, Danilo and Jardim, Gabriel and Vargas, Fundacao Getulio and Saporito, Yuri},
	year = {2018},
	file = {Al-Aradi et al. - Solving Nonlinear and High-Dimensional Partial Dif.pdf:/home/x/Library/library-ml-zotero/storage/BIY4K6GY/Al-Aradi et al. - Solving Nonlinear and High-Dimensional Partial Dif.pdf:application/pdf},
}

@misc{dwivedi_distributed_2019,
	title = {Distributed physics informed neural network for data-efficient solution to partial differential equations},
	copyright = {Distributed PINN},
	shorttitle = {Distributed {PINN}},
	url = {http://arxiv.org/abs/1907.08967},
	abstract = {The physics informed neural network (PINN) is evolving as a viable method to solve partial diﬀerential equations. In the recent past PINNs have been successfully tested and validated to ﬁnd solutions to both linear and non-linear partial diﬀerential equations (PDEs). However, the literature lacks detailed investigation of PINNs in terms of their representation capability. In this work, we ﬁrst test the original PINN method in terms of its capability to represent a complicated function. Further, to address the shortcomings of the PINN architecture, we propose a novel distributed PINN, named DPINN. We ﬁrst perform a direct comparison of the proposed DPINN approach against PINN to solve a non-linear PDE (Burgers’ equation). We show that DPINN not only yields a more accurate solution to the Burgers’ equation, but it is found to be more data-eﬃcient as well. At last, we employ our novel DPINN to two-dimensional steady-state Navier-Stokes equation, which is a system of non-linear PDEs. To the best of the authors’ knowledge, this is the ﬁrst such attempt to directly solve the Navier-Stokes equation using a physics informed neural network.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Dwivedi, Vikas and Parashar, Nishant and Srinivasan, Balaji},
	month = jul,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
	file = {Dwivedi et al. - 2019 - Distributed physics informed neural network for da.pdf:/home/x/Library/library-ml-zotero/storage/HFANDIYD/Dwivedi et al. - 2019 - Distributed physics informed neural network for da.pdf:application/pdf},
}

@misc{hasan_learning_2019,
	title = {Learning {Partial} {Differential} {Equations} from {Data} {Using} {Neural} {Networks}},
	shorttitle = {{ODE} \& {ANN}},
	url = {http://arxiv.org/abs/1910.10262},
	abstract = {We develop a framework for estimating unknown partial differential equations (PDEs) from noisy data, using a deep learning approach. Given noisy samples of a solution to an unknown PDE, our method interpolates the samples using a neural network, and extracts the PDE by equating derivatives of the neural network approximation. Our method applies to PDEs which are linear combinations of user-deﬁned dictionary functions, and generalizes previous methods that only consider parabolic PDEs. We introduce a regularization scheme that prevents the function approximation from overﬁtting the data and forces it to be a solution of the underlying PDE. We validate the model on simulated data generated by the known PDEs and added Gaussian noise, and we study our method under different levels of noise. We also compare the error of our method with a Cramer-Rao lower bound for an ordinary differential equation (ODE). Our results indicate that our method outperforms other methods in estimating PDEs, especially in the low signal-to-noise (SNR) regime.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Hasan, Ali and Pereira, João M. and Ravier, Robert and Farsiu, Sina and Tarokh, Vahid},
	month = oct,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Hasan et al. - 2019 - Learning Partial Differential Equations from Data .pdf:/home/x/Library/library-ml-zotero/storage/93LRSDEQ/Hasan et al. - 2019 - Learning Partial Differential Equations from Data .pdf:application/pdf},
}

@article{lu_deepxde_2020,
	title = {{DeepXDE}: {A} {Deep} {Learning} {Library} for {Solving} {Forward} and {Inverse} {Differential} {Equations}},
	shorttitle = {{DeepXDE} {PINN} {Library} (2)},
	url = {https://lululxvi.github.io/files/talks/2020SIAMMDS_MS70.pdf},
	language = {en},
	journal = {Opt Express},
	author = {Lu, Lu},
	year = {2020},
	file = {2020 DeepXDE (slides) - Lu.pdf:/home/x/Library/library-ml-zotero/storage/B9FJSCVJ/2020 DeepXDE (slides) - Lu.pdf:application/pdf;Lu - 2020 - DeepXDE A Deep Learning Library for Solving Forwa.pdf:/home/x/Library/library-ml-zotero/storage/S5VWGMPR/Lu - 2020 - DeepXDE A Deep Learning Library for Solving Forwa.pdf:application/pdf},
}

@misc{xu_distributed_2020,
	title = {Distributed {Machine} {Learning} for {Computational} {Engineering} using {MPI}},
	copyright = {NN, PDE, MPI},
	shorttitle = {{ANN}, {PDE}, and {MPI}},
	url = {http://arxiv.org/abs/2011.01349},
	abstract = {We propose a framework for training neural networks that are coupled with partial diﬀerential equations (PDEs) in a parallel computing environment. Unlike most distributed computing frameworks for deep neural networks, our focus is to parallelize both numerical solvers and deep neural networks in forward and adjoint computations. Our parallel computing model views data communication as a node in the computational graph for numerical simulations. The advantage of our model is that data communication and computing are cleanly separated and thus provide better ﬂexibility, modularity, and testability. We demonstrate using various large-scale problems that we can achieve substantial acceleration by using parallel solvers for PDEs in training deep neural networks that are coupled with PDEs.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Xu, Kailai and Zhu, Weiqiang and Darve, Eric},
	month = nov,
	year = {2020},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Xu et al. - 2020 - Distributed Machine Learning for Computational Eng.pdf:/home/x/Library/library-ml-zotero/storage/4VZ3P2Z5/Xu et al. - 2020 - Distributed Machine Learning for Computational Eng.pdf:application/pdf},
}

@misc{chen_learning_2021,
	title = {Learning {Neural} {Event} {Functions} for {Ordinary} {Differential} {Equations}},
	shorttitle = {Neural {Event} {ODE}},
	url = {http://arxiv.org/abs/2011.03902},
	abstract = {The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly deﬁned termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete and instantaneous changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discrete- and continuous- systems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Amos, Brandon and Nickel, Maximilian},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chen et al. - 2021 - Learning Neural Event Functions for Ordinary Diffe.pdf:/home/x/Library/library-ml-zotero/storage/G7HBIGXW/Chen et al. - 2021 - Learning Neural Event Functions for Ordinary Diffe.pdf:application/pdf},
}

@article{karniadakis_physics-informed_2021,
	title = {Physics-informed {Machine} {Learning}},
	volume = {3},
	issn = {2522-5820},
	shorttitle = {Physics-informed {ML}},
	url = {https://www.nature.com/articles/s42254-021-00314-5},
	doi = {10.1038/s42254-021-00314-5},
	abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-d imensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-t ime domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-b ased regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-d imensional problems.},
	language = {en},
	number = {6},
	urldate = {2023-04-28},
	journal = {Nature Reviews Physics},
	author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	month = may,
	year = {2021},
	pages = {422--440},
	file = {Karniadakis et al. - 2021 - Physics-informed machine learning.pdf:/home/x/Library/library-ml-zotero/storage/JN2EGE8J/Karniadakis et al. - 2021 - Physics-informed machine learning.pdf:application/pdf},
}

@misc{markidis_old_2021,
	title = {The {Old} and the {New}: {Can} {Physics}-{Informed} {Deep}-{Learning} {Replace} {Traditional} {Linear} {Solvers}?},
	shorttitle = {{PINN} to replace linear solvers},
	url = {http://arxiv.org/abs/2103.09655},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks encoding the problem governing equations, such as Partial Differential Equations (PDE), as a part of the neural network. PINNs have emerged as a new essential tool to solve various challenging problems, including computing linear systems arising from PDEs, a task for which several traditional methods exist. In this work, we focus first on evaluating the potential of PINNs as linear solvers in the case of the Poisson equation, an omnipresent equation in scientific computing. We characterize PINN linear solvers in terms of accuracy and performance under different network configurations (depth, activation functions, input data set distribution). We highlight the critical role of transfer learning. Our results show that low-frequency components of the solution converge quickly as an effect of the F-principle. In contrast, an accurate solution of the high frequencies requires an exceedingly long time. To address this limitation, we propose integrating PINNs into traditional linear solvers. We show that this integration leads to the development of new solvers whose performance is on par with other high-performance solvers, such as PETSc conjugate gradient linear solvers, in terms of performance and accuracy. Overall, while the accuracy and computational performance are still a limiting factor for the direct use of PINN linear solvers, hybrid strategies combining old traditional linear solver approaches with new emerging deep-learning techniques are among the most promising methods for developing a new class of linear solvers.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Markidis, Stefano},
	month = jul,
	year = {2021},
	keywords = {Physics - Computational Physics, Mathematics - Numerical Analysis, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/HQMITYQ8/2103.html:text/html;Markidis - 2021 - The Old and the New Can Physics-Informed Deep-Lea.pdf:/home/x/Library/library-ml-zotero/storage/L9HLM5XW/Markidis - 2021 - The Old and the New Can Physics-Informed Deep-Lea.pdf:application/pdf},
}

@article{wang_eigenvector_2021,
	title = {On the eigenvector bias of {Fourier} feature networks: {From} regression to solving multi-scale {PDEs} with physics-informed neural networks},
	volume = {384},
	copyright = {Eigenvector Bias of Fourier},
	issn = {00457825},
	shorttitle = {Eigenvector {Bias} of {Fourier}},
	url = {http://arxiv.org/abs/2012.10047},
	doi = {10.1016/j.cma.2021.113938},
	abstract = {Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction-diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/ PredictiveIntelligenceLab/MultiscalePINNs.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {113938},
	file = {Wang et al. - 2021 - On the eigenvector bias of Fourier feature network.pdf:/home/x/Library/library-ml-zotero/storage/5J73HG28/Wang et al. - 2021 - On the eigenvector bias of Fourier feature network.pdf:application/pdf},
}

@article{niaki_physics-informed_2021,
	title = {Physics-{Informed} {Neural} {Network} for {Modelling} the {Thermochemical} {Curing} {Process} of {Composite}-{Tool} {Systems} {During} {Manufacture}},
	volume = {384},
	copyright = {Thermochemical},
	issn = {00457825},
	shorttitle = {{PINN} {Thermochemical}},
	url = {http://arxiv.org/abs/2011.13511},
	doi = {10.1016/j.cma.2021.113959},
	abstract = {We present a Physics-Informed Neural Network (PINN) to simulate the thermochemical evolution of a composite material on a tool undergoing cure in an autoclave. In particular, we solve the governing coupled system of diﬀerential equations—including conductive heat transfer and resin cure kinetics—by optimizing the parameters of a deep neural network (DNN) using a physics-based loss function. To account for the vastly diﬀerent behaviour of thermal conduction and resin cure, we design a PINN consisting of two disconnected subnetworks, and develop a sequential training algorithm that mitigates instability present in traditional training methods. Further, we incorporate explicit discontinuities into the DNN at the composite-tool interface and enforce known physical behaviour directly in the loss function to improve the solution near the interface. We train the PINN with a technique that automatically adapts the weights on the loss terms corresponding to PDE, boundary, interface, and initial conditions. Finally, we demonstrate that one can include problem parameters as an input to the model—resulting in a surrogate that provides real-time simulation for a range of problem settings—and that one can use transfer learning to signiﬁcantly reduce the training time for problem settings similar to that of an initial trained model. The performance of the proposed PINN is demonstrated in multiple scenarios with diﬀerent material thicknesses and thermal boundary conditions.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Niaki, Sina Amini and Haghighat, Ehsan and Campbell, Trevor and Poursartip, Anoush and Vaziri, Reza},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Machine Learning, 35Q79, 74A40, 68T07, 65M22,, J.2},
	pages = {113959},
	file = {Niaki et al. - 2021 - Physics-Informed Neural Network for Modelling the .pdf:/home/x/Library/library-ml-zotero/storage/U4E4ZMP3/Niaki et al. - 2021 - Physics-Informed Neural Network for Modelling the .pdf:application/pdf},
}

@article{feng_physics-informed_2023,
	title = {Physics-informed neural networks of the {Saint}-{Venant} equations for downscaling a large-scale river model},
	volume = {59},
	copyright = {Saint-Venant},
	issn = {0043-1397, 1944-7973},
	shorttitle = {Saint-{Venant}},
	url = {http://arxiv.org/abs/2210.03240},
	doi = {10.1029/2022WR033168},
	abstract = {Large-scale river models are being reﬁned over coastal regions to improve the scientiﬁc understanding of coastal processes, hazards and responses to climate change. However, coarse mesh resolutions and approximations in physical representations of tidal rivers limit the performance of such models at resolving the complex ﬂow dynamics especially near the river-ocean interface, resulting in inaccurate simulations of ﬂood inundation. In this research, we propose a machine learning (ML) framework based on the state-of-the-art physics-informed neural network (PINN) to simulate the downscaled ﬂow at the subgrid scale. First, we demonstrate that PINN is able to assimilate observations of various types and solve the one-dimensional (1-D) Saint-Venant equations (SVE) directly. We perform the ﬂow simulations over a ﬂoodplain and along an open channel in several synthetic case studies. The PINN performance is evaluated against analytical solutions and numerical models. Our results indicate that the PINN solutions of water depth have satisfactory accuracy with limited observations assimilated. In the case of ﬂood wave propagation induced by storm surge and tide, a new neural network architecture is proposed based on Fourier feature embeddings that seamlessly encodes the periodic tidal boundary condition in the PINN’s formulation. Furthermore, we show that the PINNbased downscaling can produce more reasonable subgrid solutions of the along-channel water depth by assimilating observational data. The PINN solution outperforms the simple linear interpolation in resolving the topography and dynamic ﬂow regimes at the subgrid scale. This study provides a promising path towards improving emulation capabilities in large-scale models to characterize ﬁne-scale coastal processes.},
	language = {en},
	number = {2},
	urldate = {2023-04-28},
	journal = {Water Resources Research},
	author = {Feng, Dongyu and Tan, Zeli and He, QiZhi},
	month = feb,
	year = {2023},
	keywords = {Physics - Atmospheric and Oceanic Physics, Physics - Fluid Dynamics},
	file = {Feng et al. - 2023 - Physics-informed neural networks of the Saint-Vena.pdf:/home/x/Library/library-ml-zotero/storage/ISHBQBBB/Feng et al. - 2023 - Physics-informed neural networks of the Saint-Vena.pdf:application/pdf},
}

@misc{das_state---art_2022,
	title = {State-of-the-{Art} {Review} of {Design} of {Experiments} for {Physics}-{Informed} {Deep} {Learning}},
	copyright = {Review},
	shorttitle = {{PINN} state-of-the-art review},
	abstract = {This paper presents a comprehensive review of the design of experiments used in the surrogate models. In particular, this study demonstrates the necessity of the design of experiment schemes for the Physics-Informed Neural Network (PINN), which belongs to the supervised learning class. Many complex partial diﬀerential equations (PDEs) do not have any analytical solution; only numerical methods are used to solve the equations, which is computationally expensive. In recent decades, PINN has gained popularity as a replacement for numerical methods to reduce the computational budget. PINN uses physical information in the form of diﬀerential equations to enhance the performance of the neural networks. Though it works eﬃciently, the choice of the design of experiment scheme is important as the accuracy of the predicted responses using PINN depends on the training data. In this study, ﬁve diﬀerent PDEs are used for numerical purposes, i.e., viscous Burger’s equation, Shro¨dinger equation, heat equation, Allen-Cahn equation, and Korteweg-de Vries equation. A comparative study is performed to establish the necessity of the selection of a DoE scheme. It is seen that the Hammersley sampling-based PINN performs better than other DoE sample strategies.},
	language = {en},
	publisher = {arXiv},
	author = {Das, Sourav and Tesfamariam, Solomon},
	month = feb,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Das and Tesfamariam - 2022 - State-of-the-Art Review of Design of Experiments f.pdf:/home/x/Library/library-ml-zotero/storage/QF4QLMJC/Das and Tesfamariam - 2022 - State-of-the-Art Review of Design of Experiments f.pdf:application/pdf},
}

@article{lucor_simple_2022,
	title = {Simple computational strategies for more effective physics-informed neural networks modeling of turbulent natural convection},
	volume = {456},
	copyright = {Convection},
	issn = {00219991},
	shorttitle = {Convection {PINN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999122000845},
	doi = {10.1016/j.jcp.2022.111022},
	abstract = {Recent works have explored the potential of machine learning as data-driven turbulence closures for RANS and LES techniques. Beyond these advances, the high expressivity and agility of physics-informed neural networks (PINNs) make them promising candidates for full ﬂuid ﬂow PDE modeling. An important question is whether this new paradigm, exempt from the traditional notion of discretization of the underlying operators very much connected to the ﬂow scales resolution, is capable of sustaining high levels of turbulence characterized by multi-scale features? We investigate the use of PINNs surrogate modeling for turbulent Rayleigh-B´enard (RB) convection ﬂows in rough and smooth rectangular cavities, mainly relying on DNS temperature data from the ﬂuid bulk. We carefully quantify the computational requirements under which the formulation is capable of accurately recovering the ﬂow hidden quantities. We then propose a new padding technique to distribute some of the scattered coordinates - at which PDE residuals are minimized - around the region of labeled data acquisition. We show how it comes to play as a regularization close to the training boundaries which are zones of poor accuracy for standard PINNs and results in a noticeable global accuracy improvement at iso-budget. Finally, we propose for the ﬁrst time to relax the incompressibility condition in such a way that it drastically beneﬁts the optimization search and results in a much improved convergence of the composite loss function. The RB results obtained at high Rayleigh number Ra = 2 · 109 are particularly impressive: the predictive accuracy of the surrogate over the entire half a billion DNS coordinates yields errors for all ﬂow variables ranging between [0.3\% − 4\%] in the relative L2 norm, with a training relying only on 1.6\% of the DNS data points.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Journal of Computational Physics},
	author = {Lucor, Didier and Agrawal, Atul and Sergent, Anne},
	month = may,
	year = {2022},
	pages = {111022},
	file = {Lucor et al. - 2022 - Simple computational strategies for more effective.pdf:/home/x/Library/library-ml-zotero/storage/B5KNX8RF/Lucor et al. - 2022 - Simple computational strategies for more effective.pdf:application/pdf},
}

@book{nocedal_numerical_2006,
	address = {New York},
	edition = {2nd ed},
	series = {Springer series in operations research},
	title = {Numerical {Optimization}},
	copyright = {Book},
	isbn = {978-0-387-30303-1},
	shorttitle = {{BFGS} {Method}},
	url = {http://www.ime.unicamp.br/~pulino/MT404/TextosOnline/NocedalJ.pdf},
	language = {en},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {2006},
	note = {OCLC: ocm68629100},
	keywords = {Mathematical optimization},
	file = {Nocedal and Wright - 2006 - Numerical optimization.pdf:/home/x/Library/library-ml-zotero/storage/JHIHCI26/Nocedal and Wright - 2006 - Numerical optimization.pdf:application/pdf},
}

@article{sukumar_exact_2022,
	title = {Exact imposition of boundary conditions with distance functions in physics-informed deep neural networks},
	volume = {389},
	issn = {00457825},
	shorttitle = {{PINN} boundary conditions},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782521006186},
	doi = {10.1016/j.cma.2021.114333},
	abstract = {In this paper, we introduce a new approach based on distance ﬁelds to exactly impose boundary conditions in physicsinformed deep neural networks. The challenges in satisfying Dirichlet boundary conditions in meshfree and particle methods are well-known. This issue is also pertinent in the development of physics informed neural networks (PINN) for the solution of partial diﬀerential equations. We introduce geometry-aware trial functions in artiﬁcal neural networks to improve the training in deep learning for partial diﬀerential equations. To this end, we use concepts from constructive solid geometry (R-functions) and generalized barycentric coordinates (mean value potential ﬁelds) to construct φ(x), an approximate distance function to the boundary of a domain in Rd. To exactly impose homogeneous Dirichlet boundary conditions, the trial function is taken as φ(x) multiplied by the PINN approximation, and its generalization via transﬁnite interpolation is used to a priori satisfy inhomogeneous Dirichlet (essential), Neumann (natural), and Robin boundary conditions on complex geometries. In doing so, we eliminate modeling error associated with the satisfaction of boundary conditions in a collocation method and ensure that kinematic admissibility is met pointwise in a Ritz method. With this new ansatz, the training for the neural network is simpliﬁed: sole contribution to the loss function is from the residual error at interior collocation points where the governing equation is required to be satisﬁed. Numerical solutions are computed using strong form collocation and Ritz minimization. To convey the main ideas and to assess the accuracy of the approach, we present numerical solutions for linear and nonlinear boundary-value problems over convex and nonconvex polygonal domains as well as over domains with curved boundaries. Benchmark problems in one dimension for linear elasticity, advection-diﬀusion, and beam bending; and in two dimensions for the steady-state heat equation, Laplace equation, biharmonic equation (Kirchhoﬀ plate bending), and the nonlinear Eikonal equation are considered. The construction of approximate distance functions using R-functions extends to higher dimensions, and we showcase its use by solving a Poisson problem with homogeneous Dirichlet boundary conditions over the four-dimensional hypercube. The proposed approach consistently outperforms a standard PINN-based collocation method, which underscores the importance of exactly (a priori) satisfying the boundary condition when constructing a loss function in PINN. This study provides a pathway for meshfree analysis to be conducted on the exact geometry without domain discretization.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Sukumar, N. and Srivastava, Ankit},
	month = feb,
	year = {2022},
	pages = {114333},
	file = {Sukumar and Srivastava - 2022 - Exact imposition of boundary conditions with dista.pdf:/home/x/Library/library-ml-zotero/storage/4FW4XVSQ/Sukumar and Srivastava - 2022 - Exact imposition of boundary conditions with dista.pdf:application/pdf},
}

@article{ma_preliminary_2022,
	title = {A {Preliminary} {Study} on the {Resolution} of {Electro}-{Thermal} {Multi}-{Physics} {Coupling} {Problem} {Using} {Physics}-{Informed} {Neural} {Network} ({PINN})},
	volume = {15},
	copyright = {With heat sources},
	issn = {1999-4893},
	shorttitle = {Electro-{Thermal} {PINN}},
	url = {https://www.mdpi.com/1999-4893/15/2/53},
	doi = {10.3390/a15020053},
	abstract = {The problem of electro-thermal coupling is widely present in the integrated circuit (IC). The accuracy and efﬁciency of traditional solution methods, such as the ﬁnite element method (FEM), are tightly related to the quality and density of mesh construction. Recently, PINN (physics-informed neural network) was proposed as a method for solving differential equations. This method is mesh free and generalizes the process of solving PDEs regardless of the equations’ structure. Therefore, an experiment is conducted to explore the feasibility of PINN in solving electro-thermal coupling problems, which include the electrokinetic ﬁeld and steady-state thermal ﬁeld. We utilize two neural networks in the form of sequential training to approximate the electric ﬁeld and the thermal ﬁeld, respectively. The experimental results show that PINN provides good accuracy in solving electro-thermal coupling problems.},
	language = {en},
	number = {2},
	urldate = {2023-04-28},
	journal = {Algorithms},
	author = {Ma, Yaoyao and Xu, Xiaoyu and Yan, Shuai and Ren, Zhuoxiang},
	month = feb,
	year = {2022},
	keywords = {deep learning, electro-thermal coupling, PDEs, physics-informed neural network},
	pages = {53},
	file = {Ma et al. - 2022 - A Preliminary Study on the Resolution of Electro-T.pdf:/home/x/Library/library-ml-zotero/storage/C5JH25BP/Ma et al. - 2022 - A Preliminary Study on the Resolution of Electro-T.pdf:application/pdf},
}

@article{katsikis_gentle_2022,
	title = {A {Gentle} {Introduction} to {Physics}-{Informed} {Neural} {Networks}, with {Applications} in {Static} {Rod} and {Beam} {Problems}},
	volume = {9},
	issn = {2409-5761},
	shorttitle = {{PINN} {Static} {Rod} and {Beam}},
	url = {https://avantipublishers.com/index.php/jaacm/article/view/1246},
	doi = {10.15377/2409-5761.2022.09.8},
	abstract = {A modern approach to solving mathematical models involving differential equations, the so-called Physics-Informed Neural Network (PINN), is based on the techniques which include the use of artificial neural networks and the method of fitting the governing differential equations at collocation points. In this paper, training of the PINN with an application of optimization techniques is performed on simple onedimensional mechanical problems of elasticity, namely rods and beams. Different boundary conditions are considered.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Journal of Advances in Applied \& Computational Mathematics},
	author = {Katsikis, Dimitrios and Muradova, Aliki D. and Stavroulakis, Georgios E.},
	month = may,
	year = {2022},
	pages = {103--128},
	file = {Katsikis et al. - 2022 - A Gentle Introduction to Physics-Informed Neural N.pdf:/home/x/Library/library-ml-zotero/storage/H4LPNVKC/Katsikis et al. - 2022 - A Gentle Introduction to Physics-Informed Neural N.pdf:application/pdf},
}

@inproceedings{martin_reinforcement_2022,
	address = {San Diego, CA \& Virtual},
	title = {Reinforcement {Learning} and {Orbit}-{Discovery} {Enhanced} by {Small}-{Body} {Physics}-{Informed} {Neural} {Network} {Gravity} {Models}},
	isbn = {978-1-62410-631-6},
	shorttitle = {{PINN} gravity model},
	url = {https://arc.aiaa.org/doi/10.2514/6.2022-2272},
	doi = {10.2514/6.2022-2272},
	language = {en},
	urldate = {2023-04-28},
	booktitle = {{AIAA} {SCITECH} 2022 {Forum}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Martin, John and Schaub, Hanspeter},
	month = jan,
	year = {2022},
	file = {Martin and Schaub - 2022 - Reinforcement Learning and Orbit-Discovery Enhance.pdf:/home/x/Library/library-ml-zotero/storage/VUGXYVAN/Martin and Schaub - 2022 - Reinforcement Learning and Orbit-Discovery Enhance.pdf:application/pdf},
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	shorttitle = {{ODE} \& deep learning},
	url = {https://pnas.org/doi/full/10.1073/pnas.1718942115},
	doi = {10.1073/pnas.1718942115},
	abstract = {Significance
            Partial differential equations (PDEs) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional PDEs has been notoriously difficult due to the “curse of dimensionality.” This paper introduces a practical algorithm for solving nonlinear PDEs in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.
          , 
            Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the “curse of dimensionality.” This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black–Scholes equation, the Hamilton–Jacobi–Bellman equation, and the Allen–Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.},
	language = {en},
	number = {34},
	urldate = {2023-04-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	month = aug,
	year = {2018},
	pages = {8505--8510},
	file = {Han et al. - 2018 - Solving high-dimensional partial differential equa.pdf:/home/x/Library/library-ml-zotero/storage/BLFE32CV/Han et al. - 2018 - Solving high-dimensional partial differential equa.pdf:application/pdf},
}

@article{cai_physics-informed_2021,
	title = {Physics-{Informed} {Neural} {Networks} for {Heat} {Transfer} {Problems}},
	volume = {143},
	issn = {0022-1481},
	shorttitle = {{PINN} for heat transfer},
	url = {https://doi.org/10.1115/1.4050542},
	doi = {10.1115/1.4050542},
	abstract = {Physics-informed neural networks (PINNs) have gained popularity across different engineering fields due to their effectiveness in solving realistic problems with noisy data and often partially missing physics. In PINNs, automatic differentiation is leveraged to evaluate differential operators without discretization errors, and a multitask learning problem is defined in order to simultaneously fit observed data while respecting the underlying governing laws of physics. Here, we present applications of PINNs to various prototype heat transfer problems, targeting in particular realistic conditions not readily tackled with traditional computational methods. To this end, we first consider forced and mixed convection with unknown thermal boundary conditions on the heated surfaces and aim to obtain the temperature and velocity fields everywhere in the domain, including the boundaries, given some sparse temperature measurements. We also consider the prototype Stefan problem for two-phase flow, aiming to infer the moving interface, the velocity and temperature fields everywhere as well as the different conductivities of a solid and a liquid phase, given a few temperature measurements inside the domain. Finally, we present some realistic industrial applications related to power electronics to highlight the practicality of PINNs as well as the effective use of neural networks in solving general heat transfer problems of industrial complexity. Taken together, the results presented herein demonstrate that PINNs not only can solve ill-posed problems, which are beyond the reach of traditional computational methods, but they can also bridge the gap between computational and experimental heat transfer.},
	number = {6},
	urldate = {2023-04-28},
	journal = {Journal of Heat Transfer},
	author = {Cai, Shengze and Wang, Zhicheng and Wang, Sifan and Perdikaris, Paris and Karniadakis, George Em},
	month = apr,
	year = {2021},
	file = {Cai et al. - 2021 - Physics-Informed Neural Networks for Heat Transfer.pdf:/home/x/Library/library-ml-zotero/storage/SQ2FCBZM/Cai et al. - 2021 - Physics-Informed Neural Networks for Heat Transfer.pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/Q23CTBDY/Physics-Informed-Neural-Networks-for-Heat-Transfer.html:text/html},
}

@phdthesis{alhubail_application_2021,
	type = {Thesis},
	title = {Application of {Physics}-{Informed} {Neural} {Networks} to {Solve} 2-{D} {Single}-phase {Flow} in {Heterogeneous} {Porous} {Media}},
	copyright = {Thesis},
	shorttitle = {{PINN} {2D} single-phase flow},
	url = {https://repository.kaust.edu.sa/handle/10754/670174},
	abstract = {Neural networks have recently seen tremendous advancements in applicability in
many areas, one of which is their utilization in solving physical problems governed by
partial differential equations and the constraints of these equations. Physics-informed
neural networks is the name given to such neural networks. They are different from
typical neural networks in that they include loss terms that represent the physics
of the problem. These terms often include partial derivatives of the neural network
outputs with respect to its inputs, and these derivatives are found through the use of
automatic differentiation.
The purpose of this thesis is to showcase the ability of physics-informed neural
networks to solve basic fluid flow problems in homogeneous and heterogeneous porous
media. This is done through the utilization of the pressure equation under a set of
assumptions as well as the inclusion of Dirichlet and Neumann boundary conditions.
The goal is to create a surrogate model that allows for finding the pressure and
velocity profiles everywhere inside the domain of interest.
In the homogeneous case, minimization of the loss function that included the
boundary conditions term and the partial differential equation term allowed for producing
results that show good agreement with the results from a numerical simulator.
However, in the case of heterogeneous media where there are sharp discontinuities in
hydraulic conductivity inside the domain, the model failed to produce accurate results.
To resolve this issue, extended physics-informed neural networks were used.
This method involves the decomposition of the domain into multiple homogeneous
sub-domains. Each sub-domain has its own physics informed neural network structure,
equation parameters, and equation constraints. To allow the sub-domains to
communicate, interface conditions are placed on the interfaces that separate the different
sub-domains. The results from this method matched well with the results of
the simulator. In both the homogeneous and heterogeneous cases, neural networks
with only one hidden layer with thirty nodes were used. Even with this simple structure
for the neural networks, the computations are expensive and a large number of
training iterations is required to converge.},
	language = {en},
	urldate = {2023-04-28},
	author = {Alhubail, Ali},
	month = jul,
	year = {2021},
	doi = {10.25781/KAUST-37Z86},
	file = {Full Text PDF:/home/x/Library/library-ml-zotero/storage/ZJXYNTV5/Alhubail - 2021 - Application of Physics-Informed Neural Networks to.pdf:application/pdf},
}

@article{zhang_data_2020,
	title = {Data {Driven} {Solutions} and {Discoveries} in {Mechanics} {Using} {Physics} {Informed} {Neural} {Network}},
	copyright = {Heat 2D},
	shorttitle = {{PINN} {Mechanics}},
	url = {https://www.preprints.org/manuscript/202006.0258/v1},
	doi = {10.20944/preprints202006.0258.v1},
	abstract = {Deep learning has achieved remarkable success in diverse computer science applications, however, its use in other traditional engineering fields has emerged only recently. In this project, we solved several mechanics problems governed by differential equations, using physics informed neural networks (PINN). The PINN embeds the differential equations into the loss of the neural network using automatic differentiation. We present our developments in the context of solving two main classes of problems: data-driven solutions and data-driven discoveries, and we compare the results with either analytical solutions or numerical solutions using the finite element method. The remarkable achievements of the PINN model shown in this report suggest the bright prospect of the physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters. More broadly, this study shows that PINN provides an attractive alternative to solve traditional engineering problems.},
	urldate = {2023-04-28},
	author = {Zhang, Qi and Chen, Yilin and Yang, Ziyi},
	month = jun,
	year = {2020},
	file = {Zhang et al. - 2020 - Data Driven Solutions and Discoveries in Mechanics.pdf:/home/x/Library/library-ml-zotero/storage/Y9K69FD9/Zhang et al. - 2020 - Data Driven Solutions and Discoveries in Mechanics.pdf:application/pdf},
}

@article{lagaris_artificial_1998,
	title = {Artificial {Neural} {Networks} for {Solving} {Ordinary} and {Partial} {Differential} {Equations}},
	volume = {9},
	issn = {10459227},
	shorttitle = {{ANN} \& {PDE}},
	url = {http://arxiv.org/abs/physics/9705023},
	doi = {10.1109/72.712178},
	abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the boundary (or initial) conditions and contains no adjustable parameters. The second part is constructed so as not to affect the boundary conditions. This part involves a feedforward neural network, containing adjustable parameters (the weights). Hence by construction the boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ODE's, to systems of coupled ODE's and also to PDE's. In this article we illustrate the method by solving a variety of model problems and present comparisons with finite elements for several cases of partial differential equations.},
	number = {5},
	urldate = {2023-04-28},
	journal = {IEEE Transactions on Neural Networks},
	author = {Lagaris, I. E. and Likas, A. and Fotiadis, D. I.},
	month = sep,
	year = {1998},
	keywords = {Physics - Computational Physics, Nonlinear Sciences - Cellular Automata and Lattice Gases, Quantum Physics},
	pages = {987--1000},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/NLE728IW/9705023.html:text/html;Lagaris et al. - 1998 - Artificial Neural Networks for Solving Ordinary an.pdf:/home/x/Library/library-ml-zotero/storage/JZWJLJKI/Lagaris et al. - 1998 - Artificial Neural Networks for Solving Ordinary an.pdf:application/pdf},
}

@unpublished{jeffrey_wong_lecture_2018,
	title = {Lecture {Notes} on {PDEs}, part {I}: {The} heat equation and the eigenfunction method},
	copyright = {Theory},
	shorttitle = {Heat equation (lecture)},
	url = {https://services.math.duke.edu/~jtwong/},
	author = {{Jeffrey Wong}},
	year = {2018},
	file = {Lecture Notes on PDEs.pdf:/home/x/Library/library-ml-zotero/storage/JESBBLDD/Lecture Notes on PDEs.pdf:application/pdf},
}

@phdthesis{cerqueira_state---art_2021,
	title = {A state-of-the-art of physics-informed neural networks in engineering},
	copyright = {Thesis},
	shorttitle = {{PINN} in engineering},
	url = {http://pantheon.ufrj.br/handle/11422/15774},
	abstract = {Machine learning techniques have gained space in the industrial scenario as a tool to convert the increasing flux of information (data) in process improvement. Among these techniques, neural networks has got much attention due to their universal approximators capacity, of which performance can be improved by providing previous physical knowledge: one has, therefore, the development of the so called Physicsinformed neural networks (PINN). In such context and having noticed a “gap” in the works related on this topics and in the diffusion of this theme in the School of Chemistry, this work proposes a state-of-the-art of the mentioned technique. Particular interesting concerning PINN in fluid mechanics and heat transfer has been noticed. Moreover, PINN have been pointed as important tools for solving forward and inverse problems. Finally, through practical examples, this work has shown the use of neural networks for solving one particular example in chemical engineering without informing the physics of the problem (obtaining the friction factor) and using the differential equation that describes it (solving the 1D heat diffusion equation).},
	language = {eng},
	urldate = {2023-04-28},
	author = {Cerqueira, Pedro Henrique da Silva Singue},
	month = aug,
	year = {2021},
	file = {Cerqueira - 2021 - A state-of-the-art of physics-informed neural netw.pdf:/home/x/Library/library-ml-zotero/storage/BK2J3BU7/Cerqueira - 2021 - A state-of-the-art of physics-informed neural netw.pdf:application/pdf},
}

@article{laneryd_physics_2022,
	series = {10th {Vienna} {International} {Conference} on {Mathematical} {Modelling} {MATHMOD} 2022},
	title = {Physics {Informed} {Neural} {Networks} for {Power} {Transformer} {Dynamic} {Thermal} {Modelling}},
	volume = {55},
	issn = {2405-8963},
	shorttitle = {{PINN} thermal modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896322012526},
	doi = {10.1016/j.ifacol.2022.09.070},
	abstract = {The emerging methodology of Physics Informed Neural Networks (PINNs) promises to combine available data and physical knowledge to achieve high accuracy and fast evaluation. Dynamic thermal modelling of power transformers is an application specifically set to benefit from these characteristics. Data collected during typical operation is not representative of extreme loading scenarios and the number of thermal sensors is limited. The detailed geometry is often not known by the asset owner which creates high uncertainty for physics-based simulation models. In this study, the transformer is modeled by the one-dimensional heat diffusion equation. PINN is constructed with a loss function including both data-based and physics-based terms. A time-dependent source term from a time series of measurement is also part of the PINN. The result is compared with a finite volume solution demonstrating good agreement. The PINN approach will be useful for further development in thermal modelling for power transformers.},
	language = {en},
	number = {20},
	urldate = {2023-04-28},
	journal = {IFAC-PapersOnLine},
	author = {Laneryd, Tor and Bragone, Federica and Morozovska, Kateryna and Luvisotto, Michele},
	month = jan,
	year = {2022},
	keywords = {artificial intelligence for modelling, Comparison of methods, Energy Systems, Environmental systems, Finite Volume Method, Machine learning, Physics-Informed Neural Networks},
	pages = {49--54},
	file = {Laneryd et al. - 2022 (1) - Physics Informed Neural Networks for Power Transfo.pdf:/home/x/Library/library-ml-zotero/storage/ZVLBXVYI/Laneryd et al. - 2022 (1) - Physics Informed Neural Networks for Power Transfo.pdf:application/pdf;Laneryd et al. - 2022 (2) - Physics Informed Neural Networks for Power Transfo.pdf:/home/x/Library/library-ml-zotero/storage/8T6JE5SH/Laneryd et al. - 2022 (2) - Physics Informed Neural Networks for Power Transfo.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/KP92EI5H/S2405896322012526.html:text/html},
}

@unpublished{daileda_two_2012,
	title = {The two dimensional heat equation},
	copyright = {Theory},
	shorttitle = {Heat equation {2D} (lecture)},
	url = {http://ramanujan.math.trinity.edu/rdaileda/teach/s12/m3357/lectures/lecture_3_6_short.pdf},
	author = {Daileda, Ryan C.},
	year = {2012},
	file = {Daileda - The two dimensional heat equation.pdf:/home/x/Library/library-ml-zotero/storage/YACRWCL9/Daileda - The two dimensional heat equation.pdf:application/pdf},
}

@misc{dagrada_introduction_2022,
	title = {Introduction to {Physics}-informed {Neural} {Networks}},
	copyright = {website},
	shorttitle = {Introduction to {PINN} (website)},
	url = {https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4},
	abstract = {A hands-on tutorial with PyTorch},
	language = {en},
	urldate = {2023-04-28},
	journal = {Medium},
	author = {Dagrada, Mario},
	year = {2022},
	note = {Sources: https://github.com/madagra/basic-pinn},
	file = {Dagrada - 2022 - Introduction to Physics-informed Neural Networks.pdf:/home/x/Library/library-ml-zotero/storage/PSXPY2M8/Dagrada - 2022 - Introduction to Physics-informed Neural Networks.pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/R88GZFHH/solving-differential-equations-with-neural-networks-afdcf7b8bcc4.html:text/html},
}

@misc{hannay_differential_2020,
	title = {Differential {Equations} as a {Neural} {Network} {Layer}},
	copyright = {website},
	shorttitle = {{ODA} as {NN} layer (website)},
	url = {https://towardsdatascience.com/differential-equations-as-a-neural-network-layer-ac3092632255},
	abstract = {A first step to adding domain knowledge to your neural network models},
	language = {en},
	urldate = {2023-04-28},
	journal = {Medium},
	author = {Hannay, Kevin},
	month = apr,
	year = {2020},
	file = {2020 DE as a Neural Network Layers - HANNAY.pdf:/home/x/Library/library-ml-zotero/storage/7TCPKV3Q/2020 DE as a Neural Network Layers - HANNAY.pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/QX4VVYR5/differential-equations-as-a-neural-network-layer-ac3092632255.html:text/html},
}

@misc{jacquier_tensorflow_2019,
	title = {{TensorFlow} 2.0 implementation of {Maziar} {Raissi}'s {Physics} {Informed} {Neural} {Networks} ({PINNs})},
	copyright = {Software},
	shorttitle = {{PINN} {Raissi} \& {TF2} (website)},
	url = {https://github.com/pierremtb/PINNs-TF2.0},
	author = {Jacquier, Pierre},
	year = {2019},
}

@article{wu_symmetric_1998,
	title = {Symmetric functional differential equations and neural networks with memory},
	volume = {350},
	issn = {0002-9947, 1088-6850},
	shorttitle = {Symmetric {PINN}},
	url = {https://www.ams.org/tran/1998-350-12/S0002-9947-98-02083-2/},
	doi = {10.1090/S0002-9947-98-02083-2},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	number = {12},
	urldate = {2023-04-28},
	journal = {Transactions of the American Mathematical Society},
	author = {Wu, Jianhong},
	year = {1998},
	keywords = {delay differential equation, equivariant degree, global bifurcation., neural network, Periodic solution, symmetry, wave},
	pages = {4799--4838},
	file = {Full Text PDF:/home/x/Library/library-ml-zotero/storage/PF6YD3IW/Wu - 1998 - Symmetric functional differential equations and ne.pdf:application/pdf},
}

@article{berg_unified_2018,
	title = {A unified deep artificial neural network approach to partial differential equations in complex geometries},
	volume = {317},
	issn = {0925-2312},
	shorttitle = {{PINN} complex geometries},
	url = {https://www.sciencedirect.com/science/article/pii/S092523121830794X},
	doi = {10.1016/j.neucom.2018.06.056},
	abstract = {In this paper, we use deep feedforward artificial neural networks to approximate solutions to partial differential equations in complex geometries. We show how to modify the backpropagation algorithm to compute the partial derivatives of the network output with respect to the space variables which is needed to approximate the differential operator. The method is based on an ansatz for the solution which requires nothing but feedforward neural networks and an unconstrained gradient based optimization method such as gradient descent or a quasi-Newton method. We show an example where classical mesh based methods cannot be used and neural networks can be seen as an attractive alternative. Finally, we highlight the benefits of deep compared to shallow neural networks and device some other convergence enhancing techniques.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Neurocomputing},
	author = {Berg, Jens and Nyström, Kaj},
	month = nov,
	year = {2018},
	keywords = {Advection, Complex geometries, Deep neural networks, Diffusion, Partial differential equations},
	pages = {28--41},
	file = {ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/LTU5M2TW/S092523121830794X.html:text/html;Submitted Version:/home/x/Library/library-ml-zotero/storage/5MXE2DMS/Berg and Nyström - 2018 - A unified deep artificial neural network approach .pdf:application/pdf},
}

@inproceedings{rubanova_latent_2019,
	title = {Latent {Ordinary} {Differential} {Equations} for {Irregularly}-{Sampled} {Time} {Series}},
	volume = {32},
	shorttitle = {{RNN} \& {ODE}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/42a6845a557bef704ad8ac9cb4461d43-Abstract.html},
	abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
	urldate = {2023-04-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David K},
	year = {2019},
	file = {Full Text PDF:/home/x/Library/library-ml-zotero/storage/HI64FF2Z/Rubanova et al. - 2019 - Latent Ordinary Differential Equations for Irregul.pdf:application/pdf},
}

@misc{rackauckas_diffeqfluxjl_2019,
	title = {{DiffEqFlux}.jl - {A} {Julia} {Library} for {Neural} {Differential} {Equations}},
	shorttitle = {{DiffEqFlux}.jl library},
	url = {http://arxiv.org/abs/1902.02376},
	doi = {10.48550/arXiv.1902.02376},
	abstract = {DiffEqFlux.jl is a library for fusing neural networks and differential equations. In this work we describe differential equations from the viewpoint of data science and discuss the complementary nature between machine learning models and differential equations. We demonstrate the ability to incorporate DifferentialEquations.jl-defined differential equation problems into a Flux-defined neural network, and vice versa. The advantages of being able to use the entire DifferentialEquations.jl suite for this purpose is demonstrated by counter examples where simple integration strategies fail, but the sophisticated integration strategies provided by the DifferentialEquations.jl library succeed. This is followed by a demonstration of delay differential equations and stochastic differential equations inside of neural networks. We show high-level functionality for defining neural ordinary differential equations (neural networks embedded into the differential equation) and describe the extra models in the Flux model zoo which includes neural stochastic differential equations. We conclude by discussing the various adjoint methods used for backpropogation of the differential equation solvers. DiffEqFlux.jl is an important contribution to the area, as it allows the full weight of the differential equation solvers developed from decades of research in the scientific computing field to be readily applied to the challenges posed by machine learning and data science.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and Bettencourt, Jesse and White, Lyndon and Dixit, Vaibhav},
	month = feb,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/LG9T9JLZ/Rackauckas et al. - 2019 - DiffEqFlux.jl - A Julia Library for Neural Differe.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/RB3DQ6DB/1902.html:text/html},
}

@article{samaniego_energy_2020,
	title = {An energy approach to the solution of partial differential equations in computational mechanics via machine learning: {Concepts}, implementation and applications},
	volume = {362},
	issn = {0045-7825},
	shorttitle = {{PDE} {DNN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782519306826},
	doi = {10.1016/j.cma.2019.112790},
	abstract = {Partial Differential Equations (PDEs) are fundamental to model different phenomena in science and engineering mathematically. Solving them is a crucial step towards a precise knowledge of the behavior of natural and engineered systems. In general, in order to solve PDEs that represent real systems to an acceptable degree, analytical methods are usually not enough. One has to resort to discretization methods. For engineering problems, probably the best-known option is the finite element method (FEM). However, powerful alternatives such as mesh-free methods and Isogeometric Analysis (IGA) are also available. The fundamental idea is to approximate the solution of the PDE by means of functions specifically built to have some desirable properties. In this contribution, we explore Deep Neural Networks (DNNs) as an option for approximation. They have shown impressive results in areas such as visual recognition. DNNs are regarded here as function approximation machines. There is great flexibility to define their structure and important advances in the architecture and the efficiency of the algorithms to implement them make DNNs a very interesting alternative to approximate the solution of a PDE. We concentrate on applications that have an interest for Computational Mechanics. Most contributions explore this possibility have adopted a collocation strategy. In this work, we concentrate on mechanical problems and analyze the energetic format of the PDE. The energy of a mechanical system seems to be the natural loss function for a machine learning method to approach a mechanical problem. In order to prove the concepts, we deal with several problems and explore the capabilities of the method for applications in engineering.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Samaniego, E. and Anitescu, C. and Goswami, S. and Nguyen-Thanh, V. M. and Guo, H. and Hamdia, K. and Zhuang, X. and Rabczuk, T.},
	month = apr,
	year = {2020},
	keywords = {Deep neural networks, Energy approach, Physics informed},
	pages = {112790},
	file = {ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/CYTA4PSZ/S0045782519306826.html:text/html;Submitted Version:/home/x/Library/library-ml-zotero/storage/FST388A3/Samaniego et al. - 2020 - An energy approach to the solution of partial diff.pdf:application/pdf},
}

@article{chen_neurodiffeq_2020,
	title = {{NeuroDiffEq}: {A} {Python} package for solving differential equations with neural networks},
	volume = {5},
	copyright = {NeurIPS},
	issn = {2475-9066},
	shorttitle = {{NeurIPS} {Library}},
	url = {https://joss.theoj.org/papers/10.21105/joss.01931},
	doi = {10.21105/joss.01931},
	abstract = {Chen et al., (2020). NeuroDiffEq: A Python package for solving differential equations with neural networks. Journal of Open Source Software, 5(46), 1931, https://doi.org/10.21105/joss.01931},
	language = {en},
	number = {46},
	urldate = {2023-04-28},
	journal = {Journal of Open Source Software},
	author = {Chen, Feiyu and Sondak, David and Protopapas, Pavlos and Mattheakis, Marios and Liu, Shuheng and Agarwal, Devansh and Giovanni, Marco Di},
	month = feb,
	year = {2020},
	pages = {1931},
	file = {2018 Neural ODE (NeurIPS) - Chen.pdf:/home/x/Library/library-ml-zotero/storage/FKUISQSD/2018 Neural ODE (NeurIPS) - Chen.pdf:application/pdf;2018 Neural ODE (NeurIPS) (inclui apendices) - Chen.pdf:/home/x/Library/library-ml-zotero/storage/TATENQR2/2018 Neural ODE (NeurIPS) (inclui apendices) - Chen.pdf:application/pdf;Chen et al. - 2020 - NeuroDiffEq A Python package for solving differen.pdf:/home/x/Library/library-ml-zotero/storage/R5IAHBA6/Chen et al. - 2020 - NeuroDiffEq A Python package for solving differen.pdf:application/pdf},
}

@misc{lou_neural_2020,
	title = {Neural {Manifold} {Ordinary} {Differential} {Equations}},
	shorttitle = {Neural {Manifold} {ODE}},
	url = {http://arxiv.org/abs/2006.10254},
	doi = {10.48550/arXiv.2006.10254},
	abstract = {To better conform to data geometry, recent deep generative modelling techniques adapt Euclidean constructions to non-Euclidean spaces. In this paper, we study normalizing flows on manifolds. Previous work has developed flow models for specific cases; however, these advancements hand craft layers on a manifold-by-manifold basis, restricting generality and inducing cumbersome design constraints. We overcome these issues by introducing Neural Manifold Ordinary Differential Equations, a manifold generalization of Neural ODEs, which enables the construction of Manifold Continuous Normalizing Flows (MCNFs). MCNFs require only local geometry (therefore generalizing to arbitrary manifolds) and compute probabilities with continuous change of variables (allowing for a simple and expressive flow construction). We find that leveraging continuous manifold dynamics produces a marked improvement for both density estimation and downstream tasks.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Lou, Aaron and Lim, Derek and Katsman, Isay and Huang, Leo and Jiang, Qingxuan and Lim, Ser-Nam and De Sa, Christopher},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Differential Geometry},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/M6LDWDUA/Lou et al. - 2020 - Neural Manifold Ordinary Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/I7IMRLSY/2006.html:text/html},
}

@inproceedings{pal_opening_2021,
	title = {Opening the {Blackbox}: {Accelerating} {Neural} {Differential} {Equations} by {Regularizing} {Internal} {Solver} {Heuristics}},
	shorttitle = {Accelerating {NDE}},
	url = {https://proceedings.mlr.press/v139/pal21a.html},
	abstract = {Democratization of machine learning requires architectures that automatically adapt to new problems. Neural Differential Equations (NDEs) have emerged as a popular modeling framework by removing the need for ML practitioners to choose the number of layers in a recurrent model. While we can control the computational cost by choosing the number of layers in standard architectures, in NDEs the number of neural network evaluations for a forward pass can depend on the number of steps of the adaptive ODE solver. But, can we force the NDE to learn the version with the least steps while not increasing the training cost? Current strategies to overcome slow prediction require high order automatic differentiation, leading to significantly higher training time. We describe a novel regularization method that uses the internal cost heuristics of adaptive differential equation solvers combined with discrete adjoint sensitivities to guide the training process towards learning NDEs that are easier to solve. This approach opens up the blackbox numerical analysis behind the differential equation solver’s algorithm and directly uses its local error estimates and stiffness heuristics as cheap and accurate cost estimates. We incorporate our method without any change in the underlying NDE framework and show that our method extends beyond Ordinary Differential Equations to accommodate Neural Stochastic Differential Equations. We demonstrate how our approach can halve the prediction time and, unlike other methods which can increase the training time by an order of magnitude, we demonstrate similar reduction in training times. Together this showcases how the knowledge embedded within state-of-the-art equation solvers can be used to enhance machine learning.},
	language = {en},
	urldate = {2023-04-28},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pal, Avik and Ma, Yingbo and Shah, Viral and Rackauckas, Christopher V.},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8325--8335},
	file = {Full Text PDF:/home/x/Library/library-ml-zotero/storage/EIS8RBUM/Pal et al. - 2021 - Opening the Blackbox Accelerating Neural Differen.pdf:application/pdf},
}

@misc{qiu_accuracy_2021,
	title = {Accuracy and {Architecture} {Studies} of {Residual} {Neural} {Network} solving {Ordinary} {Differential} {Equations}},
	shorttitle = {{ODE} {Accuracy}},
	url = {http://arxiv.org/abs/2101.03583},
	doi = {10.48550/arXiv.2101.03583},
	abstract = {In this paper we consider utilizing a residual neural network (ResNet) to solve ordinary differential equations. Stochastic gradient descent method is applied to obtain the optimal parameter set of weights and biases of the network. We apply forward Euler, Runge-Kutta2 and Runge-Kutta4 finite difference methods to generate three sets of targets training the ResNet and carry out the target study. The well trained ResNet behaves just as its counterpart of the corresponding one-step finite difference method. In particular, we carry out (1) the architecture study in terms of number of hidden layers and neurons per layer to find the optimal ResNet structure; (2) the target study to verify the ResNet solver behaves as accurate as its finite difference method counterpart; (3) solution trajectory simulation. Even the ResNet solver looks like and is implemented in a way similar to forward Euler scheme, its accuracy can be as high as any one step method. A sequence of numerical examples are presented to demonstrate the performance of the ResNet solver.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Qiu, Changxin and Bendickson, Aaron and Kalyanapu, Joshua and Yan, Jue},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Dynamical Systems},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/TC3TSVUG/Qiu et al. - 2021 - Accuracy and Architecture Studies of Residual Neur.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/7FTPMD6S/2101.html:text/html},
}

@misc{kidger_neural_2022,
	title = {On {Neural} {Differential} {Equations}},
	shorttitle = {On {NDE}},
	url = {http://arxiv.org/abs/2202.02435},
	doi = {10.48550/arXiv.2202.02435},
	abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Kidger, Patrick},
	month = feb,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Dynamical Systems, Mathematics - Classical Analysis and ODEs},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/DF8V2MGA/Kidger - 2022 - On Neural Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/QICQGJ6Q/2202.html:text/html},
}

@article{bolten_sde_2019,
	title = {An {SDE} waveform-relaxation method with application in distributed neural network simulations},
	volume = {19},
	issn = {1617-7061},
	shorttitle = {Distributed {NN}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pamm.201900373},
	doi = {10.1002/pamm.201900373},
	abstract = {Waveform-relaxation methods are a set of iterative methods to solve systems of differential equations by dividing them into subsystems. Several of these methods, such as the Jacobi waveform-relaxation method, enable potential for parallelization across the system and are for that reason interesting in applications with a highly parallel setting. Here we present an SDE waveform-relaxation methods with applications in the fields of computational neuroscience. We give a short overview how and where the application of the method can speed up the simulation of functionally inspired rate-based units in a distributed neural network simulator that was originally designed for biologically grounded spiking neuron models.},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {PAMM},
	author = {Bolten, Matthias and Hahne, Jan},
	year = {2019},
	pages = {e201900373},
	file = {Bolten and Hahne - 2019 - An SDE waveform-relaxation method with application.pdf:/home/x/Library/library-ml-zotero/storage/DXBDE4V3/Bolten and Hahne - 2019 - An SDE waveform-relaxation method with application.pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/E6JH6VLX/pamm.html:text/html},
}

@article{wang_massive_2019,
	title = {Massive computational acceleration by using neural networks to emulate mechanism-based biological models},
	volume = {10},
	issn = {2041-1723},
	shorttitle = {{ANN} performance},
	url = {https://www.nature.com/articles/s41467-019-12342-y},
	doi = {10.1038/s41467-019-12342-y},
	abstract = {For many biological applications, exploration of the massive parametric space of a mechanism-based model can impose a prohibitive computational demand. To overcome this limitation, we present a framework to improve computational efficiency by orders of magnitude. The key concept is to train a neural network using a limited number of simulations generated by a mechanistic model. This number is small enough such that the simulations can be completed in a short time frame but large enough to enable reliable training. The trained neural network can then be used to explore a much larger parametric space. We demonstrate this notion by training neural networks to predict pattern formation and stochastic gene expression. We further demonstrate that using an ensemble of neural networks enables the self-contained evaluation of the quality of each prediction. Our work can be a platform for fast parametric space screening of biological models with user defined objectives.},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {Nature Communications},
	author = {Wang, Shangying and Fan, Kai and Luo, Nan and Cao, Yangxiaolu and Wu, Feilun and Zhang, Carolyn and Heller, Katherine A. and You, Lingchong},
	month = sep,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, High-throughput screening, Synthetic biology, Systems analysis},
	pages = {4354},
	file = {Full Text PDF:/home/x/Library/library-ml-zotero/storage/Q7THB8MN/Wang et al. - 2019 - Massive computational acceleration by using neural.pdf:application/pdf},
}

@article{white_using_2021,
	title = {Using neural networks to reduce communication in numerical solution of partial differential equations},
	shorttitle = {{PINN} communication},
	url = {https://ml4physicalsciences.github.io/2021/files/NeurIPS_ML4PS_2021_12.pdf},
	author = {White, Laurent and Dasika, Ganesh and Rama, Saketh},
	year = {2021},
	file = {Full Text:/home/x/Library/library-ml-zotero/storage/F4P29JFN/White et al. - Using neural networks to reduce communication in n.pdf:application/pdf},
}

@inproceedings{stiller_large-scale_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Large-{Scale} {Neural} {Solvers} for {Partial} {Differential} {Equations}},
	isbn = {978-3-030-63393-6},
	shorttitle = {Large-scale {PINN}},
	url = {https://link.springer.com/chapter/10.1007/978-3-030-63393-6_2},
	doi = {10.1007/978-3-030-63393-6_2},
	abstract = {Solving partial differential equations (PDE) is an indispensable part of many branches of science as many processes can be modelled in terms of PDEs. However, recent numerical solvers require manual discretization of the underlying equation as well as sophisticated, tailored code for distributed computing. Scanning the parameters of the underlying model significantly increases the runtime as the simulations have to be cold-started for each parameter configuration. Machine Learning based surrogate models denote promising ways for learning complex relationship among input, parameter and solution. However, recent generative neural networks require lots of training data, i.e. full simulation runs making them costly. In contrast, we examine the applicability of continuous, mesh-free neural solvers for partial differential equations, physics-informed neural networks (PINNs) solely requiring initial/boundary values and validation points for training but no simulation data. The induced curse of dimensionality is approached by learning a domain decomposition that steers the number of neurons per unit volume and significantly improves runtime. Distributed training on large-scale cluster systems also promises great utilization of large quantities of GPUs which we assess by a comprehensive evaluation study. Finally, we discuss the accuracy of GatedPINN with respect to analytical solutions- as well as state-of-the-art numerical solvers, such as spectral solvers.},
	language = {en},
	booktitle = {Driving {Scientific} and {Engineering} {Discoveries} {Through} the {Convergence} of {HPC}, {Big} {Data} and {AI}},
	publisher = {Springer International Publishing},
	author = {Stiller, Patrick and Bethke, Friedrich and Böhme, Maximilian and Pausch, Richard and Torge, Sunna and Debus, Alexander and Vorberger, Jan and Bussmann, Michael and Hoffmann, Nico},
	editor = {Nichols, Jeffrey and Verastegui, Becky and Maccabe, Arthur ‘Barney’ and Hernandez, Oscar and Parete-Koon, Suzanne and Ahearn, Theresa},
	year = {2020},
	pages = {20--34},
	file = {Submitted Version:/home/x/Library/library-ml-zotero/storage/94YVREYS/Stiller et al. - 2020 - Large-Scale Neural Solvers for Partial Differentia.pdf:application/pdf},
}

@inproceedings{dumont_hyppo_2021,
	title = {{HYPPO}: {A} {Surrogate}-{Based} {Multi}-{Level} {Parallelism} {Tool} for {Hyperparameter} {Optimization}},
	shorttitle = {{HYPPO} library},
	url = {https://ieeexplore.ieee.org/document/9653176},
	doi = {10.1109/MLHPC54614.2021.00013},
	abstract = {We present a new software, HYPPO, that enables the automatic tuning of hyperparameters of various deep learning (DL) models. Unlike other hyperparameter optimization (HPO) methods, HYPPO uses adaptive surrogate models and directly accounts for uncertainty in model predictions to find accurate and reliable models that make robust predictions. Using asynchronous nested parallelism, we are able to significantly alleviate the computational burden of training complex architectures and quantifying the uncertainty. HYPPO is implemented in Python and can be used with both TensorFlow and PyTorch libraries. We demonstrate various software features on time-series prediction and image classification problems as well as a scientific application in computed tomography image reconstruction. Finally, we show that (1) we can reduce by an order of magnitude the number of evaluations necessary to find the most optimal region in the hyperparameter space and (2) we can reduce by two orders of magnitude the throughput for such HPO process to complete.},
	booktitle = {2021 {IEEE}/{ACM} {Workshop} on {Machine} {Learning} in {High} {Performance} {Computing} {Environments} ({MLHPC})},
	author = {Dumont, Vincent and Garner, Casey and Trivedi, Anuradha and Jones, Chelsea and Ganapati, Vidya and Mueller, Juliane and Perciano, Talita and Kiran, Mariam and Day, Marc},
	month = nov,
	year = {2021},
	note = {ISSN: 2768-4253},
	keywords = {Adaptation models, Computational modeling, Parallel processing, Predictive models, Stochastic processes, Training, Uncertainty},
	pages = {81--93},
	file = {IEEE Xplore Abstract Record:/home/x/Library/library-ml-zotero/storage/XU4BY87M/9653176.html:text/html;Submitted Version:/home/x/Library/library-ml-zotero/storage/89ERCXHN/Dumont et al. - 2021 - HYPPO A Surrogate-Based Multi-Level Parallelism T.pdf:application/pdf},
}

@phdthesis{lakshmiranganatha_hpc_2021,
	title = {{HPC} and {Machine} {Learning} {Techniques} for {Reducing} the {Computation} {Burden} of {Determining} {Time}-{Evolution} of {Complex} {Dynamic} {Systems}},
	copyright = {thesis},
	shorttitle = {{HPC} \& {ML}},
	url = {https://www.proquest.com/openview/7b44755c06084f2e3f628d0801664333/1},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	language = {en},
	urldate = {2023-04-28},
	author = {Lakshmiranganatha, Sumathi},
	year = {2021},
	file = {Lakshmiranganatha - 2021 - HPC and Machine Learning Techniques for Reducing t.pdf:/home/x/Library/library-ml-zotero/storage/JXRGSC8B/Lakshmiranganatha - 2021 - HPC and Machine Learning Techniques for Reducing t.pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/RT6NB4V9/1.html:text/html},
}

@phdthesis{corsini_deep_2021,
	title = {Deep learning methods for inverse supercontinuum generation},
	copyright = {Thesis},
	shorttitle = {Inverse supercontinuum ({Thesis})},
	url = {https://www.politesi.polimi.it/handle/10589/183786},
	urldate = {2023-04-28},
	author = {Corsini, Andrea},
	year = {2021},
	file = {Deep learning methods for inverse supercontinuum generation:/home/x/Library/library-ml-zotero/storage/DXWPK935/183786.html:text/html},
}

@phdthesis{ramzi_advanced_2022,
	type = {phdthesis},
	title = {Advanced deep neural networks for {MRI} image reconstruction from highly undersampled data in challenging acquisition settings},
	copyright = {Thesis},
	shorttitle = {{ANN} \& {MRI}},
	url = {https://theses.hal.science/tel-03623570},
	abstract = {Magnetic Resonance Imaging (MRI) is one of the most prominent imaging techniques in the world. Its main purpose is to probe soft tissues in a non-invasive and non-ionizing way. However, its wider adoption is hindered by an overall high scan time. In order to reduce this duration, several approaches have been proposed, among which Parallel Imaging (PI) and Compressed Sensing (CS) are the most important. Using these techniques, MR data can be acquired in a highly compressed way which allows the reduction of acquisition times. However, the algorithms typically used to reconstruct the MR images from these undersampled data are slow and underperform in highly accelerated scenarios. In order to address these issues, unrolled neural networks have been introduced. The core idea of these models is to unroll the iterations of classical reconstruction algorithms into a finite computation graph. The main objective of this PhD thesis is to propose new architecture designs for acquisition scenarios which deviate from the typical Cartesian 2D sampling. To this end, we first review a handful of neural networks for MRI reconstruction. After selecting the best performer, the PDNet, we extend it to two contexts: the fastMRI 2020 reconstruction challenge and the 3D non-Cartesian data problem. We also chose to adress the concerns of many regarding the clinical applicability of deep learning for medical imaging. We do so by proposing ways to build robust and inspectable models, but also by simply testing the trained networks in out-of-distribution settings. Finally, after noticing how the implicit deep learning framework can help implement deeper MRI reconstruction models, we introduce a new acceleration method (called SHINE) for the training of such models.},
	language = {en},
	urldate = {2023-04-28},
	school = {Université Paris-Saclay},
	author = {Ramzi, Zaccharie},
	month = feb,
	year = {2022},
	file = {Ramzi - 2022 - Advanced deep neural networks for MRI image recons.pdf:/home/x/Library/library-ml-zotero/storage/75QXRKVH/Ramzi - 2022 - Advanced deep neural networks for MRI image recons.pdf:application/pdf},
}

@misc{honchar_neural_2017,
	title = {Neural networks for solving differential equations},
	copyright = {website},
	shorttitle = {{ANN} \& {ODE} (website)},
	url = {https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c},
	abstract = {We mostly know neural networks as big hierarchical models that can learn patterns from data with complicated nature or distribution. That’s…},
	language = {en},
	urldate = {2023-04-28},
	journal = {Medium},
	author = {Honchar, Alex},
	year = {2017},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/M8DLWJYR/neural-networks-for-solving-differential-equations-fa230ac5e04c.html:text/html},
}

@misc{parkhill_solving_2021,
	title = {Solving multidimensional {PDEs} in {Pytorch}},
	copyright = {website},
	shorttitle = {{PDEs} in {Pytorch} (website)},
	url = {https://jparkhill.netlify.app/SolvingDiffusions/},
	abstract = {Solving multi-dimensional partial differential equations (PDE’s) is something I’ve spent most of my adult life doing. Most of them are somewhat similar to the heat equation:},
	language = {en},
	urldate = {2023-04-28},
	journal = {jparkhill.github.io},
	author = {Parkhill, John},
	month = apr,
	year = {2021},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/DTETFR6G/SolvingDiffusions.html:text/html},
}

@misc{black_how_2021,
	title = {How {Machine} {Learning} {Is} {Revolutionizing} {HPC} {Simulations}},
	copyright = {website},
	shorttitle = {{PINN} and {HPC} (website)},
	url = {https://insidehpc.com/2021/08/how-machine-learning-is-revolutionizing-hpc-simulations/},
	abstract = {Physics-based simulations, that staple of traditional HPC, may be evolving toward an emerging, AI-based technique that could radically accelerate simulation runs while cutting costs. Called “surrogate machine learning models,” the topic was a focal point in a keynote on Tuesday at the International Conference on Parallel Processing by Argonne National Lab’s Rick Stevens. Stevens, ANL’s […]},
	language = {en-US},
	urldate = {2023-04-28},
	journal = {High-Performance Computing News Analysis {\textbar} insideHPC},
	author = {Black, Doug},
	month = aug,
	year = {2021},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/AVLZ88I2/how-machine-learning-is-revolutionizing-hpc-simulations.html:text/html},
}

@misc{pareti_surrogate_2021,
	title = {Surrogate {Models} integrating an {HPC} solver and a {Machine} {Learning} component},
	copyright = {website},
	shorttitle = {Weather predictions (website)},
	url = {https://docs.google.com/document/d/1t8HJsiVelvTmsxFUxR9YHXnYgEpY0DX42i5tdARQ0Kk/edit},
	abstract = {Surrogate Models integrating an HPC Solver and a Machine Learning Component  Overview	1 Nvidia	1 Deep Learning for Turbulence Modeling	2 Large Eddy Simulation, or LES	3 PyTorch-based Lattice Boltzmann Framework	3 Conversation with the Authors of the LBM paper	3 SmartSim: Using Machine Learning a...},
	language = {en-GB},
	urldate = {2023-04-28},
	journal = {Google Docs},
	author = {Pareti, Joseph},
	year = {2021},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/5EPJHD3Y/edit.html:text/html},
}

@article{barker_monte_2008,
	title = {The {Monte} {Carlo} {Independent} {Column} {Approximation}: {An} {Assessment} {Using} {Several} {Global} {Atmospheric} {Models}},
	volume = {134},
	copyright = {Atmospheric},
	shorttitle = {Monte {Carlo} {Atmospheric} {Model}},
	number = {635},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Barker, H. W. and Cole, J. N. S. and Morcrette, J.-J. and Pincus, R. and Räisänen, P. and von Salzen, K. and Vaillancourt, P. A.},
	year = {2008},
	pages = {1463--1478},
	file = {Barker et al. - 2008 - The Monte Carlo Independent Column Approximation .pdf:/home/x/Library/library-ml-zotero/storage/WYAEBPR3/Barker et al. - 2008 - The Monte Carlo Independent Column Approximation .pdf:application/pdf},
}

@article{burkardt_investigating_2013,
	title = {Investigating {Uncertain} {Parameters} in the {Burgers} {Equation}},
	shorttitle = {Burgers equation},
	url = {https://people.sc.fsu.edu/~jburkardt/presentations/presentations.html},
	author = {Burkardt, John},
	year = {2013},
	note = {Presentation: https://people.sc.fsu.edu/{\textasciitilde}jburkardt/presentations/burgers\_2013\_ajou.pdf},
	file = {Burkardt - 2013 - Investigating Uncertain Parameters in the Burgers .pdf:/home/x/Library/library-ml-zotero/storage/A8QCY5FU/Burkardt - 2013 - Investigating Uncertain Parameters in the Burgers .pdf:application/pdf},
}

@book{chollet_deep_2018,
	title = {Deep {Learning} with {Python}},
	copyright = {Book},
	shorttitle = {Deep {Learning} (book)},
	publisher = {Manning Publications Co), oCLC: ocn982650571},
	author = {Chollet, F.},
	year = {2018},
}

@unpublished{noauthor_ecrad_2022,
	title = {{ECRAD} - {ECMWF} {Atmospheric} {Radiation} {Scheme}},
	shorttitle = {{ECRAD} (website)},
	url = {https://github.com/ecmwf-ifs/ecrad},
	abstract = {ECMWF atmospheric radiation scheme},
	urldate = {2022-10-08},
	month = sep,
	year = {2022},
}

@unpublished{noauthor_monan_nodate,
	title = {{MONAN} {Scientific} {Committee}},
	shorttitle = {{MONAN} (website)},
	url = {https://monanadmin.github.io/monan_cc_docs/},
	urldate = {2022-10-08},
}

@book{patterson_deep_2017,
	title = {Deep {Learning}: {A} {Practitioner}'s {Approach}},
	copyright = {Book},
	shorttitle = {Deel {Learning} (book)},
	publisher = {" O'Reilly Media, Inc."},
	author = {Patterson, Josh and Gibson, Adam},
	year = {2017},
}

@book{raschka_python_2019,
	title = {Python {Machine} {Learning}: {Machine} {Learning} and {Deep} {Learning} with {Python}, {Scikit}-{Learn}, and {TensorFlow} 2},
	copyright = {Book},
	shorttitle = {{ML} \& {Python} (book)},
	publisher = {Packt Publishing Ltd},
	author = {Raschka, Sebastian and Mirjalili, Vahid},
	year = {2019},
}

@misc{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	shorttitle = {Neural {ODE} ({NIPS2018})},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {Sources: https://github.com/rtqichen/torchdiffeq},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/MYKG3RWG/1806.html:text/html;Chen et al. - 2019 - Neural Ordinary Differential Equations (1).pdf:/home/x/Library/library-ml-zotero/storage/WMM4F4LE/Chen et al. - 2019 - Neural Ordinary Differential Equations (1).pdf:application/pdf;Chen et al. - 2019 - Neural Ordinary Differential Equations (2).pdf:/home/x/Library/library-ml-zotero/storage/BVFGSLVI/Chen et al. - 2019 - Neural Ordinary Differential Equations (2).pdf:application/pdf},
}

@article{kasim_building_2022,
	title = {Building high accuracy emulators for scientific simulations with deep neural architecture search},
	volume = {3},
	copyright = {2 billion times speedup},
	issn = {2632-2153},
	shorttitle = {{ANN} {Performance}},
	url = {http://arxiv.org/abs/2001.08055},
	doi = {10.1088/2632-2153/ac3ffa},
	abstract = {Computer simulations are invaluable tools for scientific discovery. However, accurate simulations are often slow to execute, which limits their applicability to extensive parameter exploration, large-scale data analysis, and uncertainty quantification. A promising route to accelerate simulations by building fast emulators with machine learning requires large training datasets, which can be prohibitively expensive to obtain with slow simulations. Here we present a method based on neural architecture search to build accurate emulators even with a limited number of training data. The method successfully accelerates simulations by up to 2 billion times in 10 scientific cases including astrophysics, climate science, biogeochemistry, high energy density physics, fusion energy, and seismology, using the same super-architecture, algorithm, and hyperparameters. Our approach also inherently provides emulator uncertainty estimation, adding further confidence in their use. We anticipate this work will accelerate research involving expensive simulations, allow more extensive parameters exploration, and enable new, previously unfeasible computational discovery.},
	number = {1},
	urldate = {2023-05-01},
	journal = {Machine Learning: Science and Technology},
	author = {Kasim, M. F. and Watson-Parris, D. and Deaconu, L. and Oliver, S. and Hatfield, P. and Froula, D. H. and Gregori, G. and Jarvis, M. and Khatiwala, S. and Korenaga, J. and Topp-Mugglestone, J. and Viezzer, E. and Vinko, S. M.},
	month = mar,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning, Physics - Atmospheric and Oceanic Physics, Physics - Plasma Physics},
	pages = {015013},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/Z4I9DE9Y/2001.html:text/html;Kasim et al. - 2022 - Building high accuracy emulators for scientific si.pdf:/home/x/Library/library-ml-zotero/storage/FLICKXIF/Kasim et al. - 2022 - Building high accuracy emulators for scientific si.pdf:application/pdf},
}

@article{hogan_incorporating_2013,
	title = {Incorporating the {Effects} of {3D} {Radiative} {Transfer} in the {Presence} of {Clouds} into {Two}-{Stream} {Multilayer} {Radiation} {Schemes}},
	volume = {70},
	issn = {0022-4928, 1520-0469},
	shorttitle = {Incorporating {3D} radiation},
	url = {https://journals.ametsoc.org/doi/10.1175/JAS-D-12-041.1},
	doi = {10.1175/JAS-D-12-041.1},
	abstract = {Abstract
            This paper presents a new method for representing the important effects of horizontal radiation transport through cloud sides in two-stream radiation schemes. Ordinarily, the radiative transfer equations are discretized separately for the clear and cloudy regions within each model level, but here terms are introduced that represent the exchange of radiation laterally between regions and the resulting coupled equations are solved for each layer. This approach may be taken with both the direct incoming shortwave radiation, which is governed by Beer’s law, and the diffuse shortwave and longwave radiation, governed by the two-stream equations. The rate of lateral exchange is determined by the area of cloud “edge.” The validity of the method is demonstrated by comparing with rigorous 3D radiative transfer calculations in the literature for two cloud types in which the 3D effect is strong, specifically cumulus and aircraft contrails. The 3D effect on shortwave cloud radiative forcing varies between around −25\% and around +100\%, depending on solar zenith angle. Even with an otherwise very simplistic representation of the cloud, the new scheme exhibits good agreement with the rigorous calculations in the shortwave, opening the way for efficient yet accurate representation of this important effect in climate models.},
	language = {en},
	number = {2},
	urldate = {2023-05-01},
	journal = {Journal of the Atmospheric Sciences},
	author = {Hogan, Robin J. and Shonk, Jonathan K. P.},
	month = feb,
	year = {2013},
	pages = {708--724},
	file = {Hogan and Shonk - 2013 - Incorporating the Effects of 3D Radiative Transfer.pdf:/home/x/Library/library-ml-zotero/storage/MNCGIVQZ/Hogan and Shonk - 2013 - Incorporating the Effects of 3D Radiative Transfer.pdf:application/pdf},
}

@article{schafer_representing_2016,
	title = {Representing 3-{D} cloud radiation effects in two-stream schemes: 1. {Longwave} considerations and effective cloud edge length},
	volume = {121},
	copyright = {Radiation 3D},
	issn = {2169897X},
	shorttitle = {Representing {3D} radiation},
	url = {http://doi.wiley.com/10.1002/2016JD024876},
	doi = {10.1002/2016JD024876},
	language = {en},
	number = {14},
	urldate = {2023-05-01},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Schäfer, Sophia A. K. and Hogan, Robin J. and Klinger, Carolin and Chiu, J. Christine and Mayer, Bernhard},
	month = jul,
	year = {2016},
	pages = {8567--8582},
	file = {Schäfer et al. - 2016 - Representing 3-D cloud radiation effects in two-st.pdf:/home/x/Library/library-ml-zotero/storage/FJ36PZ27/Schäfer et al. - 2016 - Representing 3-D cloud radiation effects in two-st.pdf:application/pdf},
}

@article{shonk_tripleclouds_2008,
	title = {Tripleclouds: {An} {Efficient} {Method} for {Representing} {Horizontal} {Cloud} {Inhomogeneity} in {1D} {Radiation} {Schemes} by {Using} {Three} {Regions} at {Each} {Height}},
	volume = {21},
	copyright = {Radiation},
	issn = {1520-0442, 0894-8755},
	shorttitle = {Radiation {1D}},
	url = {http://journals.ametsoc.org/doi/10.1175/2007JCLI1940.1},
	doi = {10.1175/2007JCLI1940.1},
	abstract = {Abstract
            Radiation schemes in general circulation models currently make a number of simplifications when accounting for clouds, one of the most important being the removal of horizontal inhomogeneity. A new scheme is presented that attempts to account for the neglected inhomogeneity by using two regions of cloud in each vertical level of the model as opposed to one. One of these regions is used to represent the optically thinner cloud in the level, and the other represents the optically thicker cloud. So, along with the clear-sky region, the scheme has three regions in each model level and is referred to as “Tripleclouds.” In addition, the scheme has the capability to represent arbitrary vertical overlap between the three regions in pairs of adjacent levels. This scheme is implemented in the Edwards–Slingo radiation code and tested on 250 h of data from 12 different days. The data are derived from cloud retrievals using radar, lidar, and a microwave radiometer at Chilbolton, southern United Kingdom. When the data are grouped into periods equivalent in size to general circulation model grid boxes, the shortwave plane-parallel albedo bias is found to be 8\%, while the corresponding bias is found to be less than 1\% using Tripleclouds. Similar results are found for the longwave biases. Tripleclouds is then compared to a more conventional method of accounting for inhomogeneity that multiplies optical depths by a constant scaling factor, and Tripleclouds is seen to improve on this method both in terms of top-of-atmosphere radiative flux biases and internal heating rates.},
	language = {en},
	number = {11},
	urldate = {2023-05-01},
	journal = {Journal of Climate},
	author = {Shonk, Jonathan K. P. and Hogan, Robin J.},
	month = jun,
	year = {2008},
	pages = {2352--2370},
	file = {Shonk and Hogan - 2008 - Tripleclouds An Efficient Method for Representing.pdf:/home/x/Library/library-ml-zotero/storage/CC6VFYVQ/Shonk and Hogan - 2008 - Tripleclouds An Efficient Method for Representing.pdf:application/pdf},
}

@misc{bragone_physics-informed_2021,
	title = {Physics-informed machine learning in power transformer dynamic thermal modelling},
	shorttitle = {{PINN} thermal modelling},
	author = {Bragone, Federica},
	year = {2021},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/45IFHW7G/record.html:text/html},
}

@article{krestenitis_oil_2019,
	title = {Oil {Spill} {Identification} from {Satellite} {Images} {Using} {Deep} {Neural} {Networks}},
	volume = {11},
	copyright = {Palestra 230504},
	issn = {2072-4292},
	shorttitle = {Oil spill {ANN}},
	url = {https://www.mdpi.com/2072-4292/11/15/1762},
	doi = {10.3390/rs11151762},
	abstract = {Oil spill is considered one of the main threats to marine and coastal environments. Efficient monitoring and early identification of oil slicks are vital for the corresponding authorities to react expediently, confine the environmental pollution and avoid further damage. Synthetic aperture radar (SAR) sensors are commonly used for this objective due to their capability for operating efficiently regardless of the weather and illumination conditions. Black spots probably related to oil spills can be clearly captured by SAR sensors, yet their discrimination from look-alikes poses a challenging objective. A variety of different methods have been proposed to automatically detect and classify these dark spots. Most of them employ custom-made datasets posing results as non-comparable. Moreover, in most cases, a single label is assigned to the entire SAR image resulting in a difficulties when manipulating complex scenarios or extracting further information from the depicted content. To overcome these limitations, semantic segmentation with deep convolutional neural networks (DCNNs) is proposed as an efficient approach. Moreover, a publicly available SAR image dataset is introduced, aiming to consist a benchmark for future oil spill detection methods. The presented dataset is employed to review the performance of well-known DCNN segmentation models in the specific task. DeepLabv3+ presented the best performance, in terms of test set accuracy and related inference time. Furthermore, the complex nature of the specific problem, especially due to the challenging task of discriminating oil spills and look-alikes is discussed and illustrated, utilizing the introduced dataset. Results imply that DCNN segmentation models, trained and evaluated on the provided dataset, can be utilized to implement efficient oil spill detectors. Current work is expected to contribute significantly to the future research activity regarding oil spill identification and SAR image processing.},
	language = {en},
	number = {15},
	urldate = {2023-05-04},
	journal = {Remote Sensing},
	author = {Krestenitis, Marios and Orfanidis, Georgios and Ioannidis, Konstantinos and Avgerinakis, Konstantinos and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
	month = jan,
	year = {2019},
	keywords = {deep convolutional neural networks, oil spill detection, remote sensing, SAR imagery, semantic image segmentation},
	pages = {1762},
	file = {Full Text PDF:/home/x/Library/library-ml-zotero/storage/THQ2FWE5/Krestenitis et al. - 2019 - Oil Spill Identification from Satellite Images Usi.pdf:application/pdf},
}

@misc{raval_neural_2019,
	title = {Neural {Differential} {Equations}},
	copyright = {Video},
	shorttitle = {{NDE} (video)},
	url = {https://youtu.be/AD3K8j12EIE},
	abstract = {This won the best paper award at NeurIPS (the biggest AI conference of the year) out of over 4800 other research papers! Neural Ordinary Differential Equations is the official name of the paper and in it the authors introduce a new type of neural network. This new network doesn't have any layers! Its framed as a differential equation, which allows us to use differential equation solvers on it to approximate the underlying function of time series data. Its very cool and will ultimately allow us to learn from irregular time series datasets more efficiently, which applies to many different industries. I'll cover all the prerequisites in this video and point to helpful resources down below. Enjoy! 

Code for this video:
https://github.com/llSourcell/Neural\_Differential\_Equations/

More learning resources:
   • Backpropagation i...  https://www.youtube.com/watch?v=q555kfIFUCM\&t=0s
   • Build a Neural Ne...  https://www.youtube.com/watch?v=h3l4qz76JhQ\&t=9s
   • The essence of ca...  https://www.youtube.com/watch?v=WUvTyaaNkzM\&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr
https://towardsdatascience.com/paper-summary-neural-ordinary-differential-equations-37c4e52df128
https://arxiv.org/abs/1806.07366
https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/
https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/},
	urldate = {2023-05-04},
	collaborator = {Raval, Siraj},
	month = jan,
	year = {2019},
	note = {Sources: https://github.com/llSourcell/Neural\_Differential\_Equations},
}

@misc{ali_mit_2023,
	title = {{MIT} {Deep} {Learning} {Book} (beautiful and flawless {PDF} version)},
	shorttitle = {{MIT} {Deep} {Learning}},
	url = {https://github.com/janishar/mit-deep-learning-book-pdf},
	abstract = {MIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville},
	urldate = {2023-05-18},
	author = {Ali, Janishar},
	month = may,
	year = {2023},
	note = {https://www.deeplearningbook.org/},
	keywords = {book, chapter, clear, deep-learning, deeplearning, excercises, good, learning, lecture-notes, linear-algebra, machine, machine-learning, mit, neural-network, neural-networks, pdf, print, printable, thinking},
}

@inproceedings{yin_strategies_2022,
	address = {Lyon, France},
	title = {Strategies for {Integrating} {Deep} {Learning} {Surrogate} {Models} with {HPC} {Simulation} {Applications}},
	isbn = {978-1-66549-747-3},
	url = {https://ieeexplore.ieee.org/document/9835386/},
	doi = {10.1109/IPDPSW55747.2022.00222},
	abstract = {The emerging trend of the convergence of high performance computing (HPC), machine learning/deep learning (ML/DL), and big data analytics presents a host of challenges for large-scale computing campaigns that seek best practices to interleave traditional scientiﬁc simulation-based workloads with ML/DL models. A portfolio of systematic approaches to incorporate deep learning into modeling and simulation serves a vital need when we support AI for science at a computing facility. In this paper, we evaluate several strategies for deploying deep learning surrogate models in a representative physics application on supercomputers at the Oak Ridge Leadership Computing Facility (OLCF). We discuss a set of recommended deployment architectures and implementation approaches. We analyze and evaluate these alternatives and show their performance and scalability up to 1000 GPUs on two mainstream platforms equipped with different deep learning hardware and software stacks.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	publisher = {IEEE},
	author = {Yin, Junqi and Wang, Feiyi and Shankar, Mallikarjun},
	month = may,
	year = {2022},
	pages = {01--10},
	file = {Yin et al. - 2022 - Strategies for Integrating Deep Learning Surrogate.pdf:/home/x/Library/library-ml-zotero/storage/MLLNYPQD/Yin et al. - 2022 - Strategies for Integrating Deep Learning Surrogate.pdf:application/pdf},
}

@misc{noauthor_online_nodate,
	title = {Online {Machine} {Learning} for {Exascale} {CFD} {\textbar} {Argonne} {Leadership} {Computing} {Facility}},
	url = {https://www.alcf.anl.gov/support-center/training-assets/online-machine-learning-exascale-cfd},
	urldate = {2023-05-18},
	file = {Online Machine Learning for Exascale CFD  Argonne.pdf:/home/x/Library/library-ml-zotero/storage/YFQT7RTQ/Online Machine Learning for Exascale CFD  Argonne.pdf:application/pdf;Online Machine Learning for Exascale CFD | Argonne Leadership Computing Facility:/home/x/Library/library-ml-zotero/storage/BEMYBFUX/online-machine-learning-exascale-cfd.html:text/html},
}

@article{partee_using_2022,
	title = {Using {Machine} {Learning} at scale in numerical simulations with {SmartSim}: {An} application to ocean climate modeling},
	volume = {62},
	issn = {1877-7503},
	shorttitle = {Using {Machine} {Learning} at scale in numerical simulations with {SmartSim}},
	url = {https://www.sciencedirect.com/science/article/pii/S1877750322001065},
	doi = {10.1016/j.jocs.2022.101707},
	abstract = {We demonstrate the first climate-scale, numerical ocean simulations improved through distributed, online inference of Deep Neural Networks (DNN) using SmartSim. SmartSim is a library dedicated to enabling online analysis and Machine Learning (ML) for high performance, numerical simulations. In this paper, we detail the SmartSim architecture and provide benchmarks including online inference with a shared ML model, EKE-ResNet, on heterogeneous HPC systems. We demonstrate the capability of SmartSim by using it to run a 12-member ensemble of global-scale, high-resolution ocean simulations, each spanning 19 compute nodes, all communicating with the same ML architecture at each simulation timestep. In total, 970 billion inferences are collectively served by running the ensemble for a total of 120 simulated years. The inferences are used to predict the oceanic eddy kinetic energy (EKE), which is a variable that is used to tune different turbulence closures in the model and thus directly affects the simulation. The root-mean-square of the error in EKE (as compared to an eddy-resolving simulation) is 20\% lower when using the ML-prediction than the previous state of the art. This demonstration is an example of how machine learning methods can be integrated into traditional numerical simulations, replace prognostic equations, and preserve overall simulation stability without significantly affecting the time to solution.},
	language = {en},
	urldate = {2023-05-18},
	journal = {Journal of Computational Science},
	author = {Partee, Sam and Ellis, Matthew and Rigazzi, Alessandro and Shao, Andrew E. and Bachman, Scott and Marques, Gustavo and Robbins, Benjamin},
	month = jul,
	year = {2022},
	keywords = {Climate modeling, Deep learning, High performance computing, Numerical simulation, SmartSim},
	pages = {101707},
	file = {Partee et al. - 2021 - Preprint - Using Machine Learning at Scale in HPC Simulations.pdf:/home/x/Library/library-ml-zotero/storage/4GYATY9V/Partee et al. - 2021 - Preprint - Using Machine Learning at Scale in HPC Simulations.pdf:application/pdf;Partee et al. - 2022 - Using Machine Learning at scale in numerical simul.pdf:/home/x/Library/library-ml-zotero/storage/GSCH643S/Partee et al. - 2022 - Using Machine Learning at scale in numerical simul.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/ZGWVFPAL/S1877750322001065.html:text/html},
}

@misc{noauthor_online_nodate-1,
	title = {Online {Learning} with {SmartSim} {\textbar} {Argonne} {Leadership} {Computing} {Facility}},
	url = {https://www.alcf.anl.gov/support-center/training-assets/online-learning-smartsim},
	urldate = {2023-05-18},
	file = {Online Learning with SmartSim  Argonne Leadership.pdf:/home/x/Library/library-ml-zotero/storage/X3E4SLPB/Online Learning with SmartSim  Argonne Leadership.pdf:application/pdf;Online Learning with SmartSim | Argonne Leadership Computing Facility:/home/x/Library/library-ml-zotero/storage/EJA9SF7J/online-learning-smartsim.html:text/html},
}

@misc{noauthor_argonne_2021,
	title = {Argonne {Annual} {Report}},
	year = {2021},
	file = {ALCF_2021AR.pdf:/home/x/Library/library-ml-zotero/storage/2XNADXEV/ALCF_2021AR.pdf:application/pdf},
}

@article{kurz_relexi_2022,
	title = {Relexi — {A} scalable open source reinforcement learning framework for high-performance computing},
	volume = {14},
	issn = {26659638},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2665963822001063},
	doi = {10.1016/j.simpa.2022.100422},
	abstract = {Relexi is an open source reinforcement learning (RL) framework written in Python and based on TensorFlow’s RL library TF-Agents. Relexi allows to employ RL for environments that require computationally intensive simulations like applications in computational fluid dynamics. For this, Relexi couples legacy simulation codes with the RL library TF-Agents at scale on modern high-performance computing (HPC) hardware using the SmartSim library. Relexi thus provides an easy way to explore the potential of RL for HPC applications.},
	language = {en},
	urldate = {2023-05-18},
	journal = {Software Impacts},
	author = {Kurz, Marius and Offenhäuser, Philipp and Viola, Dominic and Resch, Michael and Beck, Andrea},
	month = dec,
	year = {2022},
	pages = {100422},
	file = {Kurz et al. - 2022 - Relexi — A scalable open source reinforcement lear.pdf:/home/x/Library/library-ml-zotero/storage/DVWU88Q3/Kurz et al. - 2022 - Relexi — A scalable open source reinforcement lear.pdf:application/pdf},
}

@inproceedings{ward_colmena_2021,
	address = {St. Louis, MO, USA},
	title = {Colmena: {Scalable} {Machine}-{Learning}-{Based} {Steering} of {Ensemble} {Simulations} for {High} {Performance} {Computing}},
	isbn = {978-1-66541-124-0},
	shorttitle = {Colmena},
	url = {https://ieeexplore.ieee.org/document/9653177/},
	doi = {10.1109/MLHPC54614.2021.00007},
	abstract = {Scientiﬁc applications that involve simulation ensembles can be accelerated greatly by using experiment design methods to select the best simulations to perform. Methods that use machine learning (ML) to create proxy models of simulations show particular promise for guiding ensembles but are challenging to deploy because of the need to coordinate dynamic mixes of simulation and learning tasks. We present Colmena, an open-source Python framework that allows users to steer campaigns by providing just the implementations of individual tasks plus the logic used to choose which tasks to execute when. Colmena handles task dispatch, results collation, ML model invocation, and ML model (re)training, using Parsl to execute tasks on HPC systems. We describe the design of Colmena and illustrate its capabilities by applying it to electrolyte design, where it both scales to 65 536 CPUs and accelerates the discovery rate for high-performance molecules by a factor of 100 over unguided searches.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {2021 {IEEE}/{ACM} {Workshop} on {Machine} {Learning} in {High} {Performance} {Computing} {Environments} ({MLHPC})},
	publisher = {IEEE},
	author = {Ward, Logan and Sivaraman, Ganesh and Pauloski, J. Gregory and Babuji, Yadu and Chard, Ryan and Dandu, Naveen and Redfern, Paul C. and Assary, Rajeev S. and Chard, Kyle and Curtiss, Larry A. and Thakur, Rajeev and Foster, Ian},
	month = nov,
	year = {2021},
	pages = {9--20},
	file = {Ward et al. - 2021 - Colmena Scalable Machine-Learning-Based Steering .pdf:/home/x/Library/library-ml-zotero/storage/TMFDN959/Ward et al. - 2021 - Colmena Scalable Machine-Learning-Based Steering .pdf:application/pdf},
}

@article{jansen_energy_2015,
	title = {Energy budget-based backscatter in an eddy permitting primitive equation model},
	volume = {94},
	issn = {1463-5003},
	url = {https://www.sciencedirect.com/science/article/pii/S1463500315001341},
	doi = {10.1016/j.ocemod.2015.07.015},
	abstract = {Increasing computational resources are starting to allow global ocean simulations at so-called “eddy-permitting” resolutions, at which the largest mesoscale eddies can be resolved explicitly. However, an adequate parameterization of the interactions with the unresolved part of the eddy energy spectrum remains crucial. Hyperviscous closures, which are commonly applied in eddy-permitting ocean models, cause spurious energy dissipation at these resolutions, leading to low levels of eddy kinetic energy (EKE) and weak eddy induced transports. It has recently been proposed to counteract the spurious energy dissipation of hyperviscous closures by an additional forcing term, which represents “backscatter” of energy from the un-resolved scales to the resolved scales. This study proposes a parameterization of energy backscatter based on an explicit sub-grid EKE budget. Energy dissipated by hyperviscosity acting on the resolved flow is added to the sub-grid EKE, while a backscatter term transfers energy back from the sub-grid EKE to the resolved flow. The backscatter term is formulated deterministically via a negative viscosity, which returns energy at somewhat larger scales than the hyperviscous dissipation, thus ensuring dissipation of enstrophy. The parameterization is tested in an idealized configuration of a primitive equation ocean model, and is shown to significantly improve the solutions of simulations at typical eddy-permitting resolutions.},
	language = {en},
	urldate = {2023-05-19},
	journal = {Ocean Modelling},
	author = {Jansen, Malte F. and Held, Isaac M. and Adcroft, Alistair and Hallberg, Robert},
	month = oct,
	year = {2015},
	keywords = {Backscatter, Eddy parameterization, Eddy-permitting, Energy budget, Mesoscale, Negative viscosity},
	pages = {15--26},
	file = {Jansen et al. - 2015 - Energy budget-based backscatter in an eddy permitt.pdf:/home/x/Library/library-ml-zotero/storage/7NE37LXI/Jansen et al. - 2015 - Energy budget-based backscatter in an eddy permitt.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/7TGEPW3K/S1463500315001341.html:text/html},
}

@article{monteiro_sympl_2018,
	title = {Sympl (v. 0.4.0) and climt (v. 0.15.3) - {Towards} a flexible framework for building model hierarchies in {Python}},
	volume = {11},
	doi = {10.5194/gmd-11-3781-2018},
	abstract = {sympl (System for Modelling Planets) and climt (Climate Modelling and Diagnostics Toolkit) are an attempt to rethink climate modelling frameworks from the ground up. The aim is to use expressive data structures available in the scientific Python ecosystem along with best practices in software design to allow scientists to easily and reliably combine model components to represent the climate system at a desired level of complexity and to enable users to fully understand what the model is doing.
sympl is a framework which formulates the model in terms of a state that gets evolved forward in time or modified within a specific time by well-defined components. sympl's design facilitates building models that are self-documenting, are highly interoperable, and provide fine-grained control over model components and behaviour. sympl components contain all relevant information about the input they expect and output that they provide. Components are designed to be easily interchanged, even when they rely on different units or array configurations. sympl provides basic functions and objects which could be used in any type of Earth system model.
climt is an Earth system modelling toolkit that contains scientific components built using sympl base objects. These include both pure Python components and wrapped Fortran libraries. climt provides functionality requiring model-specific assumptions, such as state initialization and grid configuration. climt's programming interface designed to be easy to use and thus appealing to a wide audience.
Model building, configuration and execution are performed through a Python script (or Jupyter Notebook), enabling researchers to build an end-to-end Python-based pipeline along with popular Python data analysis and visualization tools.},
	journal = {Geoscientific Model Development},
	author = {Monteiro, Joy and Mcgibbon, Jeremy and Caballero, Rodrigo},
	month = sep,
	year = {2018},
	pages = {3781--3794},
	file = {Monteiro et al. - 2018 - Sympl (v. 0.4.0) and climt (v. 0.15.3) - Towards a.pdf:/home/x/Library/library-ml-zotero/storage/HUR4ZP4T/Monteiro et al. - 2018 - Sympl (v. 0.4.0) and climt (v. 0.15.3) - Towards a.pdf:application/pdf},
}

@misc{ben-nun_productive_2022,
	title = {Productive {Performance} {Engineering} for {Weather} and {Climate} {Modeling} with {Python}},
	shorttitle = {Weather {Modeling} {Python}},
	url = {http://arxiv.org/abs/2205.04148},
	abstract = {Earth system models are developed with a tight coupling to target hardware, often containing specialized code predicated on processor characteristics. This coupling stems from using imperative languages that hard-code computation schedules and layout. We present a detailed account of optimizing the Finite Volume Cubed-Sphere Dynamical Core (FV3), improving productivity and performance. By using a declarative Pythonembedded stencil domain-speciﬁc language and data-centric optimization, we abstract hardware-speciﬁc details and deﬁne a semi-automated workﬂow for analyzing and optimizing weather and climate applications. The workﬂow utilizes both local and full-program optimization, as well as user-guided ﬁne-tuning. To prune the infeasible global optimization space, we automatically utilize repeating code motifs via a novel transfer tuning approach. On the Piz Daint supercomputer, we scale to 2,400 GPUs, achieving speedups of up to 3.92× over the tuned production implementation at a fraction of the original code.},
	language = {en},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Ben-Nun, Tal and Groner, Linus and Deconinck, Florian and Wicky, Tobias and Davis, Eddie and Dahm, Johann and Elbert, Oliver D. and George, Rhea and McGibbon, Jeremy and Trümper, Lukas and Wu, Elynn and Fuhrer, Oliver and Schulthess, Thomas and Hoefler, Torsten},
	month = aug,
	year = {2022},
	note = {arXiv:2205.04148 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Ben-Nun et al. - 2022 - Productive Performance Engineering for Weather and.pdf:/home/x/Library/library-ml-zotero/storage/DP5ZMFXY/Ben-Nun et al. - 2022 - Productive Performance Engineering for Weather and.pdf:application/pdf},
}

@article{mcgibbon_fv3gfs-wrapper_2021,
	title = {fv3gfs-wrapper: a {Python} wrapper of the {FV3GFS} atmospheric model},
	volume = {14},
	issn = {1991-9603},
	shorttitle = {fv3gfs-wrapper},
	url = {https://gmd.copernicus.org/articles/14/4401/2021/},
	doi = {10.5194/gmd-14-4401-2021},
	abstract = {Abstract. Simulation software in geophysics is traditionally written in Fortran or C++ due to the stringent performance requirements these codes have to satisfy. As a result, researchers who use high-productivity languages for exploratory work often find these codes hard to understand, hard to modify, and hard to integrate with their analysis tools. fv3gfs-wrapper is an open-source Python-wrapped version of the NOAA (National Oceanic and Atmospheric Administration) FV3GFS (Finite-Volume Cubed-Sphere Global Forecast System) global atmospheric model, which is coded in Fortran. The wrapper provides simple interfaces to progress the Fortran main loop and get or set variables used by the Fortran model. These interfaces enable a wide range of use cases such as modifying the behavior of the model, introducing online analysis code, or saving model variables and reading forcings directly to and from cloud storage. Model performance is identical to the fully compiled Fortran model, unless routines to copy the state in and out of the model are used. This copy overhead is well within an acceptable range of performance and could be avoided with modifications to the Fortran source code. The wrapping approach is outlined and can be applied similarly in other Fortran models to enable more productive scientific workflows.},
	language = {en},
	number = {7},
	urldate = {2023-05-19},
	journal = {Geoscientific Model Development},
	author = {McGibbon, Jeremy and Brenowitz, Noah D. and Cheeseman, Mark and Clark, Spencer K. and Dahm, Johann P. S. and Davis, Eddie C. and Elbert, Oliver D. and George, Rhea C. and Harris, Lucas M. and Henn, Brian and Kwa, Anna and Perkins, W. Andre and Watt-Meyer, Oliver and Wicky, Tobias F. and Bretherton, Christopher S. and Fuhrer, Oliver},
	month = jul,
	year = {2021},
	pages = {4401--4409},
	file = {McGibbon et al. - 2021 - fv3gfs-wrapper a Python wrapper of the FV3GFS atm.pdf:/home/x/Library/library-ml-zotero/storage/2986GD7S/McGibbon et al. - 2021 - fv3gfs-wrapper a Python wrapper of the FV3GFS atm.pdf:application/pdf},
}

@article{dahm_pace_2023,
	title = {Pace v0.2: a {Python}-based performance-portable atmospheric model},
	volume = {16},
	issn = {1991-9603},
	shorttitle = {Pace v0.2},
	url = {https://gmd.copernicus.org/articles/16/2719/2023/},
	doi = {10.5194/gmd-16-2719-2023},
	abstract = {Abstract. Progress in leveraging current and emerging high-performance computing infrastructures using traditional weather and climate models has been slow. This has become known more broadly as the software productivity gap. With the end of Moore's law driving forward rapid specialization of hardware architectures, building simulation codes on a low-level language with hardware-specific optimizations is a significant risk. As a solution, we present Pace, an implementation of the nonhydrostatic FV3 dynamical core and GFDL cloud microphysics scheme which is entirely Python-based. In order to achieve high performance on a diverse set of hardware architectures, Pace is written using the GT4Py domain-specific language. We demonstrate that with this approach we can achieve portability and performance, while significantly improving the readability and maintainability of the code as compared to the Fortran reference implementation. We show that Pace can run at scale on leadership-class supercomputers and achieve performance speeds 3.5–4 times faster than the Fortran code on GPU-accelerated supercomputers. Furthermore, we demonstrate how a Python-based simulation code facilitates existing or enables entirely new use cases and workflows. Pace demonstrates how a high-level language can insulate us from disruptive changes, provide a more productive development environment, and facilitate the integration with new technologies such as machine learning.},
	language = {en},
	number = {9},
	urldate = {2023-05-19},
	journal = {Geoscientific Model Development},
	author = {Dahm, Johann and Davis, Eddie and Deconinck, Florian and Elbert, Oliver and George, Rhea and McGibbon, Jeremy and Wicky, Tobias and Wu, Elynn and Kung, Christopher and Ben-Nun, Tal and Harris, Lucas and Groner, Linus and Fuhrer, Oliver},
	month = may,
	year = {2023},
	pages = {2719--2736},
	file = {Dahm et al. - 2023 - Pace v0.2 a Python-based performance-portable atm.pdf:/home/x/Library/library-ml-zotero/storage/R5UGKR2K/Dahm et al. - 2023 - Pace v0.2 a Python-based performance-portable atm.pdf:application/pdf},
}

@misc{raissi_physics_2017-1,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {{PINN} {I}},
	url = {http://arxiv.org/abs/1711.10561},
	abstract = {We introduce physics informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial diﬀerential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial diﬀerential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-eﬃcient universal function approximators that naturally encode any underlying physical laws as prior information. In this ﬁrst part, we demonstrate how these networks can be used to infer solutions to partial diﬀerential equations, and obtain physics-informed surrogate models that are fully diﬀerentiable with respect to all input coordinates and free parameters.},
	language = {en},
	urldate = {2023-05-20},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv:1711.10561 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Mathematics - Dynamical Systems},
}

@misc{keisler_forecasting_2022,
	title = {Forecasting {Global} {Weather} with {Graph} {Neural} {Networks}},
	shorttitle = {Forecastin {PINN}},
	url = {http://arxiv.org/abs/2202.07575},
	doi = {10.48550/arXiv.2202.07575},
	abstract = {We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Keisler, Ryan},
	month = feb,
	year = {2022},
	note = {arXiv:2202.07575 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/2Q8QG7B8/Keisler - 2022 - Forecasting Global Weather with Graph Neural Netwo.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/C7XFZCJV/2202.html:text/html},
}

@misc{ning_graph-based_2023,
	title = {Graph-{Based} {Deep} {Learning} for {Sea} {Surface} {Temperature} {Forecasts}},
	url = {http://arxiv.org/abs/2305.09468},
	doi = {10.48550/arXiv.2305.09468},
	abstract = {Sea surface temperature (SST) forecasts help with managing the marine ecosystem and the aquaculture impacted by anthropogenic climate change. Numerical dynamical models are resource intensive for SST forecasts; machine learning (ML) models could reduce high computational requirements and have been in the focus of the research community recently. ML models normally require a large amount of data for training. Environmental data are collected on regularly-spaced grids, so early work mainly used grid-based deep learning (DL) for prediction. However, both grid data and the corresponding DL approaches have inherent problems. As geometric DL has emerged, graphs as a more generalized data structure and graph neural networks (GNNs) have been introduced to the spatiotemporal domains. In this work, we preliminarily explored graph re-sampling and GNNs for global SST forecasts, and GNNs show better one month ahead SST prediction than the persistence model in most oceans in terms of root mean square errors.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Ning, Ding and Vetrova, Varvara and Bryan, Karin R.},
	month = apr,
	year = {2023},
	note = {arXiv:2305.09468 [physics]
version: 1},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/2WTIARPE/Ning et al. - 2023 - Graph-Based Deep Learning for Sea Surface Temperat.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/77XM47Y9/2305.html:text/html},
}

@misc{agrawal_machine_2023,
	title = {A {Machine} {Learning} {Outlook}: {Post}-processing of {Global} {Medium}-range {Forecasts}},
	shorttitle = {A {Machine} {Learning} {Outlook}},
	url = {http://arxiv.org/abs/2303.16301},
	doi = {10.48550/arXiv.2303.16301},
	abstract = {Post-processing typically takes the outputs of a Numerical Weather Prediction (NWP) model and applies linear statistical techniques to produce improve localized forecasts, by including additional observations, or determining systematic errors at a finer scale. In this pilot study, we investigate the benefits and challenges of using non-linear neural network (NN) based methods to post-process multiple weather features -- temperature, moisture, wind, geopotential height, precipitable water -- at 30 vertical levels, globally and at lead times up to 7 days. We show that we can achieve accuracy improvements of up to 12\% (RMSE) in a field such as temperature at 850hPa for a 7 day forecast. However, we recognize the need to strengthen foundational work on objectively measuring a sharp and correct forecast. We discuss the challenges of using standard metrics such as root mean squared error (RMSE) or anomaly correlation coefficient (ACC) as we move from linear statistical models to more complex non-linear machine learning approaches for post-processing global weather forecasts.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Agrawal, Shreya and Carver, Rob and Gazen, Cenk and Maddy, Eric and Krasnopolsky, Vladimir and Bromberg, Carla and Ontiveros, Zack and Russell, Tyler and Hickey, Jason and Boukabara, Sid},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16301 [physics]
version: 1},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/JJ7D3J2U/Agrawal et al. - 2023 - A Machine Learning Outlook Post-processing of Glo.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/EZDDV64Z/2303.html:text/html},
}

@misc{li_towards_2023,
	title = {Towards {Spatio}-temporal {Sea} {Surface} {Temperature} {Forecasting} via {Static} and {Dynamic} {Learnable} {Personalized} {Graph} {Convolution} {Network}},
	url = {http://arxiv.org/abs/2304.09290},
	doi = {10.48550/arXiv.2304.09290},
	abstract = {Sea surface temperature (SST) is uniquely important to the Earth's atmosphere since its dynamics are a major force in shaping local and global climate and profoundly affect our ecosystems. Accurate forecasting of SST brings significant economic and social implications, for example, better preparation for extreme weather such as severe droughts or tropical cyclones months ahead. However, such a task faces unique challenges due to the intrinsic complexity and uncertainty of ocean systems. Recently, deep learning techniques, such as graphical neural networks (GNN), have been applied to address this task. Even though these methods have some success, they frequently have serious drawbacks when it comes to investigating dynamic spatiotemporal dependencies between signals. To solve this problem, this paper proposes a novel static and dynamic learnable personalized graph convolution network (SD-LPGC). Specifically, two graph learning layers are first constructed to respectively model the stable long-term and short-term evolutionary patterns hidden in the multivariate SST signals. Then, a learnable personalized convolution layer is designed to fuse this information. Our experiments on real SST datasets demonstrate the state-of-the-art performances of the proposed approach on the forecasting task.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Li, Xiaohan and Zhang, Gaowei and Huang, Kai and He, Zhaofeng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09290 [physics]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/HR7L7Y2L/Li et al. - 2023 - Towards Spatio-temporal Sea Surface Temperature Fo.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/NQXNPMGI/2304.html:text/html},
}

@mastersthesis{miranda_common_2022,
	address = {São José dos Campos},
	title = {Common {MPI}-based {HPC} {Approaches} in {Python} {Evaluated} for {Selected} {Test} {Cases}},
	url = {http://urlib.net/ibi/QABCDSTQQW/46C4U9H},
	abstract = {A number of the most common MPI-based high-performance computing approaches available in the Python programming environment of the LNCC Santos Dumont supercomputer are compared using three selected test cases. Python includes specific libraries, development tools, implementations, documentation and optimization or parallelization resources. It provides a straightforward way to allow programs to be written with a high level of abstraction, but the parallelization features to exploit multiple cores, processors or accelerators such as GPUs are diverse and may not be easily selectable by the programmer. This work compares common approaches in Python to increase computing performance for three test cases: a 2D heat transfer problem solved by the finite difference method, a 3D fast Fourier transform applied to synthetic data, and asteroid classification using a random forest. The corresponding serial and parallel implementations in Fortran 90 were taken as references to compare the computational performance. In addition to the performance results, a discussion of the trade-off between easiness of programming and computational performance is included. This work is intended as a primer for using parallel HPC resources in Python. RESUMO: Algumas das abordagens de computação de alto desempenho mais comuns baseadas em MPI disponíveis no ambiente de programação Python do supercomputador LNCC Santos Dumont são comparadas usando três casos de teste selecionados. Python inclui bibliotecas específicas, ferramentas de desenvolvimento, implementações, documentação e recursos de otimização ou paralelização. Ele fornece uma maneira direta de permitir que programas sejam escritos com um alto nível de abstração, mas os recursos de paralelização para explorar vários núcleos, processadores ou aceleradores, como GPUs, são diversos e podem não ser facilmente selecionáveis pelo programador. Este trabalho compara abordagens comuns em Python para se obter processamento de alto desempenho desempenho utilizando três casos de teste: um problema de transmissão de calor bidimensional resolvido por diferenças finitas, uma transformada rápida de Fourier tridimensional aplicada a dados sintéticos e uma classificação de asteróides por floresta aleatória. As correspondentes implementações seriais e paralelas em Fortran 90 foram tomadas como referência para comparação de desempenho nesses casos de teste. Além dos resultados de desempenho, inclui-se uma discussão sobre o compromisso entre facilidade de programação e desempenho de processamento. Este trabalho pretende ser uma introdução para o uso de recursos de processamento de alto desempenho baseados em MPI para Python.},
	language = {en},
	school = {National Institute for Space Research (INPE)},
	author = {Miranda, Eduardo Furlan},
	month = feb,
	year = {2022},
	keywords = {ambiente de programação Python, computação paralela., high performance computing, parallel computing, processamento de alto desempenho, Python programming environment},
	file = {Defesa de Mestrado de Eduardo Miranda\: Common MPI-based HPC Approaches in Python - YouTube:/home/x/Library/library-ml-zotero/storage/DAUNA9PN/watch.html:text/html;Miranda - 2022 - Common MPI-based HPC Approaches in Python Evaluate.pdf:/home/x/Library/library-ml-zotero/storage/SXTSWVCL/Miranda - 2022 - Common MPI-based HPC Approaches in Python Evaluate.pdf:application/pdf},
}

@inproceedings{miranda_comparison_2021,
	title = {Comparison of {High}-performance {Computing} {Approaches} in the {Python} {Environment} for a {Five}-point {Stencil} {Test} {Problem}},
	url = {https://sol.sbc.org.br/index.php/bresci/article/view/15786},
	doi = {10.5753/bresci.2021.15786},
	booktitle = {{XV} {Brazilian} e-{Science} {Workshop}, at {XLI} {Congress} of the {Brazilian} {Computer} {Society} ({CSBC}-2021)},
	publisher = {SBC},
	author = {Miranda, Eduardo F and Stephany, Stephan},
	year = {2021},
	pages = {33--40},
	file = {Miranda and Stephany - 2021 - Comparison of High-performance Computing Approache.pdf:/home/x/Library/library-ml-zotero/storage/2XWGICBS/Miranda and Stephany - 2021 - Comparison of High-performance Computing Approache.pdf:application/pdf},
}

@article{souza_alise_2018,
	title = {Análise de {Desempenho} de {Comunicação} {Usando} a {Funcionalidade} de {Memória} {Compartilhada} do {MPI} 3.0},
	volume = {10},
	issn = {2175-7275},
	url = {http://ojs.unirg.edu.br/index.php/1/article/view/2276},
	abstract = {Resumo
					Na execução de um programa paralelizado com a biblioteca de comunicação por troca de mensagens MPI num nó computacional de memória compartilhada, a troca de mensagens entre processos pode ocasionar uma contenção pelo acesso à memória, prejudicando a escalabilidade do programa paralelo. A versão 3.0 do MPI implementou uma nova funcionalidade, a comunicação unilateral Shared Memory (SHM) que utiliza uma janela de memória comum aos processos executados no mesmo nó computacional na qual esses processos podem efetuar leituras e escritas diretamente, sem uso de funções MPI e sem armazenamento intermediário. Este trabalho avalia o desempenhocomputacional dessa nova funcionalidade do MPI na execução de um código de diferenças finitas em C e em Fortran 90 utilizando uma máquina paralela Cray. A comunicação unilateral SHM é comparada à comunicação bilateral convencional MPI.},
	language = {pt},
	number = {2},
	urldate = {2023-05-01},
	journal = {REVISTA CEREUS},
	author = {Souza, Carlos Renato and Panetta, Jairo and Stephany, Stephan},
	month = aug,
	year = {2018},
	note = {Number: 2},
}

@article{miranda_common_2021,
	title = {Common {HPC} {Approaches} in {Python} {Evaluated} for a {Scientific} {Computing} {Test} {Case}},
	volume = {13},
	copyright = {Copyright (c) 2021 REVISTA CEREUS},
	issn = {2175-7275},
	url = {http://www.ojs.unirg.edu.br/index.php/1/article/view/3408},
	abstract = {Resumo
					Várias das abordagens de processamento de alto desempenho mais comuns disponíveis no ambiente de programação Python do supercomputador LNCC Santos Dumont são comparadas usando um caso de teste específico. Python inclui bibliotecas específicas, ferramentas de desenvolvimento, implementações, recursos de documentação e otimização/paralelização, e fornece uma maneira direta de programar em um alto nível de abstração, porém os recursos de paralelização para explorar vários núcleos, processadores ou aceleradores como GPUs, são diversos e podem não ser facilmente selecionáveis ​​pelo programador. Este trabalho faz uma comparação de abordagens comuns em Python para aumentar o desempenho computacional. O caso de teste é um conhecido problema de transmissão de calor 2D modelado pela equação diferencial parcial de Poisson, que é resolvido por um método de diferenças finitas que requer o cálculo de um estêncil de 5 pontos na grade de domínio. As implementações seriais e paralelas em Fortran 90 foram tomadas como referência para comparar o desempenho com algumas implementações Python seriais e paralelas do mesmo algoritmo. Além dos resultados de desempenho, uma discussão sobre facilidade de programação e desempenho de processamento está incluída. Este trabalho pode ser usado como um ponto de partida para a utilização de recursos PAD em Python.},
	language = {pt},
	number = {2},
	urldate = {2023-05-20},
	journal = {REVISTA CEREUS},
	author = {Miranda, Eduardo Furlan and Stephany, Stephan},
	month = jul,
	year = {2021},
	note = {Number: 2},
	pages = {84--98},
}

@misc{baker_planet_1990,
	title = {Planet {Earth}: {The} {View} from {Space}},
	shorttitle = {Planet {Earth}},
	publisher = {Harvard University Press},
	author = {Baker, D. James},
	year = {1990},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/3BHJIF4L/html.html:text/html},
}

@misc{kiefer_remote_1994,
	title = {Remote {Sensing} and {Image} {Interpretation} by {Lillesand}},
	url = {https://www.biblio.com/book/remote-sensing-image-interpretation-lillesand-thomas/d/144003002},
	abstract = {Wiley.  Used - Good.  9780399151859. . Your purchase supports More Than Words, a nonprofit job training program for youth, empowering youth to take charge of their lives by…},
	language = {en},
	urldate = {2023-05-20},
	journal = {Biblio.com},
	author = {Kiefer, Ralph W},
	year = {1994},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/PN7YJWYS/144003002.html:text/html},
}

@book{noauthor_remote_1994,
	address = {Usa},
	title = {Remote {Sensing} and {Image} {Interpretation}},
	isbn = {978-0-471-57783-6},
	url = {https://www.biblio.com/book/remote-sensing-image-interpretation/d/1536581785},
	urldate = {2023-05-20},
	publisher = {Wiley},
	month = jan,
	year = {1994},
}

@book{santos_introducao_2013,
	title = {Introdução à programação orientada a objetos usando java 2a edição},
	isbn = {978-85-352-8429-4},
	abstract = {Este livro apresenta os conceitos básicos de orientação a objetos (modelos, classes e objetos, atributos e métodos, herança, classes abstratas) e conceitos de programação (estruturas de decisão e controle, arrays, strings e estruturas simples de dados) de forma clara, detalhada e gradativa, com exemplos mais práticos que puramente conceituais, enriquecidos por exercícios de diferentes níveis de complexidade. Esta segunda edição traz novos exemplos; inclusão dos conceitos de enumeradores, laços para iteração em arrays e coleções, argumentos variáveis, argumentos variáveis, tipos genéricos e autoboxing; um índice específico para as listagens, que foram separadas das figuras, entre outras atualizações.},
	language = {pt-BR},
	publisher = {Elsevier Brasil},
	author = {Santos, Rafael},
	month = aug,
	year = {2013},
	note = {Google-Books-ID: 2pfpCgAAQBAJ},
	keywords = {Computers / Programming / General},
}

@book{press_numerical_1996,
	title = {Numerical {Recipes} in {Fortran} 90: {Volume} 2, {Volume} 2 of {Fortran} {Numerical} {Recipes}: {The} {Art} of {Parallel} {Scientific} {Computing}},
	isbn = {978-0-521-57439-6},
	shorttitle = {Numerical {Recipes} in {Fortran} 90},
	abstract = {The second volume of the Fortran Numerical Recipes series, Numerical Recipes in Fortran 90 contains a detailed introduction to the Fortran 90 language and to the basic concepts of parallel programming, plus source code for all routines from the second edition of Numerical Recipes. This volume does not repeat any of the discussion of what individual programs actually do, the mathematical methods they utilize, or how to use them.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	month = sep,
	year = {1996},
	note = {Google-Books-ID: OIZaswEACAAJ},
	keywords = {Mathematics / Numerical Analysis, Mathematics / General},
}

@misc{ricardo_picard_2021,
	title = {Picard {Iteration} - an overview},
	url = {https://www.sciencedirect.com/topics/mathematics/picard-iteration},
	urldate = {2023-06-07},
	author = {Ricardo, Henry J.},
	year = {2021},
	file = {Picard Iteration - an overview | ScienceDirect Topics:/home/x/Library/library-ml-zotero/storage/IRAYN5N2/picard-iteration.html:text/html},
}

@book{ricardo_henry_j_modern_2020,
	title = {A {Modern} {Introduction} to {Differential} {Equations}},
	isbn = {978-0-12-823417-4},
	url = {https://www.amazon.com/Modern-Introduction-Differential-Equations/dp/0128234172},
	urldate = {2023-06-07},
	author = {{Ricardo, Henry J.}},
	year = {2020},
	file = {A Modern Introduction to Differential Equations\: Ricardo, Henry J.\: 9780128234174\: Amazon.com\: Books:/home/x/Library/library-ml-zotero/storage/RWCACQMS/0128234172.html:text/html},
}

@misc{pestourie_physics-enhanced_2022,
	title = {Physics-enhanced deep surrogates for {PDEs}},
	url = {http://arxiv.org/abs/2111.05841},
	doi = {10.48550/arXiv.2111.05841},
	abstract = {We present a ''physics-enhanced deep-surrogate'' (''PEDS'') approach towards developing fast surrogate models for complex physical systems, which is described by partial differential equations (PDEs) and similar models. Specifically, a unique combination of a low-fidelity, explainable physics simulator and a neural network generator is proposed, which is trained end-to-end to globally match the output of an expensive high-fidelity numerical solver. We consider low-fidelity models derived from coarser discretizations and/or by simplifying the physical equations, which are several orders of magnitude faster than a high-fidelity ''brute-force'' PDE solver. The neural network generates an approximate input, which is adaptively mixed with a downsampled guess and fed into the low-fidelity simulator. In this way, by incorporating the limited physical knowledge from the differentiable low-fidelity model ''layer'', we ensure that the conservation laws and symmetries governing the system are respected by the design of our hybrid system. Experiments on three test problems -- diffusion, reaction-diffusion, and electromagnetic scattering models -- show that a PEDS surrogate can be up to 3\${\textbackslash}times\$ more accurate than a ''black-box'' neural network with limited data (\${\textbackslash}approx 10{\textasciicircum}3\$ training points), and reduces the data needed by at least a factor of 100 for a target error of \$5{\textbackslash}\%\$, comparable to fabrication uncertainty. PEDS even appears to learn with a steeper asymptotic power law than black-box surrogates. In summary, PEDS provides a general, data-driven strategy to bridge the gap between a vast array of simplified physical models with corresponding brute-force numerical solvers, offering accuracy, speed, data efficiency, as well as physical insights into the process.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Pestourie, Raphaël and Mroueh, Youssef and Rackauckas, Chris and Das, Payel and Johnson, Steven G.},
	month = nov,
	year = {2022},
	note = {arXiv:2111.05841 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Applied Physics},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/743CUBYR/2111.html:text/html;Pestourie et al. - 2022 - Physics-enhanced deep surrogates for PDEs.pdf:/home/x/Library/library-ml-zotero/storage/PG3C33N6/Pestourie et al. - 2022 - Physics-enhanced deep surrogates for PDEs.pdf:application/pdf},
}

@article{chen_improved_2021,
	title = {An improved data-free surrogate model for solving partial differential equations using deep neural networks},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-99037-x},
	doi = {10.1038/s41598-021-99037-x},
	abstract = {Partial differential equations (PDEs) are ubiquitous in natural science and engineering problems. Traditional discrete methods for solving PDEs are usually time-consuming and labor-intensive due to the need for tedious mesh generation and numerical iterations. Recently, deep neural networks have shown new promise in cost-effective surrogate modeling because of their universal function approximation abilities. In this paper, we borrow the idea from physics-informed neural networks (PINNs) and propose an improved data-free surrogate model, DFS-Net. Specifically, we devise an attention-based neural structure containing a weighting mechanism to alleviate the problem of unstable or inaccurate predictions by PINNs. The proposed DFS-Net takes expanded spatial and temporal coordinates as the input and directly outputs the observables (quantities of interest). It approximates the PDE solution by minimizing the weighted residuals of the governing equations and data-fit terms, where no simulation or measured data are needed. The experimental results demonstrate that DFS-Net offers a good trade-off between accuracy and efficiency. It outperforms the widely used surrogate models in terms of prediction performance on different numerical benchmarks, including the Helmholtz, Klein–Gordon, and Navier–Stokes equations.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Scientific Reports},
	author = {Chen, Xinhai and Chen, Rongliang and Wan, Qian and Xu, Rui and Liu, Jie},
	month = sep,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computational science, Computer science},
	pages = {19507},
	file = {Chen et al. - 2021 - An improved data-free surrogate model for solving .pdf:/home/x/Library/library-ml-zotero/storage/5EZ5Q4YZ/Chen et al. - 2021 - An improved data-free surrogate model for solving .pdf:application/pdf},
}

@article{wasei_investigating_2020,
	title = {Investigating {Physics}-{Informed} {Neural} {Networks} for solving {PDEs}},
	url = {https://repository.tudelft.nl/islandora/object/uuid%3A09fcb08c-370d-45b6-b887-8dd8aa908ffd},
	abstract = {Physics-Informed Neural Networks (PINNs) are a new class of numerical methods for solving partial differential equations (PDEs) that have been very promising. In this paper, four different implementations will be tested and compared. These include: the original PINN functional with equal weights for the interior and boundary loss, the same functional with custom weights, and the First Order Least Squares (FOSLS) functional with equal weights and custom weights. These custom weights are chosen to be equal to the optimal weights derived by Oosterlee et al. as well as slightly bigger and smaller. These methods will be applied to the 1D stationary advection-diffusion equation where we vary the difficulty by configuring the diffusion parameter epsilon. Furthermore, for each method we have done an elaborate parameter study where we varied epsilon and the number of collocation points. We have found that the weights derived by Oosterlee et al. did not provide accurate results. Instead, equal weights usually performed best. Also, the two functionals turned out to have very similar performance.},
	language = {en},
	urldate = {2023-06-18},
	author = {Wasei, E. a. A.},
	year = {2020},
	file = {Wasei - 2020 - Investigating Physics-Informed Neural Networks for.pdf:/home/x/Library/library-ml-zotero/storage/48CNQTLF/Wasei - 2020 - Investigating Physics-Informed Neural Networks for.pdf:application/pdf},
}

@misc{haitsiukevich_improved_2022,
	title = {Improved {Training} of {Physics}-{Informed} {Neural} {Networks} with {Model} {Ensembles}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220405108H},
	doi = {10.48550/arXiv.2204.05108},
	abstract = {Learning the solution of partial differential equations (PDEs) with a neural network is an attractive alternative to traditional solvers due to its elegance, greater flexibility and the ease of incorporating observed data. However, training such physics-informed neural networks (PINNs) is notoriously difficult in practice since PINNs often converge to wrong solutions. In this paper, we address this problem by training an ensemble of PINNs. Our approach is motivated by the observation that individual PINN models find similar solutions in the vicinity of points with targets (e.g., observed data or initial conditions) while their solutions may substantially differ farther away from such points. Therefore, we propose to use the ensemble agreement as the criterion for gradual expansion of the solution interval, that is including new points for computing the loss derived from differential equations. Due to the flexibility of the domain expansion, our algorithm can easily incorporate measurements in arbitrary locations. In contrast to the existing PINN algorithms with time-adaptive strategies, the proposed algorithm does not need a pre-defined schedule of interval expansion and it treats time and space equally. We experimentally show that the proposed algorithm can stabilize PINN training and yield performance competitive to the recent variants of PINNs trained with time adaptation.},
	urldate = {2023-06-18},
	author = {Haitsiukevich, Katsiaryna and Ilin, Alexander},
	month = apr,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv220405108H},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Haitsiukevich and Ilin - 2022 - Improved Training of Physics-Informed Neural Netwo.pdf:/home/x/Library/library-ml-zotero/storage/RBWHJ3L4/Haitsiukevich and Ilin - 2022 - Improved Training of Physics-Informed Neural Netwo.pdf:application/pdf},
}

@article{lu_learning_2021,
	title = {Learning nonlinear operators via {DeepONet} based on the universal approximation theorem of operators},
	volume = {3},
	doi = {10.1038/s42256-021-00302-5},
	abstract = {It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications. Neural networks are known as universal approximators of continuous functions, but they can also approximate any mathematical operator (mapping a function to another function), which is an important capability for complex systems such as robotics control. A new deep neural network called DeepONet can lean various mathematical operators with small generalization error.},
	journal = {Nature Machine Intelligence},
	author = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George},
	month = mar,
	year = {2021},
	pages = {218--229},
	file = {Lu et al. - 2021 - Learning nonlinear operators via DeepONet based on.pdf:/home/x/Library/library-ml-zotero/storage/3DHNBQ3T/Lu et al. - 2021 - Learning nonlinear operators via DeepONet based on.pdf:application/pdf},
}

@article{zubov_neuralpde_2021,
	title = {{NeuralPDE}: {Automating} {Physics}-{Informed} {Neural} {Networks} ({PINNs}) with {Error} {Approximations}},
	language = {en},
	author = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},
	year = {2021},
	file = {Zubov et al. - NeuralPDE Automating Physics-Informed Neural Netw.pdf:/home/x/Library/library-ml-zotero/storage/UZPIM466/Zubov et al. - NeuralPDE Automating Physics-Informed Neural Netw.pdf:application/pdf},
}

@article{li_deep_2021,
	title = {Deep learning-based method coupled with small sample learning for solving partial differential equations},
	volume = {80},
	doi = {10.1007/s11042-020-09142-8},
	abstract = {Partial differential equations (PDEs) are existing widely in the field of mathematics, physics and engineering. They are often used to describe natural phenomena and model dynamical systems, but how to solve the equations efficiently is still a hard task. In this paper, we develop a deep learning-based general numerical method coupled with small sample learning (SSL) for solving PDEs. To be more specific, we approximate the solution via a deep feedforward neural network, which is trained to satisfy the PDEs with the initial and boundary conditions. Then the proposed method is modeled to solve an optimization problem by minimizing a designed cost function, which involves the residual of the differential equations, the initial/boundary conditions and the residual of a handful of observations. With a few of sample data, the model can be rectified effectively and the predictive accuracy can be improved. The effectiveness of the proposed method is demonstrated by a wide range of benchmark problems in mathematical physics, including the classical Burgers equations, Schrödinger equations, Buckley-Leverett equation, Navier-Stokes equation, and Carburizing diffusion equations, which are applied in carburizing diffusion problems in material science. And the results validate that the proposed algorithm is effective, flexible and robust without relying on trial solutions.},
	journal = {Multimedia Tools and Applications},
	author = {Li, Ying and Mei, Fangjun},
	month = may,
	year = {2021},
	file = {Li and Mei - 2021 - Deep learning-based method coupled with small samp.pdf:/home/x/Library/library-ml-zotero/storage/SL6KYZ3A/Li and Mei - 2021 - Deep learning-based method coupled with small samp.pdf:application/pdf},
}

@misc{brunton_machine_2023,
	title = {Machine {Learning} for {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2303.17078},
	abstract = {Partial differential equations (PDEs) are among the most universal and parsimonious descriptions of natural physical laws, capturing a rich variety of phenomenology and multi-scale physics in a compact and symbolic representation. This review will examine several promising avenues of PDE research that are being advanced by machine learning, including: 1) the discovery of new governing PDEs and coarse-grained approximations for complex natural and engineered systems, 2) learning effective coordinate systems and reduced-order models to make PDEs more amenable to analysis, and 3) representing solution operators and improving traditional numerical algorithms. In each of these ﬁelds, we summarize key advances, ongoing challenges, and opportunities for further development.},
	language = {en},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Brunton, Steven L. and Kutz, J. Nathan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17078 [physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Fluid Dynamics, Mathematics - Dynamical Systems, Mathematics - Analysis of PDEs},
	file = {Brunton and Kutz - 2023 - Machine Learning for Partial Differential Equation.pdf:/home/x/Library/library-ml-zotero/storage/L2ALQR9H/Brunton and Kutz - 2023 - Machine Learning for Partial Differential Equation.pdf:application/pdf},
}

@misc{huang_partial_2022,
	title = {Partial {Differential} {Equations} {Meet} {Deep} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Partial {Differential} {Equations} {Meet} {Deep} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2211.05567v2},
	abstract = {Many problems in science and engineering can be represented by a set of partial differential equations (PDEs) through mathematical modeling. Mechanism-based computation following PDEs has long been an essential paradigm for studying topics such as computational fluid dynamics, multiphysics simulation, molecular dynamics, or even dynamical systems. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. At the same time, solving PDEs efficiently has been a long-standing challenge. Generally, except for a few differential equations for which analytical solutions are directly available, many more equations must rely on numerical approaches such as the finite difference method, finite element method, finite volume method, and boundary element method to be solved approximately. These numerical methods usually divide a continuous problem domain into discrete points and then concentrate on solving the system at each of those points. Though the effectiveness of these traditional numerical methods, the vast number of iterative operations accompanying each step forward significantly reduces the efficiency. Recently, another equally important paradigm, data-based computation represented by deep learning, has emerged as an effective means of solving PDEs. Surprisingly, a comprehensive review for this interesting subfield is still lacking. This survey aims to categorize and review the current progress on Deep Neural Networks (DNNs) for PDEs. We discuss the literature published in this subfield over the past decades and present them in a common taxonomy, followed by an overview and classification of applications of these related methods in scientific research and engineering scenarios. The origin, developing history, character, sort, as well as the future trends in each potential direction of this subfield are also introduced.},
	language = {en},
	urldate = {2023-06-20},
	journal = {arXiv.org},
	author = {Huang, Shudong and Feng, Wentao and Tang, Chenwei and Lv, Jiancheng},
	month = oct,
	year = {2022},
	file = {Huang et al. - 2022 - Partial Differential Equations Meet Deep Neural Ne.pdf:/home/x/Library/library-ml-zotero/storage/H2AEHKD2/Huang et al. - 2022 - Partial Differential Equations Meet Deep Neural Ne.pdf:application/pdf},
}

@book{albeverio_extreme_2006,
	address = {Berlin, Heidelberg},
	series = {The {Frontiers} {Collection}},
	title = {Extreme {Events} in {Nature} and {Society}},
	isbn = {978-3-540-28610-3 978-3-540-28611-0},
	url = {http://link.springer.com/10.1007/3-540-28611-X},
	language = {en},
	urldate = {2023-06-21},
	publisher = {Springer},
	editor = {Albeverio, Sergio and Jentsch, Volker and Kantz, Holger and Dragoman, Daniela and Dragoman, Mircea and Elitzur, Avshalom C. and Silverman, Mark P. and Tuszynski, Jack and Zeh, H. Dieter},
	year = {2006},
	doi = {10.1007/3-540-28611-X},
	keywords = {Dynamics, Extreme events, Financial Crisis, Forecasting, Management of extreme events, meteorology, modeling, Precipitation, Prediction, Simulation, simulations, statistics, wind},
	file = {Albeverio et al. - 2006 - Extreme Events in Nature and Society.pdf:/home/x/Library/library-ml-zotero/storage/CIN2DT2Y/Albeverio et al. - 2006 - Extreme Events in Nature and Society.pdf:application/pdf},
}

@incollection{stephany_data_2019,
	address = {Cham},
	title = {Data {Mining} {Approaches} to the {Real}-{Time} {Monitoring} and {Early} {Warning} of {Convective} {Weather} {Using} {Lightning} {Data}},
	isbn = {978-3-030-21205-6},
	url = {https://doi.org/10.1007/978-3-030-21205-6_5},
	abstract = {Tracking and monitoring of convective events may require the analysis of a huge amount of data from sensors on the ground, such as weather radars, or on board of satellites, in addition to the forecasts of numerical models. Thunderstorms associated with severe convective events have a potential to cause strong winds, floods, and landslides, with serious environmental and socio-economic impacts. New approaches based on data mining have been proposed for countries like Brazil that lack a complete weather radar coverage, but have some ground-based lightning detector networks. Lightning data may help to visualize the current state of convective systems in near real-time or to estimate the amount of convective precipitation in a given area and period of time. Data mining algorithms can be trained using numerical model data and lightning data yielding specific data mining models, which can be used to predict the occurrence of convective activity from numerical model forecasts. These data mining models may help meteorologists to improve the accuracy of early warnings and forecastings, mainly in countries that lack a complete weather radar coverage.},
	language = {en},
	urldate = {2023-06-21},
	booktitle = {Towards {Mathematics}, {Computers} and {Environment}: {A} {Disasters} {Perspective}},
	publisher = {Springer International Publishing},
	author = {Stephany, Stephan and Strauss, Cesar and Calheiros, Alan James Peixoto and de Lima, Glauston Roberto Teixeira and Garcia, João Victor Cal and Pessoa, Alex Sandro Aguiar},
	editor = {Bacelar Lima Santos, Leonardo and Galante Negri, Rogério and de Carvalho, Tiago José},
	year = {2019},
	doi = {10.1007/978-3-030-21205-6_5},
	pages = {83--101},
	file = {Stephany et al. - 2019 - Data Mining Approaches to the Real-Time Monitoring.pdf:/home/x/Library/library-ml-zotero/storage/N2ZKSTDH/Stephany et al. - 2019 - Data Mining Approaches to the Real-Time Monitoring.pdf:application/pdf},
}

@incollection{santos_about_2019,
	address = {Cham},
	title = {About {Interfaces} {Between} {Machine} {Learning}, {Complex} {Networks}, {Survivability} {Analysis}, and {Disaster} {Risk} {Reduction}},
	isbn = {978-3-030-21205-6},
	url = {https://doi.org/10.1007/978-3-030-21205-6_10},
	abstract = {Modern society strongly relies on critical infrastructures such as telecommunications, transport networks, and the supply of gas, water, and energy. Such infrastructures, which are often exposed to natural hazards, can cause significant damage when disrupted. Among the different strategies to prevent these disruptions and cope with preparedness, mathematical models can be used to support managers in several approaches, as classification and estimation problems using machine learning, vulnerability quantification on complex networks, and survivability analysis. Nevertheless, the assessment of these quantities demands a solid conceptual discussion. In this chapter, we explore concepts of non-linear dynamics, complex systems, machine learning, and survivability analysis in the context of disaster risk reduction.},
	language = {en},
	urldate = {2023-06-21},
	booktitle = {Towards {Mathematics}, {Computers} and {Environment}: {A} {Disasters} {Perspective}},
	publisher = {Springer International Publishing},
	author = {Santos, Leonardo Bacelar Lima and Londe, Luciana R. and de Carvalho, Tiago José and S. Menasché, Daniel and Vega-Oliveros, Didier A.},
	editor = {Bacelar Lima Santos, Leonardo and Galante Negri, Rogério and de Carvalho, Tiago José},
	year = {2019},
	doi = {10.1007/978-3-030-21205-6_10},
	pages = {185--215},
}

@article{zhu_numerical_2010,
	title = {Numerical solutions of two-dimensional {Burgers}’ equations by discrete {Adomian} decomposition method},
	volume = {60},
	issn = {08981221},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0898122110003883},
	doi = {10.1016/j.camwa.2010.05.031},
	abstract = {In this paper, the discrete Adomian decomposition method (ADM) is proposed to numerically solve the two-dimensional Burgers’ nonlinear difference equations. Two test problems are considered to illustrate the accuracy of the proposed discrete decomposition method. It is shown that the numerical results are in good agreement with the exact solutions for each problem.},
	language = {en},
	number = {3},
	urldate = {2023-06-22},
	journal = {Computers \& Mathematics with Applications},
	author = {Zhu, Hongqing and Shu, Huazhong and Ding, Meiyu},
	month = aug,
	year = {2010},
	pages = {840--848},
	file = {Zhu et al. - 2010 - Numerical solutions of two-dimensional Burgers’ eq.pdf:/home/x/Library/library-ml-zotero/storage/L43BKZLR/Zhu et al. - 2010 - Numerical solutions of two-dimensional Burgers’ eq.pdf:application/pdf},
}

@article{kannan_finite_2002,
	title = {Finite difference approximate solutions for the two-dimensional {Burgers}' system},
	volume = {44},
	issn = {0898-1221},
	url = {https://www.sciencedirect.com/science/article/pii/S0898122102001402},
	doi = {10.1016/S0898-1221(02)00140-2},
	abstract = {Finite difference approximate solutions for the two-dimensional Burgers system which models a turbulent flow in a channel. Implicit Euler method is applied to obtain approximate solutions. Existence of solutions is shown by using Leray-Schauder fixed-point theorem. Stability and uniqueness of the solution are also shown by judicious applications of the discrete Gronwall's inequality and energy methods.},
	language = {en},
	number = {1},
	urldate = {2023-06-22},
	journal = {Computers \& Mathematics with Applications},
	author = {Kannan, R. and Chung, S. K.},
	month = jul,
	year = {2002},
	keywords = {Burgers' system, Discrete Gronwall's inequality, Leray-Schauder fixed-point theorem},
	pages = {193--200},
	file = {Kannan and Chung - 2002 - Finite difference approximate solutions for the tw.pdf:/home/x/Library/library-ml-zotero/storage/3BYAZIS2/Kannan and Chung - 2002 - Finite difference approximate solutions for the tw.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/P6472GQ5/S0898122102001402.html:text/html},
}

@article{yagmurlu_finite_2022,
	title = {A {Finite} {Difference} {Approximation} for {Numerical} {Simulation} of {2D} {Viscous} {Coupled} {Burgers} {Equations}},
	volume = {10},
	issn = {2147-6268},
	url = {http://dergipark.org.tr/en/doi/10.36753/mathenot.981131},
	doi = {10.36753/mathenot.981131},
	abstract = {Many of the physical phenomena in nature are usually expressed in terms of algebraic, differential or integral equations.Several nonlinear phenomena playing a very important role in engineering sciences, physics and computational mathematics are usually modeled by those non-linear partial differential equations (PDEs). It is usually difﬁcult and problematic to examine and ﬁnd out nalytical solutions of initial-boundary value problems consisting of PDEs. In fact, there is no a certain method or technique working well for all these type equations. For this reason, their approximate solutions are usually preferred rather than analytical ones of such type equations. Thus, many researchers are concentrated on approximate methods and techniques to obtain numerical solutions of non-linear PDEs. In the present article, the numerical simulation of the two-dimensional coupled Burgers equation (2D-cBE) has been sought by ﬁnite difference method based on Crank-Nicolson type approximation. Widely used three test examples given with appropriate initial and boundary conditions are used for the simulation process. During the simulation process,the error norms L2, L∞ are calculated if the exact solutions are already known, otherwise the pointwise values and graphics are provided for comparison. The newly obtained error norms L2, L∞ by the presented schemes are compared with those of some of the numerical solutions in the literature. A good consistency and accuracy are observed both by numerical values and visual illustrations.},
	language = {en},
	number = {3},
	urldate = {2023-06-22},
	journal = {Mathematical Sciences and Applications E-Notes},
	author = {Yağmurlu, Murat and Gagi̇R, Abdulnasır},
	month = sep,
	year = {2022},
	pages = {146--158},
	file = {Yağmurlu and Gagi̇R - 2022 - A Finite Difference Approximation for Numerical Si.pdf:/home/x/Library/library-ml-zotero/storage/FV5F4TWS/Yağmurlu and Gagi̇R - 2022 - A Finite Difference Approximation for Numerical Si.pdf:application/pdf},
}

@article{celikten_implicit_2022,
	title = {An implicit finite difference scheme for the numerical solutions of two-dimensional {Burgers} equations},
	volume = {53},
	url = {https://ideas.repec.org//a/spr/indpam/v53y2022i1d10.1007_s13226-021-00031-w.html},
	abstract = {In this study, the system of two-dimensional Burgers equations is solved by a new approximation that approaches the solution at two time legs: approximation is explicit in x-direction and implicit in y-direction at the first leg while approximation is implicit in x-direction and explicit in y-direction at the second leg. Two test problems are used to illustrate the accuracy of the present approximation. Comparisons are made with the existing methods in the literature. The approximation is analyzed by von-Neumann stability analysis method and it is displayed that the approximation is unconditionally stable. The method is shown to be consistent and second order accurate in time and space. The obtained results show that the present approximation is successful to solve the system of two-dimensional Burgers equations.},
	language = {en},
	number = {1},
	urldate = {2023-06-23},
	journal = {Indian Journal of Pure and Applied Mathematics},
	author = {Çelikten, Gonca},
	year = {2022},
	note = {Publisher: Springer},
	keywords = {Two-dimensional Burgers equation, Von-Neumann stability analysis},
	pages = {246--260},
	file = {Çelikten - 2022 - An implicit finite difference scheme for the numer.pdf:/home/x/Library/library-ml-zotero/storage/72AXUKSV/Çelikten - 2022 - An implicit finite difference scheme for the numer.pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/BCBBALF2/v53y2022i1d10.1007_s13226-021-00031-w.html:text/html},
}

@article{fan_generalized_2014,
	title = {Generalized {Finite} {Difference} {Method} for {Solving} {Two}-dimensional {Burgers}’ {Equations}},
	volume = {79},
	doi = {10.1016/j.proeng.2014.06.310},
	abstract = {In this paper, the two-dimensional Burgers’ equations are numerically analyzed by a meshfree numerical scheme, which is a combination of the implicit Euler method, the generalized finite difference method (GFDM) and the fictitious time integration method (FTIM). Since both of the convective and the diffusive terms simultaneously appear in the time-dependent quasi-linear Burgers’ equations, it is necessary and very difficult to develop a reliable numerical scheme to solve it. The GFDM, which can truly get rid of time-consuming mesh generation and numerical quadrature, and the implicit Euler method are used for spatial and temporal discretization, respectively. Then, the resultant system of nonlinear algebraic equations for every time step is resolved by the newly-developed FTIM. Since, in comparing with the Newton's method, the calculation of the inverse of Jacobian matrix can be avoided in the FTIM, to adopt the FTIM for solving the system of nonlinear algebraic equations is very efficient and has great potential for large-scale engineering problems. Some numerical results and comparisons are provided to validate the accuracy and the simplicity of the proposed meshfree scheme.},
	journal = {Procedia Engineering},
	author = {Fan, Chia-Ming and Li, Po-Wei},
	month = dec,
	year = {2014},
	file = {Fan and Li - 2014 - Generalized Finite Difference Method for Solving T.pdf:/home/x/Library/library-ml-zotero/storage/CVT34FNA/Fan and Li - 2014 - Generalized Finite Difference Method for Solving T.pdf:application/pdf},
}

@article{mittal_numerical_2015,
	title = {Numerical {Solutions} of {Two}-{Dimensional} {Burgers}’ {Equations} using {Modified} {Bi}-cubic {B}-{Spline} {Finite} {Elements}},
	volume = {32},
	doi = {10.1108/EC-04-2014-0067},
	abstract = {Purpose
– The purpose of this paper is to develop an efficient numerical scheme for non-linear two-dimensional (2D) parabolic partial differential equations using modified bi-cubic B-spline functions. As a test case, method has been applied successfully to 2D Burgers equations.

Design/methodology/approach
– The scheme is based on collocation of modified bi-cubic B-Spline functions. The authors used these functions for space variable and for its derivatives. Collocation form of the partial differential equation results into system of first-order ordinary differential equations (ODEs). The obtained system of ODEs has been solved by strong stability preserving Runge-Kutta method. The computational complexity of the method is O(p log(p)), where p denotes total number of mesh points.

Findings
– Obtained numerical solutions are better than those available in literature. Ease of implementation and very small size of computational work are two major advantages of the present method. Moreover, this method provides approximate solutions not only at the grid points but also at any point in the solution domain.

Originality/value
– First time, modified bi-cubic B-spline functions have been applied to non-linear 2D parabolic partial differential equations. Efficiency of the proposed method has been confirmed with numerical experiments. The authors conclude that the method provides convergent approximations and handles the equations very well in different cases.},
	journal = {Engineering Computations},
	author = {Mittal, R. C. and Tripathi, Amit},
	month = jul,
	year = {2015},
	pages = {1275--1306},
	file = {Mittal and Tripathi - 2015 - Numerical Solutions of Two-Dimensional Burgers’ Eq.pdf:/home/x/Library/library-ml-zotero/storage/YEC48FGJ/Mittal and Tripathi - 2015 - Numerical Solutions of Two-Dimensional Burgers’ Eq.pdf:application/pdf},
}

@article{fletcher_generating_1983,
	title = {Generating exact solutions of the two-dimensional {Burgers}' equations},
	volume = {3},
	copyright = {Copyright © 1983 John Wiley \& Sons, Ltd},
	issn = {1097-0363},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/fld.1650030302},
	doi = {10.1002/fld.1650030302},
	language = {en},
	number = {3},
	urldate = {2023-06-23},
	journal = {International Journal for Numerical Methods in Fluids},
	author = {Fletcher, Clive A. J.},
	year = {1983},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/fld.1650030302},
	keywords = {Burgers' Equations, Exact Solution, Fluid Flow},
	pages = {213--216},
	file = {Fletcher - 1983 - Generating exact solutions of the two-dimensional .pdf:/home/x/Library/library-ml-zotero/storage/MM9C5LT8/Fletcher - 1983 - Generating exact solutions of the two-dimensional .pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/IN6CVPMB/fld.html:text/html},
}

@article{bahadir_fully_2003,
	title = {A fully implicit finite-difference scheme for two-dimensional {Burgers}’ equations},
	volume = {137},
	issn = {0096-3003},
	url = {https://www.sciencedirect.com/science/article/pii/S0096300302000917},
	doi = {10.1016/S0096-3003(02)00091-7},
	abstract = {The two-dimensional Burgers’ equations are discretized in fully implicit finite-difference form. This scheme leads to a system of nonlinear difference equations to be solved at each time-step. Newton’s method is used to solve this nonlinear system. The linear system is solved by a direct method at each iteration of Newton’s method. The accuracy of the proposed numerical scheme is examined by comparison with other analytical and numerical results. The present method performs well.},
	language = {en},
	number = {1},
	urldate = {2023-06-23},
	journal = {Applied Mathematics and Computation},
	author = {Bahadır, A. Refik},
	month = may,
	year = {2003},
	keywords = {Burgers’ equation, Implicit finite-differences},
	pages = {131--137},
	file = {Bahadır - 2003 - A fully implicit finite-difference scheme for two-.pdf:/home/x/Library/library-ml-zotero/storage/V4YXGSSK/Bahadır - 2003 - A fully implicit finite-difference scheme for two-.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/VJI8BLNP/S0096300302000917.html:text/html},
}

@article{yaghoobi_fully_2020,
	title = {A fully implicit non-standard finite difference scheme for one dimensional {Burgers}' equation},
	volume = {7},
	url = {https://doi.org/10.22105/jarie.2021.244715.1188},
	doi = {10.22105/jarie.2021.244715.1188},
	language = {en},
	number = {3},
	urldate = {2023-06-23},
	journal = {Journal of Applied Research on Industrial Engineering},
	author = {Yaghoobi, Abdolrahman and Saberi Najafi, Hashem},
	month = sep,
	year = {2020},
	file = {Yaghoobi and Saberi Najafi - 2020 - A fully implicit non-standard finite difference sc.pdf:/home/x/Library/library-ml-zotero/storage/GS765R4E/Yaghoobi and Saberi Najafi - 2020 - A fully implicit non-standard finite difference sc.pdf:application/pdf},
}

@article{sun_second-order_2023,
	title = {A {Second}-{Order} {Network} {Structure} {Based} on {Gradient}-{Enhanced} {Physics}-{Informed} {Neural} {Networks} for {Solving} {Parabolic} {Partial} {Differential} {Equations}},
	volume = {25},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/25/4/674},
	doi = {10.3390/e25040674},
	abstract = {Physics-informed neural networks (PINNs) are effective for solving partial differential equations (PDEs). This method of embedding partial differential equations and their initial boundary conditions into the loss functions of neural networks has successfully solved forward and inverse PDE problems. In this study, we considered a parametric light wave equation, discretized it using the central difference, and, through this difference scheme, constructed a new neural network structure named the second-order neural network structure. Additionally, we used the adaptive activation function strategy and gradient-enhanced strategy to improve the performance of the neural network and used the deep mixed residual method (MIM) to reduce the high computational cost caused by the enhanced gradient. At the end of this paper, we give some numerical examples of nonlinear parabolic partial differential equations to verify the effectiveness of the method.},
	language = {en},
	number = {4},
	urldate = {2023-06-23},
	journal = {Entropy},
	author = {Sun, Kuo and Feng, Xinlong},
	month = apr,
	year = {2023},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep mixed residual method, parabolic partial differential equations, physics-informed neural networks, second-order neural network},
	pages = {674},
	file = {Sun and Feng - 2023 - A Second-Order Network Structure Based on Gradient.pdf:/home/x/Library/library-ml-zotero/storage/G59P29VU/Sun and Feng - 2023 - A Second-Order Network Structure Based on Gradient.pdf:application/pdf},
}

@article{mohamed_solving_2019,
	title = {Solving one- and two-dimensional unsteady {Burgers}' equation using fully implicit finite difference schemes},
	volume = {26},
	doi = {10.1080/25765299.2019.1613746},
	abstract = {This paper introduces new fully implicit numerical schemes for solving 1D and 2D unsteady Burgers' equation. The non-linear Burgers' equation is discretized in the spatial direction by using second order Finite difference method which converts the Burgers' equation to non-linear system of ODEs. Then, the backward differentiation formula of order two (BDF-2) is employed to march the solution in the time direction. The non-linear term in the obtained system is linearized without any transformation; and it forms a system of linear algebraic equations that is solved by using Thomas's algorithm. Accuracy and performance of the proposed schemes are studied by using four test problems with Dirichlet and Neumann boundary conditions. Comparing the numerical results with exact solutions and the solutions of other schemes shows that the proposed schemes are simple, efficient and accurate even for the cases with high Reynolds numbers.},
	journal = {Arab Journal of Basic and Applied Sciences},
	author = {Mohamed, N.},
	month = jan,
	year = {2019},
	pages = {254--268},
	file = {Mohamed - 2019 - Solving one- and two-dimensional unsteady Burgers'.pdf:/home/x/Library/library-ml-zotero/storage/GM95X3A9/Mohamed - 2019 - Solving one- and two-dimensional unsteady Burgers'.pdf:application/pdf},
}

@article{zarzur_numerical_2018,
	title = {Numerical {Experiments} for {Time} {Integration} of {2D} {Burgers}’ {Equations}},
	volume = {6},
	copyright = {Copyright (c) 2018 Proceeding Series of the Brazilian Society of Computational and Applied Mathematics},
	issn = {2359-0793},
	url = {https://proceedings.sbmac.org.br/sbmac/article/view/2025},
	abstract = {Numerical solution of partial differential equations requires the choice of a time integration method capable of simulating the evolution of a problem. While traditional methods are usually categorized into explicit and implicit, each with their own sets of advantages and disadvantages, a more recent approach is the combination of both types into the so called IMEX schemes. These were designed to solve equations containing fast and slow time-scales in such a way that the slow terms can be solved explicitly, while the slow terms are solved implicitly, mitigating the disadvantages of each individual scheme. In this work, the finite difference approach is used to solve a two-dimensional, viscous Burgers’ system in a series of numerical experiments using each of the aforementioned time integration schemes},
	language = {pt},
	number = {1},
	urldate = {2023-06-23},
	journal = {Proceeding Series of the Brazilian Society of Computational and Applied Mathematics},
	author = {Zarzur, Antonio Maurıcio and Stephany, Stephan and Freitas, Saulo Ribeiro de and Velho, Haroldo Fraga de Campos},
	month = feb,
	year = {2018},
	note = {Number: 1},
	file = {Zarzur et al. - 2018 - Numerical Experiments for Time Integration of 2D B.pdf:/home/x/Library/library-ml-zotero/storage/LAJ2XWWW/Zarzur et al. - 2018 - Numerical Experiments for Time Integration of 2D B.pdf:application/pdf},
}

@article{yang_class_2021,
	title = {A class of compact finite difference schemes for solving the {2D} and {3D} {Burgers}’ equations},
	volume = {185},
	issn = {0378-4754},
	url = {https://www.sciencedirect.com/science/article/pii/S0378475421000100},
	doi = {10.1016/j.matcom.2021.01.009},
	abstract = {In this paper, a class of two-level high-order compact finite difference implicit schemes are proposed for solving the Burgers’ equations. Firstly, based on the fourth-order compact finite difference schemes for spatial derivatives and the truncation error remainder correction method for temporal derivative, the high-order compact (HOC) difference method is introduced for solving the one-dimensional (1D) Burgers’ equation. At the same time, the stability of the scheme is analyzed by using the Fourier analysis method. Because only three grid points are involved in each time level. The Thomas algorithm can be directly used to solve the tridiagonal linear system. Then, this method is extended to solve the two-dimensional (2D) and three-dimensional (3D) coupled Burgers’ equations. Finally, numerical experiments are conducted to verify the accuracy and the reliability of the present schemes.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Mathematics and Computers in Simulation},
	author = {Yang, Xiaojia and Ge, Yongbin and Lan, Bin},
	month = jul,
	year = {2021},
	keywords = {Burgers’ equation, Compact difference scheme, High accuracy, Implicit scheme, Stability analysis},
	pages = {510--534},
	file = {ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/GSU4R5FJ/S0378475421000100.html:text/html;Yang et al. - 2021 - A class of compact finite difference schemes for s.pdf:/home/x/Library/library-ml-zotero/storage/TVH57M2Z/Yang et al. - 2021 - A class of compact finite difference schemes for s.pdf:application/pdf},
}

@article{shukla_numerical_2014,
	title = {Numerical {Solution} of two dimensional coupled viscous {Burgers}’ {Equation} using the {Modified} {Cubic} {B}-{Spline} {Differential} {Quadrature} {Method}},
	abstract = {In this paper, a numerical solution of the two dimensional nonlinear coupled viscous Burgers’ equation is discussed with the appropriate initial and boundary conditions using the modified cubic B-spline differential quadrature method (MCB-DQM). In this method, the weighting coefficients are computed using the modified cubic B-spline as a basis function in the differential quadrature method. Thus, the coupled Burgers’ equations are reduced into a system of ordinary differential equations (ODEs). An optimal five stage and fourth-order strong stability preserving Runge–Kutta (SSP-RK54) scheme is applied to solve the resulting system of ODEs. The accuracy of the scheme is illustrated via two numerical examples. Computed results are compared with the exact solutions and other results available in the literature. Numerical results show that the MCB-DQM is efficient and reliable scheme for solving the two dimensional coupled Burgers’ equation.},
	language = {en},
	author = {Shukla, H S and Tamsir, Mohammad and Srivastava, Vineet K and Kumar, Jai},
	year = {2014},
	file = {Shukla et al. - Numerical Solution of two dimensional coupled visc.pdf:/home/x/Library/library-ml-zotero/storage/4WLCH6CH/Shukla et al. - Numerical Solution of two dimensional coupled visc.pdf:application/pdf},
}

@article{putra_numerical_2020,
	title = {Numerical {Study} of {Inviscid} {Two}-{Dimensional} {Burgers} {Equation} using {Lax} {Method}},
	abstract = {In this report, we present the numerical solution of the inviscid Burgers equation in two dimensions. Burgers equation is a non-linear PDE that shows discontinuity which corresponds to the appearance of shocks. To solve this equation numerically, we create the spatial discretization using transfinite interpolation (TFI) method and solve the discretization with Lax method. Finally, we present the comparison between steady analytical and numerical solution of Burgers equation.},
	author = {Putra, Cahya and Izzaturrahman, Muhammad Faiz},
	month = dec,
	year = {2020},
	file = {Putra and Izzaturrahman - 2020 - Numerical Study of Inviscid Two-Dimensional Burger.pdf:/home/x/Library/library-ml-zotero/storage/K6KKHUMN/Putra and Izzaturrahman - 2020 - Numerical Study of Inviscid Two-Dimensional Burger.pdf:application/pdf},
}

@article{wang_2d_2016,
	title = {{2D} {Burgers} {Equations} with {Large} {Reynolds} {Number} {Using} {POD}/{DEIM} and {Calibration}},
	abstract = {Model order reduction (MOR) of the 2D Burgers equation is investigated. The mathematical formulation of POD/DEIM reduced order model (ROM) is derived based on the Galerkin projection and discrete empirical interpolation method (DEIM) from the existing high ﬁdelity implicit ﬁnite diﬀerence full model. For validation we numerically compared the POD ROM, POD/DEIM and the full model in two cases of Re = 100 and Re = 1000, respectively. We found that the POD/DEIM ROM leads to a speed-up of CPU time by a factor of O(10). The computational stability of POD/DEIM ROM is maintained by means of a careful selection of POD modes and the DEIM interpolation points. The solution of POD/DEIM in the case of Re = 1000 has an accuracy with error O(10−3) versus O(10 −4) in the case of Re = 100 when compared to the high ﬁdelity model. For this turbulent ﬂow, a closure model consisting of a Tikhonov regularization is carried out in order to recover the missing information and is developed to account for the small scale dissipation eﬀect of the discarded POD modes. It is shown that the computational results of this calibrated low-order model (LOM) exhibit considerable agreement with the real high-ﬁdelity model, which implies the eﬃciency of the closure model used.},
	language = {en},
	author = {Wang, Yuepeng and Navon, I Michael and Wang, Xinyue and Cheng, Yue},
	year = {2016},
	file = {Wang et al. - 2D Burgers Equations with Large Reynolds Number Us.pdf:/home/x/Library/library-ml-zotero/storage/UHIIEPSG/Wang et al. - 2D Burgers Equations with Large Reynolds Number Us.pdf:application/pdf},
}

@misc{zhao_numerical_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Numerical {Study} of {Two}-{Dimensional} {Burgers}’{Equation} by {Using} a {Continuous} {Galerkin} {Method}},
	url = {https://papers.ssrn.com/abstract=4291774},
	doi = {10.2139/ssrn.4291774},
	abstract = {In this article, we use space-time continuous Galerkin (STCG) method to find the numerical solution for two-dimensional (2D) Burgers’equation. The STCG method differs from conventional finite element methods, both the spatial and temporal variables are discretized by finite element method, thus it can easily obtain the high accuracy in both time and space directions and the corresponding discrete scheme is unconditionally stable. We demonstrate the existence and uniqueness of numerical solution by using Brouwer’s fixed point theorem and give the a priori error estimates without needing to satisfy the limitation of the space-time mesh ratio condition. At last, we provide two numerical examples to confirm the efficiency of the method. Moreover, the numerical experiments reveal that the proposed method can handle the bigger Reynolds number than the existing literatures in the case of the absence of stabilization techniques and has the better stability as well as does not need to impose a limitation on the time step size.},
	language = {en},
	urldate = {2023-06-23},
	author = {Zhao, Zhihui and Li, Hong},
	month = dec,
	year = {2022},
	keywords = {space-time continuous Galerkin method；two-dimensional Burgers'equation；a priori error estimates；Reynolds number, Stability},
	file = {Zhao and Li - 2022 - Numerical Study of Two-Dimensional Burgers’Equatio.pdf:/home/x/Library/library-ml-zotero/storage/KGVR8WQQ/Zhao and Li - 2022 - Numerical Study of Two-Dimensional Burgers’Equatio.pdf:application/pdf},
}

@article{carasso_stable_2019,
	title = {Stable explicit stepwise marching scheme in ill-posed time-reversed {2D} {Burgers}' equation},
	volume = {27},
	issn = {1741-5977, 1741-5985},
	url = {https://www.tandfonline.com/doi/full/10.1080/17415977.2018.1523905},
	doi = {10.1080/17415977.2018.1523905},
	language = {en},
	number = {12},
	urldate = {2023-06-23},
	journal = {Inverse Problems in Science and Engineering},
	author = {Carasso, Alfred S.},
	month = dec,
	year = {2019},
	pages = {1672--1688},
	file = {Accepted Version:/home/x/Library/library-ml-zotero/storage/PYM5M2GM/Carasso - 2019 - Stable explicit stepwise marching scheme in ill-po.pdf:application/pdf},
}

@article{zhanlav_higher-order_2016,
	title = {Higher-{Order} {Numerical} {Solution} of {Two}-{Dimensional} {Coupled} {Burgers}’ {Equations}},
	volume = {06},
	issn = {2161-1203, 2161-1211},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ajcm.2016.62013},
	doi = {10.4236/ajcm.2016.62013},
	number = {02},
	urldate = {2023-06-23},
	journal = {American Journal of Computational Mathematics},
	author = {Zhanlav, T. and Chuluunbaatar, O. and Ulziibayar, V.},
	year = {2016},
	pages = {120--129},
	file = {Full Text:/home/x/Library/library-ml-zotero/storage/RZGYFB82/Zhanlav et al. - 2016 - Higher-Order Numerical Solution of Two-Dimensional.pdf:application/pdf},
}

@misc{noauthor_numerical_nodate,
	title = {Numerical solution of one- and two-dimensional time-fractional {Burgers} equation via {Lucas} polynomials coupled with {Fini}…},
	url = {https://ouci.dntb.gov.ua/en/works/408DpBm4/},
	language = {uk-UA},
	urldate = {2023-06-23},
}

@article{ali_numerical_2022,
	title = {Numerical solution of one- and two-dimensional time-fractional {Burgers} equation via {Lucas} polynomials coupled with {Finite} difference method},
	volume = {61},
	issn = {1110-0168},
	url = {https://www.sciencedirect.com/science/article/pii/S1110016821007651},
	doi = {10.1016/j.aej.2021.11.032},
	abstract = {In this article, a numerical technique based on polynomials is proposed for the solution of one and two-dimensional time-fractional Burgers equation. First, the given problem is reduced to time discrete form using θ-weighted scheme. Then, with the help of Lucas and Fibonacci polynomials the given PDEs transformed to system of algebraic equations which is easy to solve. The proposed algorithm is validated by solving some numerical examples. Despite this, convergence analysis of the scheme is briefly discussed and verified numerically. The main objective of this paper is to show that polynomial based method is convenient for 1D and 2D nonlinear time-fractional partial differential equations (TFPDEs). Efficiency and performance of the proposed technique are examined by calculating L2 and L∞ error norms. Obtained accurate results confirm applicability and efficiency of the method.},
	language = {en},
	number = {8},
	urldate = {2023-06-23},
	journal = {Alexandria Engineering Journal},
	author = {Ali, Ihteram and Haq, Sirajul and Aldosary, Saud Fahad and Nisar, Kottakkaran Sooppy and Ahmad, Faraz},
	month = aug,
	year = {2022},
	keywords = {Burgers equations, Caputo fractional derivative, Fibonacci polynomials, Finite differences, Lucas polynomials},
	pages = {6077--6087},
	file = {Ali et al. - 2022 - Numerical solution of one- and two-dimensional tim.pdf:/home/x/Library/library-ml-zotero/storage/ZC3YR9SY/Ali et al. - 2022 - Numerical solution of one- and two-dimensional tim.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/THVRDYTE/S1110016821007651.html:text/html},
}

@article{barba_cfd_2018,
	title = {{CFD} {Python}: the 12 steps to {Navier}-{Stokes} equations},
	volume = {2},
	issn = {2577-3569},
	shorttitle = {{CFD} {Python}},
	url = {https://jose.theoj.org/papers/10.21105/jose.00021},
	doi = {10.21105/jose.00021},
	abstract = {Barba et al., (2018). CFD Python: the 12 steps to Navier-Stokes equations. Journal of Open Source Education, 1(9), 21, https://doi.org/10.21105/jose.00021},
	language = {en},
	number = {16},
	urldate = {2023-06-24},
	journal = {Journal of Open Source Education},
	author = {Barba, Lorena A. and Forsyth, Gilbert F.},
	month = nov,
	year = {2018},
	pages = {21},
	file = {Barba and Forsyth - 2018 - CFD Python the 12 steps to Navier-Stokes equation.pdf:/home/x/Library/library-ml-zotero/storage/XCBC9MLK/Barba and Forsyth - 2018 - CFD Python the 12 steps to Navier-Stokes equation.pdf:application/pdf},
}

@misc{noauthor_spatiotemporal_nodate,
	title = {Spatiotemporal {Patterns} {Estimation} {Using} a {Multilayer} {Perceptron} {Neural} {Network} in a {Solar} {Physics} {Application}},
	url = {https://sbic.org.br/lnlm/publicacoes/vol2-no1/vol2-no1-art2/},
	abstract = {Título: Spatiotemporal Patterns Estimation Using a Multilayer Perceptron Neural Network in a Solar Physics Application Autores: Andrade, Maria Conceição de; Rios Neto, Atair; Rosa, Reinaldo R.; Sawant, Hanumant S.; Fernandes, Francisco C. R. Resumo: In this paper we intend to Read More ...},
	language = {pt-BR},
	urldate = {2023-06-25},
	journal = {Learning and NonLinear Models},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/T3C4JGRC/vol2-no1-art2.html:text/html},
}

@article{campos_gradient_nodate,
	title = {Gradient {Pattern} {Analysis} of {Short} {Chain} {Molecules}},
	author = {Campos, Márcia R. and Rosa, Reinaldo R. and Ramos, Fernando M. and Vijaykumar, Nandamudi and Fujiwara, Susumu and Sato, Tetsuya},
}

@article{wilter_employing_nodate,
	title = {Employing {Gradient} {Pattern} {Analysis} to {Molecular} {Dynamics} {Simulations}},
	author = {Wilter, Alan and Campos, Márcia R. and Rosa, Reinaldo R. and Pascutti, Pedro G.},
}

@article{kweyu_numerical_2012,
	title = {Numerical solutions of the {Burgers}’ system in two dimensions under varied initial and boundary conditions},
	author = {Kweyu, M. C. and Manyonge, W. A. and Koross, A. and Ssemaganda, V.},
	year = {2012},
	file = {Kweyu et al. - 2012 - Numerical solutions of the Burgers’ system in two .pdf:/home/x/Library/library-ml-zotero/storage/W4BF72UL/Kweyu et al. - 2012 - Numerical solutions of the Burgers’ system in two .pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/MTJS4B94/1784.html:text/html},
}

@inproceedings{mccabe_machine_2021,
	title = {Machine {Learning} {Based} {Morphological} {Classification} of {Type}-{Ia} {Supernova} {Host} {Galaxies}},
	volume = {53},
	booktitle = {American {Astronomical} {Society} {Meeting} {Abstracts}},
	author = {McCabe, G. M. and Uddin, S.},
	year = {2021},
	note = {Issue: 1},
	pages = {541--15},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/V3K6B6YC/abstract.html:text/html},
}

@article{bustamante_ferreira_leonor_sentinal-high_2016,
	title = {Sentinal-{A} high performance virtual workspace for management and monitoring scientific applications},
	volume = {41},
	journal = {41st COSPAR Scientific Assembly},
	author = {Bustamante Ferreira Leonor, Bruno and Rosa, Reinaldo and Abrahão Dos Santos, Walter and Bomfin Jr, Asiel},
	year = {2016},
	pages = {S--1},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/34EC4EXG/abstract.html:text/html},
}

@article{oyarzabal_competition_nodate,
	title = {Competition and synergy patterns in nonhomogeneous ant-colony lattices.},
	author = {Oyarzabal, R. S. and Diaz, D. H. and Gomez, M. A. and Abrahão, W. and Oliveira, G.},
}

@inproceedings{de_andreade_estimation_2005,
	title = {Estimation of {Spatiotemporal} {Solar} {Data} using {Artificial} {Neural} {Networks} and {Gradient} {Pattern} {Analysis}.},
	booktitle = {Artificial {Intelligence} and {Applications}},
	author = {de Andreade, Maria Conceição and Rosa, Reinaldo R. and Neto, Atair Rios and Sawant, Hanumant S. and Fernandes, Francisco CR and Syck, Robert A.},
	year = {2005},
	pages = {850--855},
	file = {Snapshot:/home/x/Library/library-ml-zotero/storage/QLLCJ5KU/Abstract.html:text/html},
}

@article{noauthor_pdf_nodate,
	title = {[{PDF}] {Machine} {Learning} {Based} {Morphological} {Classification} of {Type}-{Ia} {Supernova} {Host} {Galaxies} by {G}. {M}. {McCabe}, {S}. {Uddin} · 3137748684 · {OA}.mg},
	url = {https://oa.mg/work/3137748684},
	abstract = {Read and download Machine Learning Based Morphological Classification of Type-Ia Supernova Host Galaxies by  G. M. McCabe, S. Uddin on OA.mg},
	urldate = {2023-06-25},
}

@misc{he_learning_2023,
	title = {Learning {Physics}-{Informed} {Neural} {Networks} without {Stacked} {Back}-propagation},
	url = {http://arxiv.org/abs/2202.09340},
	doi = {10.48550/arXiv.2202.09340},
	abstract = {Physics-Informed Neural Network (PINN) has become a commonly used machine learning approach to solve partial differential equations (PDE). But, facing high-dimensional secondorder PDE problems, PINN will suffer from severe scalability issues since its loss includes second-order derivatives, the computational cost of which will grow along with the dimension during stacked back-propagation. In this work, we develop a novel approach that can significantly accelerate the training of Physics-Informed Neural Networks. In particular, we parameterize the PDE solution by the Gaussian smoothed model and show that, derived from Stein's Identity, the second-order derivatives can be efficiently calculated without back-propagation. We further discuss the model capacity and provide variance reduction methods to address key limitations in the derivative estimation. Experimental results show that our proposed method can achieve competitive error compared to standard PINN training but is significantly faster. Our code is released at https://github.com/LithiumDA/PINN-without-Stacked-BP.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {He, Di and Li, Shanda and Shi, Wenlei and Gao, Xiaotian and Zhang, Jia and Bian, Jiang and Wang, Liwei and Liu, Tie-Yan},
	month = feb,
	year = {2023},
	note = {arXiv:2202.09340 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/QYXRGJTD/2202.html:text/html;He et al. - 2023 - Learning Physics-Informed Neural Networks without .pdf:/home/x/Library/library-ml-zotero/storage/J2JA6WXM/He et al. - 2023 - Learning Physics-Informed Neural Networks without .pdf:application/pdf},
}

@misc{zhang_hybrid_2022,
	title = {A {Hybrid} {Iterative} {Numerical} {Transferable} {Solver} ({HINTS}) for {PDEs} {Based} on {Deep} {Operator} {Network} and {Relaxation} {Methods}},
	url = {http://arxiv.org/abs/2208.13273},
	doi = {10.48550/arXiv.2208.13273},
	abstract = {Iterative solvers of linear systems are a key component for the numerical solutions of partial differential equations (PDEs). While there have been intensive studies through past decades on classical methods such as Jacobi, Gauss-Seidel, conjugate gradient, multigrid methods and their more advanced variants, there is still a pressing need to develop faster, more robust and reliable solvers. Based on recent advances in scientific deep learning for operator regression, we propose HINTS, a hybrid, iterative, numerical, and transferable solver for differential equations. HINTS combines standard relaxation methods and the Deep Operator Network (DeepONet). Compared to standard numerical solvers, HINTS is capable of providing faster solutions for a wide class of differential equations, while preserving the accuracy close to machine zero. Through an eigenmode analysis, we find that the individual solvers in HINTS target distinct regions in the spectrum of eigenmodes, resulting in a uniform convergence rate and hence exceptional performance of the hybrid solver overall. Moreover, HINTS applies to equations in multidimensions, and is flexible with regards to computational domain and transferable to different discretizations.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Zhang, Enrui and Kahana, Adar and Turkel, Eli and Ranade, Rishikesh and Pathak, Jay and Karniadakis, George Em},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13273 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/59VI5HI2/2208.html:text/html;Zhang et al. - 2022 - A Hybrid Iterative Numerical Transferable Solver (.pdf:/home/x/Library/library-ml-zotero/storage/C74Y22CI/Zhang et al. - 2022 - A Hybrid Iterative Numerical Transferable Solver (.pdf:application/pdf},
}

@misc{heiden_neuralsim_2021,
	title = {{NeuralSim}: {Augmenting} {Differentiable} {Simulators} with {Neural} {Networks}},
	shorttitle = {{NeuralSim}},
	url = {http://arxiv.org/abs/2011.04217},
	doi = {10.48550/arXiv.2011.04217},
	abstract = {Differentiable simulators provide an avenue for closing the sim-to-real gap by enabling the use of efficient, gradient-based optimization algorithms to find the simulation parameters that best fit the observed sensor readings. Nonetheless, these analytical models can only predict the dynamical behavior of systems for which they have been designed. In this work, we study the augmentation of a novel differentiable rigid-body physics engine via neural networks that is able to learn nonlinear relationships between dynamic quantities and can thus learn effects not accounted for in traditional simulators.Such augmentations require less data to train and generalize better compared to entirely data-driven models. Through extensive experiments, we demonstrate the ability of our hybrid simulator to learn complex dynamics involving frictional contacts from real data, as well as match known models of viscous friction, and present an approach for automatically discovering useful augmentations. We show that, besides benefiting dynamics modeling, inserting neural networks can accelerate model-based control architectures. We observe a ten-fold speed-up when replacing the QP solver inside a model-predictive gait controller for quadruped robots with a neural network, allowing us to significantly improve control delays as we demonstrate in real-hardware experiments. We publish code, additional results and videos from our experiments on our project webpage at https://sites.google.com/usc.edu/neuralsim.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Heiden, Eric and Millard, David and Coumans, Erwin and Sheng, Yizhou and Sukhatme, Gaurav S.},
	month = may,
	year = {2021},
	note = {arXiv:2011.04217 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/XM54WN9Z/2011.html:text/html;Heiden et al. - 2021 - NeuralSim Augmenting Differentiable Simulators wi.pdf:/home/x/Library/library-ml-zotero/storage/ZU9493W8/Heiden et al. - 2021 - NeuralSim Augmenting Differentiable Simulators wi.pdf:application/pdf},
}

@misc{sanchez-gonzalez_learning_2020,
	title = {Learning to {Simulate} {Complex} {Physics} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/2002.09405},
	doi = {10.48550/arXiv.2002.09405},
	abstract = {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.},
	month = sep,
	year = {2020},
	note = {arXiv:2002.09405 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/3SQ3VWCC/2002.html:text/html;Sanchez-Gonzalez et al. - 2020 - Learning to Simulate Complex Physics with Graph Ne.pdf:/home/x/Library/library-ml-zotero/storage/5SWJXEL8/Sanchez-Gonzalez et al. - 2020 - Learning to Simulate Complex Physics with Graph Ne.pdf:application/pdf},
}

@misc{dandekar_bayesian_2022,
	title = {Bayesian {Neural} {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2012.07244},
	doi = {10.48550/arXiv.2012.07244},
	abstract = {Recently, Neural Ordinary Differential Equations has emerged as a powerful framework for modeling physical simulations without explicitly defining the ODEs governing the system, but instead learning them via machine learning. However, the question: "Can Bayesian learning frameworks be integrated with Neural ODE's to robustly quantify the uncertainty in the weights of a Neural ODE?" remains unanswered. In an effort to address this question, we primarily evaluate the following categories of inference methods: (a) The No-U-Turn MCMC sampler (NUTS), (b) Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) and (c) Stochastic Langevin Gradient Descent (SGLD). We demonstrate the successful integration of Neural ODEs with the above Bayesian inference frameworks on classical physical systems, as well as on standard machine learning datasets like MNIST, using GPU acceleration. On the MNIST dataset, we achieve a posterior sample accuracy of 98.5\% on the test ensemble of 10,000 images. Subsequently, for the first time, we demonstrate the successful integration of variational inference with normalizing flows and Neural ODEs, leading to a powerful Bayesian Neural ODE object. Finally, considering a predator-prey model and an epidemiological system, we demonstrate the probabilistic identification of model specification in partially-described dynamical systems using universal ordinary differential equations. Together, this gives a scientific machine learning tool for probabilistic estimation of epistemic uncertainties.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Dandekar, Raj and Chung, Karen and Dixit, Vaibhav and Tarek, Mohamed and Garcia-Valadez, Aslan and Vemula, Krishna Vishal and Rackauckas, Chris},
	month = feb,
	year = {2022},
	note = {arXiv:2012.07244 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/L6K8SNBJ/Dandekar et al. - 2022 - Bayesian Neural Ordinary Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/FB2958TQ/2012.html:text/html},
}

@misc{daw_physics-guided_2021,
	title = {Physics-guided {Neural} {Networks} ({PGNN}): {An} {Application} in {Lake} {Temperature} {Modeling}},
	shorttitle = {Physics-guided {Neural} {Networks} ({PGNN})},
	url = {http://arxiv.org/abs/1710.11431},
	doi = {10.48550/arXiv.1710.11431},
	abstract = {This paper introduces a framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed physics-guided neural networks (PGNN), leverages the output of physics-based model simulations along with observational features in a hybrid modeling setup to generate predictions using a neural network architecture. Further, this framework uses physics-based loss functions in the learning objective of neural networks to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. We illustrate the effectiveness of PGNN for the problem of lake temperature modeling, where physical relationships between the temperature, density, and depth of water are used to design a physics-based loss function. By using scientific knowledge to guide the construction and learning of neural networks, we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results. All the code and datasets used in this study have been made available on this link {\textbackslash}url\{https://github.com/arkadaw9/PGNN\}.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Daw, Arka and Karpatne, Anuj and Watkins, William and Read, Jordan and Kumar, Vipin},
	month = sep,
	year = {2021},
	note = {arXiv:1710.11431 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/QCBPARLM/Daw et al. - 2021 - Physics-guided Neural Networks (PGNN) An Applicat.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/Q3V64EVH/1710.html:text/html},
}

@misc{rackauckas_universal_2021,
	title = {Universal {Differential} {Equations} for {Scientific} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2001.04385},
	doi = {10.48550/arXiv.2001.04385},
	abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." In this manuscript we introduce the SciML software ecosystem as a tool for mixing the information of physical laws and scientific models with data-driven machine learning approaches. We describe a mathematical object, which we denote universal differential equations (UDEs), as the unifying framework connecting the ecosystem. We show how a wide variety of applications, from automatically discovering biological mechanisms to solving high-dimensional Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled through the UDE formalism and its tooling. We demonstrate the generality of the software tooling to handle stochasticity, delays, and implicit constraints. This funnels the wide variety of SciML applications into a core set of training mechanisms which are highly optimized, stabilized for stiff equations, and compatible with distributed parallelism and GPU accelerators.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
	month = nov,
	year = {2021},
	note = {arXiv:2001.04385 [cs, math, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Dynamical Systems, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/home/x/Library/library-ml-zotero/storage/A7G8I6TP/Rackauckas et al. - 2021 - Universal Differential Equations for Scientific Ma.pdf:application/pdf;arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/LMMFJEP5/2001.html:text/html},
}

@article{blechschmidt_three_2021,
	title = {Three ways to solve partial differential equations with neural networks — {A} review},
	volume = {44},
	issn = {1522-2608},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/gamm.202100006},
	doi = {10.1002/gamm.202100006},
	abstract = {Neural networks are increasingly used to construct numerical solution methods for partial differential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman–Kac formula and methods based on the solution of backward stochastic differential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.},
	language = {en},
	number = {2},
	urldate = {2023-06-28},
	journal = {GAMM-Mitteilungen},
	author = {Blechschmidt, Jan and Ernst, Oliver G.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/gamm.202100006},
	keywords = {backward differential equation, curse of dimensionality, Feynman–Kac, Hamilton–Jacobi–Bellman equations, neural networks, partial differential equation, PINN, stochastic process},
	pages = {e202100006},
	file = {Blechschmidt and Ernst - 2021 - Three ways to solve partial differential equations.pdf:/home/x/Library/library-ml-zotero/storage/WJFCL8S2/Blechschmidt and Ernst - 2021 - Three ways to solve partial differential equations.pdf:application/pdf;Snapshot:/home/x/Library/library-ml-zotero/storage/MTYX6MI3/gamm.html:text/html},
}

@article{bauer_quiet_2015,
	title = {The quiet revolution of numerical weather prediction},
	volume = {525},
	doi = {10.1038/nature14956},
	abstract = {Advances in numerical weather prediction represent a quiet revolution because they have resulted from a steady accumulation of scientific knowledge and technological advances over many years that, with only a few exceptions, have not been associated with the aura of fundamental physics breakthroughs. Nonetheless, the impact of numerical weather prediction is among the greatest of any area of physical science. As a computational problem, global weather prediction is comparable to the simulation of the human brain and of the evolution of the early Universe, and it is performed every day at major operational centres across the world.},
	journal = {Nature},
	author = {Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
	month = sep,
	year = {2015},
	pages = {47--55},
	file = {Bauer et al. - 2015 - The quiet revolution of numerical weather predicti.pdf:/home/x/Library/library-ml-zotero/storage/NDBS79DD/Bauer et al. - 2015 - The quiet revolution of numerical weather predicti.pdf:application/pdf},
}

@article{li_lanets_2023,
	title = {{LaNets}: {Hybrid} {Lagrange} {Neural} {Networks} for {Solving} {Partial} {Differential燛quations}},
	volume = {134},
	shorttitle = {{LaNets}},
	doi = {10.32604/cmes.2022.021277},
	journal = {Computer Modeling in Engineering \& Sciences},
	author = {Li, Ying and Xu, Longxiang and Mei, Fangjun and Ying, Shihui},
	month = jan,
	year = {2023},
	pages = {657--672},
	file = {Li et al. - 2023 - LaNets Hybrid Lagrange Neural Networks for Solvin.pdf:/home/x/Library/library-ml-zotero/storage/CU33CDMY/Li et al. - 2023 - LaNets Hybrid Lagrange Neural Networks for Solvin.pdf:application/pdf},
}

@article{yuan_-pinn_2022,
	title = {A-{PINN}: {Auxiliary} physics informed neural networks for forward and inverse problems of nonlinear integro-differential equations},
	volume = {462},
	issn = {0021-9991},
	shorttitle = {A-{PINN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999122003229},
	doi = {10.1016/j.jcp.2022.111260},
	abstract = {Physics informed neural networks (PINNs) are a novel deep learning paradigm primed for solving forward and inverse problems of nonlinear partial differential equations (PDEs). By embedding physical information delineated by PDEs in feedforward neural networks, PINNs are trained as surrogate models for approximate solution to the PDEs without need of label data. Due to the excellent capability of neural networks in describing complex relationships, a variety of PINN-based methods have been developed to solve different kinds of problems such as integer-order PDEs, fractional PDEs, stochastic PDEs and integro-differential equations (IDEs). However, for the state-of-the-art PINN methods in application to IDEs, integral discretization is a key prerequisite in order that IDEs can be transformed into ordinary differential equations (ODEs). However, integral discretization inevitably introduces discretization error and truncation error to the solution. In this study, we propose an auxiliary physics informed neural network (A-PINN) framework for solving forward and inverse problems of nonlinear IDEs. By defining auxiliary output variable(s) to represent the integral(s) in the governing equation and employing automatic differentiation of the auxiliary output to replace integral operator, the proposed A-PINN bypasses the limitation of integral discretization. Distinct from the neural network in the original PINN which only approximates the variables in the governing equation, in the proposed A-PINN framework, a multi-output neural network is constructed to simultaneously calculate the primary outputs and auxiliary outputs which respectively approximate the variables and integrals in the governing equation. Subsequently, the relationship between the primary outputs and auxiliary outputs is constrained by new output conditions in compliance with physical laws. By pursuing the first-order nonlinear Volterra IDE benchmark problem, we validate that the proposed A-PINN can obtain more accurate solution than the conventional PINN. We further demonstrate the good performance of A-PINN in solving the forward problems involving nonlinear Volterra IDEs system, nonlinear 2-dimensional Volterra IDE, nonlinear 10-dimensional Volterra IDE, and nonlinear Fredholm IDE. Finally, the A-PINN framework is implemented to solve the inverse problem of nonlinear IDEs and the results show that the unknown parameters can be satisfactorily discovered even with heavily noisy data.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Yuan, Lei and Ni, Yi-Qing and Deng, Xiang-Yun and Hao, Shuo},
	month = aug,
	year = {2022},
	keywords = {Deep learning, Auxiliary physics informed neural network (A-PINN), Integro-differential equations (IDEs), Multi-output neural network, Physics informed neural network (PINN)},
	pages = {111260},
	file = {Yuan et al. - 2022 - A-PINN Auxiliary physics informed neural networks.pdf:/home/x/Library/library-ml-zotero/storage/TAWHPBEY/Yuan et al. - 2022 - A-PINN Auxiliary physics informed neural networks.pdf:application/pdf},
}

@article{yang_b-pinns_2021,
	title = {B-{PINNs}: {Bayesian} physics-informed neural networks for forward and inverse {PDE} problems with noisy data},
	volume = {425},
	issn = {0021-9991},
	shorttitle = {B-{PINNs}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120306872},
	doi = {10.1016/j.jcp.2020.109913},
	abstract = {We propose a Bayesian physics-informed neural network (B-PINN) to solve both forward and inverse nonlinear problems described by partial differential equations (PDEs) and noisy data. In this Bayesian framework, the Bayesian neural network (BNN) combined with a PINN for PDEs serves as the prior while the Hamiltonian Monte Carlo (HMC) or the variational inference (VI) could serve as an estimator of the posterior. B-PINNs make use of both physical laws and scattered noisy measurements to provide predictions and quantify the aleatoric uncertainty arising from the noisy data in the Bayesian framework. Compared with PINNs, in addition to uncertainty quantification, B-PINNs obtain more accurate predictions in scenarios with large noise due to their capability of avoiding overfitting. We conduct a systematic comparison between the two different approaches for the B-PINNs posterior estimation (i.e., HMC or VI), along with dropout used for quantifying uncertainty in deep neural networks. Our experiments show that HMC is more suitable than VI with mean field Gaussian approximation for the B-PINNs posterior estimation, while dropout employed in PINNs can hardly provide accurate predictions with reasonable uncertainty. Finally, we replace the BNN in the prior with a truncated Karhunen-Loève (KL) expansion combined with HMC or a deep normalizing flow (DNF) model as posterior estimators. The KL is as accurate as BNN and much faster but this framework cannot be easily extended to high-dimensional problems unlike the BNN based framework.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
	month = jan,
	year = {2021},
	keywords = {Bayesian physics-informed neural networks, Hamiltonian Monte Carlo, Noisy data, Nonlinear PDEs, Variational inference},
	pages = {109913},
	file = {ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/JPUHDT43/S0021999120306872.html:text/html;Yang et al. - 2021 - B-PINNs Bayesian physics-informed neural networks.pdf:/home/x/Library/library-ml-zotero/storage/G9IAVKZI/Yang et al. - 2021 - B-PINNs Bayesian physics-informed neural networks.pdf:application/pdf},
}

@article{jin_nsfnets_2021,
	title = {{NSFnets} ({Navier}-{Stokes} flow nets): {Physics}-informed neural networks for the incompressible {Navier}-{Stokes} equations},
	volume = {426},
	issn = {0021-9991},
	shorttitle = {{NSFnets} ({Navier}-{Stokes} flow nets)},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120307257},
	doi = {10.1016/j.jcp.2020.109951},
	abstract = {In the last 50 years there has been a tremendous progress in solving numerically the Navier-Stokes equations using finite differences, finite elements, spectral, and even meshless methods. Yet, in many real cases, we still cannot incorporate seamlessly (multi-fidelity) data into existing algorithms, and for industrial-complexity applications the mesh generation is time consuming and still an art. Moreover, solving ill-posed problems (e.g., lacking boundary conditions) or inverse problems is often prohibitively expensive and requires different formulations and new computer codes. Here, we employ physics-informed neural networks (PINNs), encoding the governing equations directly into the deep neural network via automatic differentiation, to overcome some of the aforementioned limitations for simulating incompressible laminar and turbulent flows. We develop the Navier-Stokes flow nets (NSFnets) by considering two different mathematical formulations of the Navier-Stokes equations: the velocity-pressure (VP) formulation and the vorticity-velocity (VV) formulation. Since this is a new approach, we first select some standard benchmark problems to assess the accuracy, convergence rate, computational cost and flexibility of NSFnets; analytical solutions and direct numerical simulation (DNS) databases provide proper initial and boundary conditions for the NSFnet simulations. The spatial and temporal coordinates are the inputs of the NSFnets, while the instantaneous velocity and pressure fields are the outputs for the VP-NSFnet, and the instantaneous velocity and vorticity fields are the outputs for the VV-NSFnet. This is unsupervised learning and, hence, no labeled data are required beyond boundary and initial conditions and the fluid properties. The residuals of the VP or VV governing equations, together with the initial and boundary conditions, are embedded into the loss function of the NSFnets. No data is provided for the pressure to the VP-NSFnet, which is a hidden state and is obtained via the incompressibility constraint without extra computational cost. Unlike the traditional numerical methods, NSFnets inherit the properties of neural networks (NNs), hence the total error is composed of the approximation, the optimization, and the generalization errors. Here, we empirically attempt to quantify these errors by varying the sampling (“residual”) points, the iterative solvers, and the size of the NN architecture. For the laminar flow solutions, we show that both the VP and the VV formulations are comparable in accuracy but their best performance corresponds to different NN architectures. The initial convergence rate is fast but the error eventually saturates to a plateau due to the dominance of the optimization error. For the turbulent channel flow, we show that NSFnets can sustain turbulence at Reτ∼1,000, but due to expensive training we only consider part of the channel domain and enforce velocity boundary conditions on the subdomain boundaries provided by the DNS data base. We also perform a systematic study on the weights used in the loss function for balancing the data and physics components, and investigate a new way of computing the weights dynamically to accelerate training and enhance accuracy. In the last part, we demonstrate how NSFnets should be used in practice, namely for ill-posed problems with incomplete or noisy boundary conditions as well as for inverse problems. We obtain reasonably accurate solutions for such cases as well without the need to change the NSFnets and at the same computational cost as in the forward well-posed problems. We also present a simple example of transfer learning that will aid in accelerating the training of NSFnets for different parameter settings.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Jin, Xiaowei and Cai, Shengze and Li, Hui and Karniadakis, George Em},
	month = feb,
	year = {2021},
	keywords = {Ill-posed problems, PINNs, Transfer learning, Turbulence, Velocity-pressure formulation, Vorticity-velocity formulation},
	pages = {109951},
	file = {Jin et al. - 2021 - NSFnets (Navier-Stokes flow nets) Physics-informe.pdf:/home/x/Library/library-ml-zotero/storage/THBHCSKU/Jin et al. - 2021 - NSFnets (Navier-Stokes flow nets) Physics-informe.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/H6Q73CPL/S0021999120307257.html:text/html},
}

@article{kashefi_physics-informed_2022,
	title = {Physics-informed {PointNet}: {A} deep learning solver for steady-state incompressible flows and thermal fields on multiple sets of irregular geometries},
	volume = {468},
	issn = {0021-9991},
	shorttitle = {Physics-informed {PointNet}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999122005721},
	doi = {10.1016/j.jcp.2022.111510},
	abstract = {We present a novel physics-informed deep learning framework for solving steady-state incompressible flow on multiple sets of irregular geometries by incorporating two main elements: using a point-cloud based neural network to capture geometric features of computational domains, and using the mean squared residuals of the governing partial differential equations, boundary conditions, and sparse observations as the loss function of the network to capture the physics. While the solution of the continuity and Navier-Stokes equations is a function of the geometry of the computational domain, current versions of physics-informed neural networks have no mechanism to express this functionally in their outputs, and thus are restricted to obtain the solutions only for one computational domain with each training procedure. Using the proposed framework, three new facilities become available. First, the governing equations are solvable on a set of computational domains containing irregular geometries with high variations with respect to each other but requiring training only once. Second, after training the introduced framework on the set, it is now able to predict the solutions on domains with unseen geometries from seen and unseen categories as well. The former and the latter both lead to savings in computational costs. Finally, all the advantages of the point-cloud based neural network for irregular geometries, already used for supervised learning, are transferred to the proposed physics-informed framework. The effectiveness of our framework is shown through the method of manufactured solutions and thermally-driven flow for forward and inverse problems.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Kashefi, Ali and Mukerji, Tapan},
	month = nov,
	year = {2022},
	keywords = {Automatic differentiation, Incompressible flow, Irregular geometries, Physics-informed deep learning, PointNet, Thermally-driven flow},
	pages = {111510},
	file = {Kashefi and Mukerji - 2022 - Physics-informed PointNet A deep learning solver .pdf:/home/x/Library/library-ml-zotero/storage/SXKVISAW/Kashefi and Mukerji - 2022 - Physics-informed PointNet A deep learning solver .pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/97R3XHD7/S0021999122005721.html:text/html},
}

@article{shu_physics-informed_2023,
	title = {A physics-informed diffusion model for high-fidelity flow field reconstruction},
	volume = {478},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999123000670},
	doi = {10.1016/j.jcp.2023.111972},
	abstract = {Machine learning models are gaining increasing popularity in the domain of fluid dynamics for their potential to accelerate the production of high-fidelity computational fluid dynamics data. However, many recently proposed machine learning models for high-fidelity data reconstruction require low-fidelity data for model training. Such requirement restrains the application performance of these models, since their data reconstruction accuracy would drop significantly if the low-fidelity input data used in model test has a large deviation from the training data. To overcome this restraint, we propose a diffusion model which only uses high-fidelity data at training. With different configurations, our model is able to reconstruct high-fidelity data from either a regular low-fidelity sample or a randomly measured sample, and is also able to gain an accuracy increase by using physics-informed conditioning information from a known partial differential equation when that is available. Experimental results demonstrate that our model can produce accurate reconstruction results for 2d turbulent flows based on different input sources without retraining.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Shu, Dule and Li, Zijie and Barati Farimani, Amir},
	month = apr,
	year = {2023},
	keywords = {Computational fluid dynamics, Denoising diffusion probabilistic models, Super-resolution},
	pages = {111972},
	file = {ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/R3WMTZRR/S0021999123000670.html:text/html;Shu et al. - 2023 - A physics-informed diffusion model for high-fideli.pdf:/home/x/Library/library-ml-zotero/storage/FU7W8CDX/Shu et al. - 2023 - A physics-informed diffusion model for high-fideli.pdf:application/pdf},
}

@article{zhu_physics-constrained_2019,
	title = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
	volume = {394},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999119303559},
	doi = {10.1016/j.jcp.2019.05.024},
	abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Zhu, Yinhao and Zabaras, Nicholas and Koutsourelakis, Phaedon-Stelios and Perdikaris, Paris},
	month = oct,
	year = {2019},
	keywords = {Conditional generative model, Normalizing flow, Physics-constrained, Reverse KL divergence, Surrogate modeling, Uncertainty quantification},
	pages = {56--81},
	file = {ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/94XMQBLG/S0021999119303559.html:text/html;Zhu et al. - 2019 - Physics-constrained deep learning for high-dimensi.pdf:/home/x/Library/library-ml-zotero/storage/KU2CWM6X/Zhu et al. - 2019 - Physics-constrained deep learning for high-dimensi.pdf:application/pdf},
}

@article{gao_phygeonet_2021,
	title = {{PhyGeoNet}: {Physics}-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state {PDEs} on irregular domain},
	volume = {428},
	issn = {0021-9991},
	shorttitle = {{PhyGeoNet}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120308536},
	doi = {10.1016/j.jcp.2020.110079},
	abstract = {Recently, the advent of deep learning has spurred interest in the development of physics-informed neural networks (PINN) for efficiently solving partial differential equations (PDEs), particularly in a parametric setting. Among all different classes of deep neural networks, the convolutional neural network (CNN) has attracted increasing attention in the scientific machine learning community, since the parameter-sharing feature in CNN enables efficient learning for problems with large-scale spatiotemporal fields. However, one of the biggest challenges is that CNN only can handle regular geometries with image-like format (i.e., rectangular domains with uniform grids). In this paper, we propose a novel physics-constrained CNN learning architecture, aiming to learn solutions of parametric PDEs on irregular domains without any labeled data. In order to leverage powerful classic CNN backbones, elliptic coordinate mapping is introduced to enable coordinate transforms between the irregular physical domain and regular reference domain. The proposed method has been assessed by solving a number of steady-state PDEs on irregular domains, including heat equations, Navier-Stokes equations, and Poisson equations with parameterized boundary conditions, varying geometries, and spatially-varying source fields. Moreover, the proposed method has also been compared against the state-of-the-art PINN with fully-connected neural network (FC-NN) formulation. The numerical results demonstrate the effectiveness of the proposed approach and exhibit notable superiority over the FC-NN based PINN in terms of efficiency and accuracy.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Gao, Han and Sun, Luning and Wang, Jian-Xun},
	month = mar,
	year = {2021},
	keywords = {Partial differential equations, Physics-informed neural networks, Surrogate modeling, Label-free, Navier-Stokes, Physics-constrained deep learning},
	pages = {110079},
	file = {Gao et al. - 2021 - PhyGeoNet Physics-informed geometry-adaptive conv.pdf:/home/x/Library/library-ml-zotero/storage/PDLI8HEZ/Gao et al. - 2021 - PhyGeoNet Physics-informed geometry-adaptive conv.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/QL2UVIW6/S0021999120308536.html:text/html},
}

@article{raissi_hidden_2018,
	title = {Hidden physics models: {Machine} learning of nonlinear partial differential equations},
	volume = {357},
	issn = {0021-9991},
	shorttitle = {Hidden physics models},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999117309014},
	doi = {10.1016/j.jcp.2017.11.039},
	abstract = {While there is currently a lot of enthusiasm about “big data”, useful data is usually “small” and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from small data. In particular, we introduce hidden physics models, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier–Stokes, Schrödinger, Kuramoto–Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Raissi, Maziar and Karniadakis, George Em},
	month = mar,
	year = {2018},
	keywords = {Uncertainty quantification, Bayesian modeling, Fractional equations, Probabilistic machine learning, Small data, System identification},
	pages = {125--141},
	file = {Raissi and Karniadakis - 2018 - Hidden physics models Machine learning of nonline.pdf:/home/x/Library/library-ml-zotero/storage/UBEY3GBV/Raissi and Karniadakis - 2018 - Hidden physics models Machine learning of nonline.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/TUMIZPVV/S0021999117309014.html:text/html},
}

@article{chen_physics-informed_2021,
	title = {Physics-informed machine learning for reduced-order modeling of nonlinear problems},
	volume = {446},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999121005611},
	doi = {10.1016/j.jcp.2021.110666},
	abstract = {A reduced basis method based on a physics-informed machine learning framework is developed for efficient reduced-order modeling of parametrized partial differential equations (PDEs). A feedforward neural network is used to approximate the mapping from the time-parameter to the reduced coefficients. During the offline stage, the network is trained by minimizing the weighted sum of the residual loss of the reduced-order equations, and the data loss of the labeled reduced coefficients that are obtained via the projection of high-fidelity snapshots onto the reduced space. Such a network is referred to as physics-reinforced neural network (PRNN). As the number of residual points in time-parameter space can be very large, an accurate network – referred to as physics-informed neural network (PINN) – can be trained by minimizing only the residual loss. However, for complex nonlinear problems, the solution of the reduced-order equation is less accurate than the projection of high-fidelity solution onto the reduced space. Therefore, the PRNN trained with the snapshot data is expected to have higher accuracy than the PINN. Numerical results demonstrate that the PRNN is more accurate than the PINN and a purely data-driven neural network for complex problems. During the reduced basis refinement, the PRNN may obtain higher accuracy than the direct reduced-order model based on a Galerkin projection. The online evaluation of PINN/PRNN is orders of magnitude faster than that of the Galerkin reduced-order model.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Chen, Wenqian and Wang, Qian and Hesthaven, Jan S. and Zhang, Chuhua},
	month = dec,
	year = {2021},
	keywords = {Feedforward neural network, Nonlinear PDE, Physics-informed machine learning, Reduced-order modeling},
	pages = {110666},
	file = {Chen et al. - 2021 - Physics-informed machine learning for reduced-orde.pdf:/home/x/Library/library-ml-zotero/storage/XBKLZISI/Chen et al. - 2021 - Physics-informed machine learning for reduced-orde.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/AUSCPZR9/S0021999121005611.html:text/html},
}

@article{mitusch_hybrid_2021,
	title = {Hybrid {FEM}-{NN} models: {Combining} artificial neural networks with the finite element method},
	volume = {446},
	issn = {0021-9991},
	shorttitle = {Hybrid {FEM}-{NN} models},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999121005465},
	doi = {10.1016/j.jcp.2021.110651},
	abstract = {We present a methodology combining neural networks with physical principle constraints in the form of partial differential equations (PDEs). The approach allows to train neural networks while respecting the PDEs as a strong constraint in the optimisation as apposed to making them part of the loss function. The resulting models are discretised in space by the finite element method (FEM). The method applies to both stationary and transient as well as linear/nonlinear PDEs. We describe implementation of the approach as an extension of the existing FEM framework FEniCS and its algorithmic differentiation tool dolfin-adjoint. Through series of examples we demonstrate capabilities of the approach to recover coefficients and missing PDE operators from observations. Further, the proposed method is compared with alternative methodologies, namely, physics informed neural networks and standard PDE-constrained optimisation. Finally, we demonstrate the method on a complex cardiac cell model problem using deep neural networks.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Mitusch, Sebastian K. and Funke, Simon W. and Kuchta, Miroslav},
	month = dec,
	year = {2021},
	keywords = {Machine learning, Partial differential equations, Data-driven scientific computing, Finite element method, Learning unknown physics},
	pages = {110651},
	file = {Mitusch et al. - 2021 - Hybrid FEM-NN models Combining artificial neural .pdf:/home/x/Library/library-ml-zotero/storage/WEYQLT3M/Mitusch et al. - 2021 - Hybrid FEM-NN models Combining artificial neural .pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/QG9V2C86/S0021999121005465.html:text/html},
}

@article{raissi_machine_2017,
	title = {Machine learning of linear differential equations using {Gaussian} processes},
	volume = {348},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999117305582},
	doi = {10.1016/j.jcp.2017.07.050},
	abstract = {This work leverages recent advances in probabilistic machine learning to discover governing equations expressed by parametric linear operators. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or “black-box” computer simulations, as demonstrated in several synthetic examples and a realistic application in functional genomics.},
	language = {en},
	urldate = {2023-06-28},
	journal = {Journal of Computational Physics},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	keywords = {Uncertainty quantification, Probabilistic machine learning, Fractional differential equations, Functional genomics, Inverse problems},
	pages = {683--693},
	file = {Raissi et al. - 2017 - Machine learning of linear differential equations .pdf:/home/x/Library/library-ml-zotero/storage/6CXADR8H/Raissi et al. - 2017 - Machine learning of linear differential equations .pdf:application/pdf;ScienceDirect Snapshot:/home/x/Library/library-ml-zotero/storage/Y5HAHJZY/S0021999117305582.html:text/html},
}

@misc{li_fourier_2021,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:/home/x/Library/library-ml-zotero/storage/VHLDIDX3/2010.html:text/html;Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:/home/x/Library/library-ml-zotero/storage/VB8AMTWR/Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf},
}
